/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:39 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:57 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:23 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:58 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:31 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:15 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1677.5947148799896 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:49 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:17 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:55 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:16:26 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:03 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1666.5243191719055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:35 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:05 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:35 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:07 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:40 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1657.0925278663635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:16 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:57 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:40 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:12 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:48 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1685.265785932541 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:17 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:57 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:30 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:59 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:32 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1660.3419432640076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:01 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:39 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:16 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:57 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:13:34 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1690.372056722641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:12 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:24:51 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:30:28 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:03 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1689.0129714012146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:17 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:52:51 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:33 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:04:08 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:41 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1677.180371761322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:20 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:21:02 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:26:37 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:12 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:46 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1699.1880249977112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:43:37 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:06 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:38 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:00:16 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:05:45 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1661.004180431366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:11:17 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:16:51 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:33 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:17 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:33:59 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1700.249561548233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:39:34 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:45:07 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:50:38 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:10 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:01:46 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1660.6947615146637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:07:14 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:12:50 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:18:25 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:24:02 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:29:31 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1667.0920157432556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:07 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:40:46 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:46:13 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:51:58 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:37 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1687.6788353919983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:03:13 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:09:04 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:14:47 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:26 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:26:05 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1722.3324611186981 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:31:56 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:37:49 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:37 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:49:21 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:55:01 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1720.2832670211792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:00:35 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:06:08 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:11:48 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:17:30 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:23:24 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1702.911280632019 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:28:57 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:34:42 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:19 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:46:01 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:51:56 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1725.7304096221924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:57:47 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:03:40 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:09:34 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:15:23 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:20:59 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1737.3992755413055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:26:47 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:32:31 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:38:11 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:43:57 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:49:31 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1710.8218266963959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:55:13 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:00:56 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:06:35 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:12:30 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:18:18 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1733.0032029151917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:24:09 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:29:53 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:35:37 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:41:18 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:47:10 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1713.8848645687103 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:52:42 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:58:30 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:04:07 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:09:46 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:15:29 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1714.5459241867065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:21:17 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:27:08 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:32:47 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:28 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:44:06 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1711.901785850525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:49:46 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:55:22 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:01:13 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:07:03 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:12:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1722.2956171035767 seconds. 
Discarding model... 

Training complete taking 42394.402849674225 total seconds. 
Now scoring model... 
Scoring complete taking 2.1873586177825928 seconds. 
Saved predicted values as IQP_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.36924393992999,), 'R2_train': 0.9885218082670201, 'MAE_train': 1.1378144288491423, 'MSE_test': 1.7527885580910945, 'R2_test': 0.9894345167481791, 'MAE_test': 1.177123249985424}. 
Saved model results as IQP_Full-CRX_results.json. 
