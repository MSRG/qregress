/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:17 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:18 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:48 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:26 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:05 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2006.0308525562286 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:44 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:33 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:06 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:52 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:35 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2012.0378448963165 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:09 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:45:05 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:50 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:42 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:09 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2157.0801265239716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:21 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:54 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:08 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:58 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:25 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2219.9417452812195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:25 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:53 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:23 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:16:10 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:30 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2225.200659275055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:29 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:55 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:42:07 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:39 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:01:02 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2196.5406897068024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:07:00 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:12:03 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:53 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 21:28:04 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:32 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2129.799879550934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:42:44 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:09 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:37 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:03 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:39 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2245.393302679062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:20:07 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 22:25:25 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:31:23 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:54 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2188.3017086982727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:56:27 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:54 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:37 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:10 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 23:26:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2150.5819342136383 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:32:14 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:37:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 23:54:16 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:02:39 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2182.8564972877502 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:31 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 00:13:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:19:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 00:30:53 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:39:14 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2187.5658991336823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:45:07 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 00:50:31 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:56:38 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:02 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:21 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2167.0356628894806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:06 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 01:26:17 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:32:00 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:41:49 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 01:49:40 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2062.774631023407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:55:34 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:00:33 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:06:10 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 02:16:11 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:24:11 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2061.8288893699646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:29:46 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:34:39 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:40:35 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 02:50:24 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:58:19 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2049.441296339035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:04:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:09:06 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:40 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:24:36 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 03:32:38 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2066.403908252716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:38:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:43:34 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:49:17 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:59:19 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:07:23 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2077.03995347023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:13:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 04:17:59 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:23:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 04:33:49 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:41:37 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2062.0974802970886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:47:24 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 04:52:19 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:57:59 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:07:49 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 05:15:34 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2031.389760017395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:21:11 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:26:09 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:31:46 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:41:34 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 05:49:23 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2024.94269323349 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:55:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:59:58 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:05:30 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 06:15:28 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:23:02 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2016.0858836174011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:28:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 06:33:41 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:39:15 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 06:49:11 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:56:54 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2045.5368750095367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:02:35 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 07:07:29 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:12:55 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:22:47 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 07:30:22 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1998.7516450881958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:36:03 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 07:41:00 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:46:51 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:56:39 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 08:04:27 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2041.6852498054504 seconds. 
Discarding model... 

Training complete taking 52606.34583711624 total seconds. 
Now scoring model... 
Scoring complete taking 2.202908754348755 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (193.24510237542225,), 'R2_train': 0.0637923351235985, 'MAE_train': 12.576626094729802, 'MSE_test': 169.18972262632695, 'R2_test': -0.019844163482825028, 'MAE_test': 11.370996008300859}. 
Saved model results as A1-A1-CNOT_Full-CRX_results.json. 
