/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:56:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:07 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:45 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:15 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:26 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:41 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1864.8189506530762 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:04 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:24 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:28 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:25 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1786.787607908249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:45 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:52 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:56 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:25:01 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1778.8023557662964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:28 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:23 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:35 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:31 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:36 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1771.6163794994354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:43 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:33 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:35 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1737.1453802585602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:28:47 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:34:52 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:55 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:48 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:47 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1749.9403626918793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:01 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:00 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:03 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:05 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:59 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1750.896220445633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:12 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:14 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:16 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:15 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:51:16 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1760.2914426326752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:29 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:25 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:26 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:20 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:20:22 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1746.8544981479645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:36 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:31:43 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:37:39 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 22:43:41 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:40 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1756.071520805359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:49 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:45 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:45 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:41 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:41 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1740.507290840149 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:23:49 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:29:45 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:35:44 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:49 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:53 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1754.463071346283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:53:07 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:08 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:07 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 00:11:06 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:01 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1744.341418504715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:22:15 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:12 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:11 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 00:40:43 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:39 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1854.4522626399994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:53:19 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:45 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:06:18 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 01:12:55 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:19:34 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1916.90229678154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:25:15 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:31:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:38:12 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 01:44:33 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:51:26 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1912.729297876358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:57:20 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:03:53 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:11:02 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 02:18:14 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:25:13 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2033.125473022461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:31:12 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:38:14 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:45:14 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 02:52:21 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:59:35 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2052.1856350898743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:05:28 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:12:24 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:19:23 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 03:26:08 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:33:13 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2013.7090566158295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:39:07 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:46:08 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:15 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 04:00:04 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:06:55 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2038.7201609611511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:13:16 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:20:00 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:26:10 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 04:32:09 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:38:42 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1907.867514848709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:44:46 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:51:50 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:58:33 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 05:05:49 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:12:37 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2027.1764032840729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:18:37 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:25:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:33:03 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 05:40:03 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:47:00 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2070.06706738472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:53:12 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:59:37 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:05:40 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 06:12:04 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:19:10 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1931.7912185192108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:25:20 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:32:31 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:39:31 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Mon Mar 25 06:46:31 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:53:36 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 2064.580178499222 seconds. 
Discarding model... 

Training complete taking 46765.84383392334 total seconds. 
Now scoring model... 
Scoring complete taking 2.411133289337158 seconds. 
Saved predicted values as A2-A2-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (227.63053609532918,), 'R2_train': -0.10279355095043141, 'MAE_train': 13.231385674740682, 'MSE_test': 214.86093294927545, 'R2_test': -0.2951417203558677, 'MAE_test': 12.615998637645161}. 
Saved model results as A2-A2-CZ_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:31:49 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:55 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:59 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:05 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:52:09 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1488.0548040866852 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:56:35 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:39 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:06:42 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:47 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:16:50 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1482.8184945583344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:18 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:23 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:31:27 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 13:36:33 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:41:37 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1485.821979045868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:46:02 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:05 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:56:09 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:15 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:06:21 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1484.294382572174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:10:50 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:16:00 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:06 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 14:26:11 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:18 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1497.1263582706451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:45 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:40:53 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:02 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 14:51:09 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:56:18 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1500.8543293476105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:00:47 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:10:54 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 15:15:57 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:20:59 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1480.9711480140686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:25:27 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:30:31 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:35:35 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:38 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:45:43 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1482.5767681598663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:50:09 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:55:14 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:00:18 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 16:05:23 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:10:27 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1485.1031987667084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:14:55 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:19:59 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:25:04 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 16:30:08 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:35:18 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1489.361670255661 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:39:44 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:44:51 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:50:02 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 16:55:18 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:00:29 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1513.3040771484375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:05:00 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:10:09 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:15:26 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 17:20:35 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:25:49 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1519.8631808757782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:30:21 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:30 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:40:44 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 17:45:51 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:51:00 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1512.3466067314148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:55:33 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:38 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:05:45 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 18:10:54 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:16:07 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1507.0860233306885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:20:42 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:25:56 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:31:10 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 18:36:23 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:41:29 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1520.8458614349365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:46:01 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:51:13 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:56:24 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 19:01:32 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:06:39 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1509.882375717163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:11:09 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:16:17 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:24 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 19:26:31 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:31:42 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1502.254677772522 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:36:10 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:41:14 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:46:21 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 19:51:30 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:56:36 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1493.8327450752258 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:01:02 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:06:09 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:11:16 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 20:16:24 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:21:35 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1497.906913280487 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:26:03 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:31:15 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:36:23 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 20:41:33 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:46:38 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1504.218314409256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:51:08 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:56:21 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:01:28 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 21:06:37 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:11:46 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1507.702428817749 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:16:12 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:21:21 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:26:30 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 21:31:40 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:36:47 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1506.0907154083252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:41:21 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:28 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:51:37 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 21:56:46 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:01:49 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1497.8917894363403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:06:21 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:11:30 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:16:39 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 22:21:58 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:27:12 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1525.3178355693817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:31:48 2024]  Iteration number: 0 with current cost as 0.40206921785744676 and parameters 
[-4.25425889  2.23743495 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552045 -1.06648293  0.60271542  1.14432476
  1.31029914 -1.87354649  0.7296508   2.88578451 -0.54534304 -0.47522454
 -2.02654209  0.72897401  1.60512695  2.83077107 -1.26456694 -0.25136105
 -2.39279218 -2.27309774  3.1333717   2.54856974 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:37:01 2024]  Iteration number: 0 with current cost as 0.32432523471465174 and parameters 
[-4.44706128  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010926
  3.06858469  2.1896016   1.18551998 -1.06648308  0.6027151   1.1443246
  1.31029899 -1.87354651  0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.0265424   0.7289737   1.60512664  2.83077093 -1.26456724 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:42:18 2024]  Iteration number: 0 with current cost as 0.3677723206822532 and parameters 
[-4.3440553   2.23743487 -2.1242794  -0.11653103  0.55388732 -2.77010897
  3.06858475  2.18960169  1.18552022 -1.06648308  0.60271534  1.14432469
  1.31029922 -1.87354633  0.72965104  2.88578443 -0.54534311 -0.47522461
 -2.0265424   0.72897393  1.60512687  2.83077083 -1.26456733 -0.25136105
 -2.39279194 -2.27309774  3.13337179  2.54856982 -0.67550764 -2.69002178]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:32 2024]  Iteration number: 0 with current cost as 0.3637808400363888 and parameters 
[-4.3546006   2.23743479 -2.12427964 -0.11653087  0.55388724 -2.77010897
  3.06858483  2.18960177  1.18552014 -1.06648308  0.6027151   1.14432461
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.02654256  0.7289737   1.60512679  2.83077091 -1.2645671  -0.25136105
 -2.39279202 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:52:42 2024]  Iteration number: 0 with current cost as 0.35971460533124766 and parameters 
[-7.59805129  2.23743527 -2.124279   -0.11653039  0.55388771 -2.77010961
  3.06858435  2.18960209  1.18552062 -1.06648308  0.60271574  1.14432508
  1.31029962 -1.87354553  0.72965017  2.88578483 -0.54534272 -0.47522422
 -2.0265424   0.72897433  1.60512727  2.83077044 -1.2645671  -0.25136041
 -2.39279155 -2.27309711  3.13337218  2.54857022 -0.67550724 -2.69002202]. 
Training complete taking 1528.056406736374 seconds. 
Discarding model... 

Training complete taking 37523.583691358566 total seconds. 
Now scoring model... 
Scoring complete taking 2.101583957672119 seconds. 
Saved predicted values as A2-A2-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (227.63053609532918,), 'R2_train': -0.10279355095043141, 'MAE_train': 13.231385674740682, 'MSE_test': 214.86093294927545, 'R2_test': -0.2951417203558677, 'MAE_test': 12.615998637645161}. 
Saved model results as A2-A2-CZ_Full-CRX_results.json. 
