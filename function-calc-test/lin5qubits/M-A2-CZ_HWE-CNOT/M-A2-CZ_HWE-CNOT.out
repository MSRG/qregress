/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:45:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:39 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 17:55:39 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:06 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 18:04:07 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:40 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 18:17:08 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1932.645471572876 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:36 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:44 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 18:27:45 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:14 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 18:36:13 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:02 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:43 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 18:49:11 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1923.0286138057709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:39 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:56 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 18:59:52 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:19 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 19:08:19 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:08 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:52 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 19:21:12 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.577864408493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:40 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:45 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 19:31:42 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:07 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 19:40:07 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:55 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:35 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 19:52:54 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1903.057496547699 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:22 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 20:03:25 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:52 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 20:11:48 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 20:12:36 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 20:18:13 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 20:24:24 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1890.163289308548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:52 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:55 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 20:34:52 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 20:37:18 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 20:43:12 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:03 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:49 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 20:56:03 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1899.2031517028809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:31 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:35 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 21:06:29 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 21:08:55 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 21:15:08 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 21:15:56 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:43 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 21:28:32 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1948.8835337162018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:29:00 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:05 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 21:38:57 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:23 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 21:47:17 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:46 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 22:00:05 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1893.4039344787598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:33 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:35 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 22:10:26 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:51 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 22:18:40 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 22:19:28 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:06 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 22:31:21 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1874.3618223667145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:49 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:52 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 22:41:45 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:09 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 22:50:03 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 22:50:51 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:28 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 23:02:41 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1881.3898768424988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:09 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:12 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 23:13:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 23:15:32 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 23:21:25 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:13 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 23:28:03 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 23:34:17 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1894.888986825943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:44 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:45 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 23:44:38 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 23:47:04 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 23:52:56 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:44 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 23:59:28 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 00:05:53 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1896.0228173732758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:21 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 00:10:21 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 00:16:16 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 00:18:41 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 00:24:35 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:24 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 00:31:13 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 00:37:32 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1899.1461162567139 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:38:00 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 00:42:03 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 00:47:54 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 00:50:17 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 00:56:43 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:32 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 01:03:40 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 01:09:54 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1941.2066857814789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:10:21 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 01:14:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 01:20:54 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:20 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 01:29:10 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 01:29:59 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:41 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 01:41:56 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1922.0356767177582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:42:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:28 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 01:52:21 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 01:54:48 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 02:01:12 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 02:02:01 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 02:07:38 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 02:13:52 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1917.561429977417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:20 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 02:18:27 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 02:24:20 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:46 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 02:32:40 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:29 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 02:39:15 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 02:45:25 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1893.697907447815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:45:54 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 02:50:01 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 02:56:07 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 02:58:33 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 03:04:25 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 03:05:14 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 03:11:02 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 03:17:38 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1930.4879038333893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:05 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 03:22:13 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 03:28:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 03:30:33 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 03:36:31 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 03:37:19 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 03:42:58 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 03:49:50 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1933.8555552959442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:50:18 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 03:54:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 04:00:17 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 04:02:43 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 04:08:36 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 04:09:24 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 04:15:26 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 04:21:52 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1922.1519935131073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:22:20 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:22 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 04:32:15 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 04:34:39 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 04:40:31 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 04:41:20 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 04:46:58 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 04:53:18 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1884.27694272995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:53:46 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 04:57:48 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 05:03:42 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 05:06:07 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 05:12:04 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 05:12:53 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 05:18:30 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 05:24:44 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1885.8887164592743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:25:12 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 05:29:15 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 05:35:13 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 05:37:38 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 05:43:30 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 05:44:19 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 05:49:59 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 05:56:45 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.998821735382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:57:12 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 06:01:14 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 06:07:07 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 06:09:32 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 06:15:52 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:48 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 06:22:32 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 06:28:45 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.248530626297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:13 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 06:33:14 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 06:39:05 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 06:41:30 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 06:47:24 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 06:48:13 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 06:53:59 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 07:00:22 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1896.7232296466827 seconds. 
Discarding model... 

Training complete taking 47725.908440589905 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9637057781219482 seconds. 
Saved predicted values as M-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (73.9255397258951,), 'R2_train': 0.6418555706159501, 'MAE_train': 6.095121144103766, 'MSE_test': 62.48130724117116, 'R2_test': 0.62337430709417, 'MAE_test': 5.595772223441487}. 
Saved model results as M-A2-CZ_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:22:05 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:20 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 12:26:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 12:32:25 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 12:34:53 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 12:40:51 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:39 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 12:47:26 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 12:53:44 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1914.8501427173615 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:13 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 12:58:22 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 13:04:22 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 13:06:51 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 13:12:46 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:36 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 13:19:16 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 13:25:34 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1907.607179403305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:02 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:08 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 13:36:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 13:38:32 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 13:44:29 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 13:45:17 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 13:51:01 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 13:57:23 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1909.032199382782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:57:49 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 14:01:55 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 14:07:56 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 14:10:21 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 14:16:27 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 14:17:25 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 14:23:10 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 14:29:34 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1931.1648964881897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:30:02 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:08 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 14:40:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 14:42:34 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 14:48:37 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 14:49:27 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 14:55:13 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 15:01:45 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1932.5520958900452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:02:13 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 15:06:33 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 15:12:31 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 15:14:59 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 15:20:58 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:51 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 15:27:37 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 15:33:55 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1930.2599349021912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:34:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 15:38:30 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 15:44:26 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 15:46:56 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 15:53:19 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 15:54:15 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:09 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 16:06:39 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1962.2721993923187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:07:07 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 16:11:12 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 16:17:07 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:34 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 16:25:44 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 16:26:38 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 16:32:42 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 16:39:01 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1943.0976328849792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:39:29 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 16:43:37 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 16:49:35 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:02 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 16:57:56 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 16:58:48 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 17:04:38 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 17:10:55 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1914.6045904159546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:11:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 17:15:29 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 17:21:26 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 17:23:54 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 17:29:52 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 17:30:41 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 17:36:23 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 17:42:42 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1905.2950549125671 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:43:10 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 17:47:16 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 17:53:14 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 17:55:43 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 18:01:40 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 18:02:29 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 18:08:11 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 18:14:28 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1907.1585195064545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:14:56 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 18:19:00 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 18:24:59 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 18:27:27 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 18:33:25 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 18:34:15 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 18:39:58 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 18:46:16 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1907.260062456131 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:46:44 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 18:50:48 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 18:56:51 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 18:59:18 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 19:05:36 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 19:06:26 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 19:12:16 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 19:18:33 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1938.5386283397675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:19:02 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 19:23:06 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 19:29:02 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 19:31:28 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 19:37:24 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 19:38:14 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 19:43:57 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 19:50:15 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1900.5993025302887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:50:44 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 19:54:50 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 20:00:48 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 20:03:16 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 20:09:09 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 20:09:59 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 20:15:44 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 20:22:03 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1908.7205963134766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:22:31 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 20:26:37 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 20:32:35 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 20:35:02 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 20:41:00 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 20:41:49 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 20:47:31 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 20:53:50 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1905.8784832954407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:54:18 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 20:58:24 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 21:04:23 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 21:06:53 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 21:12:52 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 21:13:40 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 21:19:24 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 21:25:44 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1915.9127507209778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:26:13 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 21:30:24 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 21:36:27 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 21:38:54 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 21:44:52 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 21:45:41 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 21:51:30 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 21:57:55 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1928.8494384288788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:58:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 22:02:37 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 22:08:59 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 22:11:27 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 22:17:26 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 22:18:14 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 22:24:19 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 22:30:55 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1980.3913497924805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:31:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:29 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 22:41:32 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 22:44:00 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 22:49:58 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 22:50:47 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 22:56:35 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 23:03:02 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1927.0651745796204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:03:30 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 23:07:42 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 23:13:40 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 23:16:08 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 23:22:04 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 23:22:52 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Thu Apr  4 23:28:38 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Thu Apr  4 23:35:37 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1954.9678559303284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:36:05 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Thu Apr  4 23:40:11 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Thu Apr  4 23:46:31 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Thu Apr  4 23:48:58 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Thu Apr  4 23:55:12 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Thu Apr  4 23:56:01 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Fri Apr  5 00:01:47 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Fri Apr  5 00:08:22 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1967.117593050003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:08:51 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Fri Apr  5 00:13:13 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Fri Apr  5 00:19:17 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Fri Apr  5 00:21:48 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Fri Apr  5 00:27:48 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Fri Apr  5 00:28:37 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Fri Apr  5 00:34:20 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Fri Apr  5 00:40:38 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1933.5017404556274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:41:06 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Fri Apr  5 00:45:12 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Fri Apr  5 00:51:10 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Fri Apr  5 00:53:38 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Fri Apr  5 00:59:37 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Fri Apr  5 01:00:26 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Fri Apr  5 01:06:11 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Fri Apr  5 01:13:10 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1952.1320054531097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:13:38 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Fri Apr  5 01:17:45 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Fri Apr  5 01:23:43 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Fri Apr  5 01:26:11 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Fri Apr  5 01:32:21 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Fri Apr  5 01:33:10 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Fri Apr  5 01:39:41 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Fri Apr  5 01:46:00 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1971.0514867305756 seconds. 
Discarding model... 

Training complete taking 48249.88244223595 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1164350509643555 seconds. 
Saved predicted values as M-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (73.9255397258951,), 'R2_train': 0.6418555706159501, 'MAE_train': 6.095121144103766, 'MSE_test': 62.48130724117116, 'R2_test': 0.62337430709417, 'MAE_test': 5.595772223441487}. 
Saved model results as M-A2-CZ_HWE-CNOT_results.json. 
