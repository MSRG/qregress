/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:45:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:39 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 17:55:39 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:06 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 18:04:07 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:40 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 18:17:08 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1932.645471572876 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:36 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:44 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 18:27:45 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:14 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 18:36:13 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:02 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:43 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 18:49:11 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1923.0286138057709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:39 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:56 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 18:59:52 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:19 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 19:08:19 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:08 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:52 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 19:21:12 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.577864408493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:40 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:45 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 19:31:42 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:07 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 19:40:07 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:55 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:35 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 19:52:54 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1903.057496547699 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:22 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 20:03:25 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:52 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 20:11:48 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 20:12:36 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 20:18:13 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 20:24:24 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1890.163289308548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:52 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:55 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 20:34:52 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 20:37:18 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 20:43:12 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:03 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:49 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 20:56:03 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1899.2031517028809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:31 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:35 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 21:06:29 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 21:08:55 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 21:15:08 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 21:15:56 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:43 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 21:28:32 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1948.8835337162018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:29:00 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:05 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 21:38:57 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:23 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 21:47:17 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:46 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 22:00:05 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1893.4039344787598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:33 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:35 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 22:10:26 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:51 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 22:18:40 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 22:19:28 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:06 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 22:31:21 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1874.3618223667145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:49 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:52 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 22:41:45 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:09 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 22:50:03 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 22:50:51 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:28 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 23:02:41 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1881.3898768424988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:09 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:12 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 23:13:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 23:15:32 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 23:21:25 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:13 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 23:28:03 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Sun Mar 24 23:34:17 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1894.888986825943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:44 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:45 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Sun Mar 24 23:44:38 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Sun Mar 24 23:47:04 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Sun Mar 24 23:52:56 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:44 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Sun Mar 24 23:59:28 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 00:05:53 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1896.0228173732758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:21 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 00:10:21 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 00:16:16 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 00:18:41 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 00:24:35 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:24 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 00:31:13 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 00:37:32 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1899.1461162567139 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:38:00 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 00:42:03 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 00:47:54 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 00:50:17 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 00:56:43 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:32 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 01:03:40 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 01:09:54 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1941.2066857814789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:10:21 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 01:14:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 01:20:54 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:20 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 01:29:10 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 01:29:59 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:41 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 01:41:56 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1922.0356767177582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:42:23 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:28 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 01:52:21 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 01:54:48 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 02:01:12 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 02:02:01 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 02:07:38 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 02:13:52 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1917.561429977417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:20 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 02:18:27 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 02:24:20 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:46 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 02:32:40 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:29 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 02:39:15 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 02:45:25 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1893.697907447815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:45:54 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 02:50:01 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 02:56:07 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 02:58:33 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 03:04:25 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 03:05:14 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 03:11:02 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 03:17:38 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1930.4879038333893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:05 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 03:22:13 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 03:28:06 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 03:30:33 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 03:36:31 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 03:37:19 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 03:42:58 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 03:49:50 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1933.8555552959442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:50:18 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 03:54:26 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 04:00:17 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 04:02:43 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 04:08:36 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 04:09:24 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 04:15:26 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 04:21:52 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1922.1519935131073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:22:20 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:22 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 04:32:15 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 04:34:39 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 04:40:31 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 04:41:20 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 04:46:58 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 04:53:18 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1884.27694272995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:53:46 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 04:57:48 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 05:03:42 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 05:06:07 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 05:12:04 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 05:12:53 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 05:18:30 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 05:24:44 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1885.8887164592743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:25:12 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 05:29:15 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 05:35:13 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 05:37:38 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 05:43:30 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 05:44:19 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 05:49:59 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 05:56:45 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.998821735382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:57:12 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 06:01:14 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 06:07:07 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 06:09:32 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 06:15:52 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:48 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 06:22:32 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 06:28:45 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1920.248530626297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:13 2024]  Iteration number: 0 with current cost as 0.3263349580975624 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.047261    0.56129129 -2.67690681
  3.08188279  2.25056626  1.1278758  -1.01102131  0.6921021   1.2395963
  1.45677345 -1.7761101   0.58718631]. 
Working on 0.4 fold... 
[Mon Mar 25 06:33:14 2024]  Iteration number: 0 with current cost as 0.30513783998360267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0517282   0.56898238 -2.66994931
  3.10575055  2.24706704  1.08872172 -1.01083459  0.68509663  1.23763264
  1.44846889 -1.78017699  0.60048101]. 
[Mon Mar 25 06:39:05 2024]  Iteration number: 50 with current cost as 0.11457991728979394 and parameters 
[-2.90318342  2.23743475 -2.12427965 -0.13077723 -0.82178927 -2.2926373
  3.50010616  2.21143177  1.16485586 -1.14853839  0.56949082 -0.32267846
  2.80205429 -3.28919963 -0.01713483]. 
Working on 0.6 fold... 
[Mon Mar 25 06:41:30 2024]  Iteration number: 0 with current cost as 0.30261803238180174 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.05139235  0.56502327 -2.67583972
  3.09372908  2.2475401   1.10917677 -1.01370496  0.67586834  1.23126304
  1.43692622 -1.78935562  0.60633666]. 
[Mon Mar 25 06:47:24 2024]  Iteration number: 50 with current cost as 0.11030261191689143 and parameters 
[-2.90318328  2.2374347  -2.12427954 -0.03724304 -1.02962122 -2.41574455
  3.6209719   2.39878102  1.29348417 -1.23972991  0.95538119 -0.35004534
  2.79019612 -3.18500179  0.08137442]. 
Working on 0.8 fold... 
[Mon Mar 25 06:48:13 2024]  Iteration number: 0 with current cost as 0.3049912794817481 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.04814284  0.56481956 -2.67234236
  3.10060612  2.24791844  1.09710561 -1.00783176  0.68263825  1.24050911
  1.44204323 -1.78549837  0.60298273]. 
Working on 1.0 fold... 
[Mon Mar 25 06:53:59 2024]  Iteration number: 0 with current cost as 0.30663999918408735 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.04987997  0.56711388 -2.67074251
  3.09654012  2.25130121  1.10218871 -1.01577328  0.69295305  1.234094
  1.44711059 -1.78335526  0.59365873]. 
[Mon Mar 25 07:00:22 2024]  Iteration number: 50 with current cost as 0.11973150889275161 and parameters 
[-2.90318343  2.23743471 -2.12428057  0.07471056 -1.01494675 -2.39974728
  3.48659384  2.2326277   1.1788877  -1.27010878  0.93212538 -0.23991727
  2.72577407 -3.25077246 -0.05209478]. 
Training complete taking 1896.7232296466827 seconds. 
Discarding model... 

Training complete taking 47725.908440589905 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9637057781219482 seconds. 
Saved predicted values as M-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (73.9255397258951,), 'R2_train': 0.6418555706159501, 'MAE_train': 6.095121144103766, 'MSE_test': 62.48130724117116, 'R2_test': 0.62337430709417, 'MAE_test': 5.595772223441487}. 
Saved model results as M-A2-CZ_HWE-CNOT_results.json. 
