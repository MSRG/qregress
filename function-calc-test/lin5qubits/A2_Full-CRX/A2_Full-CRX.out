/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:57 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:43 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:24 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:59 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:36 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1838.0762996673584 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:49 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:31 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:34 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:17 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1840.261414527893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:30 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:02 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:22 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:16 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1864.5926387310028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:35 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:30 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:27:07 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:08 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1913.505863904953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:30 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:48:18 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:56 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:55 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1900.719108581543 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:08 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:03 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:55 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:20 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:15 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1880.9918792247772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:31 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:32 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:17 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:41 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:35 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1882.320340871811 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:46 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:48 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:47 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:48 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1875.1805582046509 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:23 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:13 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:26 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:31 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:40 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1974.8032989501953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:18 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:22:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:27:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:37:51 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:42:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1923.9197120666504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:49:02 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:53:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:58:48 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:19 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:03 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1873.046094417572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:15 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:23 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:30:11 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:40:54 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:44 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1900.9367861747742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:58 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:56:44 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:01:37 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:12:19 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:10 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1887.5518407821655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:32 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:33:28 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:43:49 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:48:46 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1897.2844495773315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:56 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:00:05 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:05:10 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:15:57 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:20:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1918.2422771453857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:27:18 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:32:00 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:37:07 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:48:00 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:53:01 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1946.9826874732971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:59:30 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:04:29 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:09:17 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:19:56 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:24:45 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1888.4348368644714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:30:59 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:59 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:40:59 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:51:31 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:41 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1918.719687461853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:02:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:47 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:12:46 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:23:37 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:28:25 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1905.9480493068695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:35:07 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:39:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:44:33 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:55:07 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:59:47 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1888.1409356594086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:06:05 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:15:50 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:25:57 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 04:30:46 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1859.2632775306702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:37:04 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:41:56 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:46:48 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:57:02 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:01:41 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1852.1329085826874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:08:07 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:12:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:17:30 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:27:35 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:24 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1834.9939773082733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:36 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:43:29 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:48:43 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:59:27 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:04:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1925.985464334488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:10:44 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:15:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:20:21 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:31:09 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:35:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1894.61319565773 seconds. 
Discarding model... 

Training complete taking 47286.64851284027 total seconds. 
Now scoring model... 
Scoring complete taking 2.754962921142578 seconds. 
Saved predicted values as A2_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:32:17 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:36:13 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:10 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 11:48:33 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 11:52:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1521.314043045044 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:57:36 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:01:31 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:05:26 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:13:51 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 12:17:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1518.5009303092957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:26:46 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:30:44 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:09 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:05 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1516.4792323112488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:10 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:52:06 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:01 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:04:25 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 13:08:21 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1514.791919708252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:13:25 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:17:22 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:17 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:41 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 13:33:34 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1512.2294540405273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:38:33 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:46:23 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:56 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1525.817664861679 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:07 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:08:06 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:12:09 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:46 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 14:24:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1551.2146360874176 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:57 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:59 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:38:04 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:46:40 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 14:50:45 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1556.99906873703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:55:53 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:59:52 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:03:52 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:12:20 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 15:16:15 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1529.0273797512054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:21:20 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:25:19 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:29:15 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:37:41 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 15:41:36 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1518.792175769806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:46:41 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:50:45 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:45 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:03:27 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 16:07:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1558.8781311511993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:12:47 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:20:50 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:29:26 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 16:33:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1558.8427865505219 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:38:43 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:42:42 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:46:43 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:55:15 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 16:59:23 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1554.0703327655792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:04:43 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:08:56 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:12:57 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:21:40 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 17:25:45 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1581.313850402832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:30:57 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:34:57 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:38:59 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:47:56 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 17:52:05 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1577.8990886211395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:57:21 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:01:29 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:05:39 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:14:17 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 18:18:27 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1586.2413778305054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:23:44 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:27:52 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:32:02 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:40:54 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 18:45:01 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1588.0602259635925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:50:14 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:54:22 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:58:23 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:07:11 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 19:11:15 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1578.9188661575317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:16:32 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:20:28 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:24:25 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:32:52 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 19:36:46 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1527.9825370311737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:41:50 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:45:46 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:49:41 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:58:04 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 20:02:00 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1513.1626780033112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:07:07 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:11:21 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:15:28 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:24:38 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 20:29:07 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1637.7215540409088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:34:44 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:39:01 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:43:28 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:52:55 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 20:57:21 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1694.9448673725128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:02:56 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:07:18 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:11:49 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:21:05 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 21:25:04 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1653.7255692481995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:30:12 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:34:11 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:38:25 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:47:48 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 21:52:16 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1644.8479220867157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:57:57 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:02:23 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:06:44 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:16:13 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 22:20:48 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1709.8286368846893 seconds. 
Discarding model... 

Training complete taking 39231.60569906235 total seconds. 
Now scoring model... 
Scoring complete taking 1.9613816738128662 seconds. 
Saved predicted values as A2_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRX_results.json. 
