test.sh: line 9: python: command not found
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_train.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_train.bin into X and y data. 
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_test.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_test.bin into X and y data. 
Training model with dataset /home/grierjones/qregress/function-calc-test/linear/linear_train.bin 
 at time Fri Mar 22 05:45:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 22 05:46:54 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 05:48:32 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 05:50:08 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 05:53:35 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 05:55:12 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 622.7005205154419 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 22 05:57:16 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 05:58:52 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:00:28 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:03:55 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:05:32 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 620.5750048160553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 22 06:07:36 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 06:09:13 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:10:50 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:14:16 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:15:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 620.0996835231781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 22 06:17:57 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 06:19:34 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:21:11 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:24:40 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:26:17 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 625.147034406662 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 22 06:28:23 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 06:30:00 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:31:38 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:35:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:36:45 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 628.156733751297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 22 06:38:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 06:40:28 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:42:06 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:45:35 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:47:13 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 627.6507203578949 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 22 06:49:18 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 22 06:50:55 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 06:52:33 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 06:56:02 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 06:57:39 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 626.1153016090393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 22 06:59:44 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:01:21 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 07:02:58 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 07:06:26 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 07:08:03 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 624.1657223701477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 22 07:10:09 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:11:46 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 07:13:23 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 07:16:52 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 07:18:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 625.4242944717407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 22 07:20:34 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:22:12 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.6 fold... 
[Fri Mar 22 07:23:51 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 07:27:23 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 07:29:01 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 632.8760335445404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 22 07:31:08 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:32:46 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 07:34:24 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 07:37:54 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 07:39:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 630.4339842796326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 22 07:41:38 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:43:16 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 07:44:54 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 07:48:24 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 07:50:02 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 630.2888858318329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 22 07:52:08 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 07:53:46 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 07:55:24 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Fri Mar 22 07:58:52 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 08:00:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 627.9260718822479 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 22 08:02:35 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:04:12 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:05:49 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 08:09:17 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 08:10:54 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 623.4128220081329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 22 08:12:58 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:14:35 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:16:11 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 08:19:39 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 08:21:17 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 623.1333911418915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 22 08:23:21 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:24:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:26:34 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 08:30:01 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 1.0 fold... 
[Fri Mar 22 08:31:37 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 620.1068329811096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 22 08:33:42 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:35:19 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:36:56 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 08:40:23 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 08:41:59 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 622.1540338993073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 22 08:44:04 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:45:41 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:47:18 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 08:50:45 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 08:52:23 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 623.0403778553009 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 22 08:54:27 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 08:56:04 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 08:57:41 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:01:09 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:02:46 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 623.4500052928925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 22 09:04:50 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 09:06:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 09:08:03 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:11:31 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:13:08 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 622.4561595916748 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 22 09:15:13 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 09:16:50 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 09:18:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:21:54 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:23:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 622.9540989398956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 22 09:25:36 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 09:27:13 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 09:28:50 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:32:18 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:33:55 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 624.0441045761108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 22 09:36:00 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 22 09:37:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 09:39:14 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:42:41 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:44:18 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 623.2062044143677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 22 09:46:23 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 09:48:00 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 09:49:37 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 09:53:06 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 09:54:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 624.121237039566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 22 09:56:47 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 22 09:58:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 22 10:00:02 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Mar 22 10:03:31 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Fri Mar 22 10:05:07 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 624.772186756134 seconds. 
Discarding model... 

Training complete taking 15618.411763906479 total seconds. 
Now scoring model... 
Scoring complete taking 0.7955644130706787 seconds. 
Saved predicted values as A2_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRX_results.json. 
