test.sh: line 9: python: command not found
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_train.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_train.bin into X and y data. 
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_test.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_test.bin into X and y data. 
Training model with dataset /home/grierjones/qregress/function-calc-test/linear/linear_train.bin 
 at time Mon Mar 18 05:56:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 18 05:56:33 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 05:57:21 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 05:58:15 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 05:59:27 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:00:20 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 286.95750617980957 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 18 06:01:19 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:02:07 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:03:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:04:13 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:05:07 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 286.71719694137573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 18 06:06:06 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:06:54 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:07:48 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:09:00 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:09:54 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 287.52506613731384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 18 06:10:54 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:11:42 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:12:36 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:13:48 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:14:42 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 287.8991148471832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 18 06:15:42 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:16:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:17:24 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Mon Mar 18 06:18:35 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:19:28 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 284.9571421146393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 18 06:20:27 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:21:14 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:22:07 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:23:18 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:24:11 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 283.4539740085602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 18 06:25:10 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:25:57 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:26:51 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:28:02 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:28:56 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 284.87118697166443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 18 06:29:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:30:42 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:31:35 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:32:45 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:33:39 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 283.04871702194214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 18 06:34:38 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:35:25 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:36:18 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:37:29 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:38:22 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 283.07541680336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 18 06:39:21 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:40:08 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:41:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Mon Mar 18 06:42:11 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:43:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.1185374259949 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 18 06:44:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:44:50 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:45:43 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:46:53 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:47:46 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.4488501548767 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 18 06:48:45 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:49:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:50:25 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:51:35 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:52:27 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 280.61463260650635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 18 06:53:26 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:54:13 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:55:06 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 06:56:17 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 06:57:10 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.57102155685425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 18 06:58:08 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 06:58:55 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 06:59:47 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:00:58 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:01:50 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 280.2683799266815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 18 07:02:49 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:03:35 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:04:28 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Mon Mar 18 07:05:39 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:06:32 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 281.86566972732544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 18 07:07:31 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:08:17 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:09:10 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:10:20 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:11:13 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 281.1169857978821 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 18 07:12:12 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:12:59 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:13:52 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:15:03 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:15:56 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 283.49150109291077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 18 07:16:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:17:42 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:18:35 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:19:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:20:38 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 281.97646164894104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 18 07:21:37 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:22:24 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:23:17 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:24:27 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:25:20 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.32188606262207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 18 07:26:20 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:27:07 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:28:00 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Mon Mar 18 07:29:11 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:30:03 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.3438436985016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 18 07:31:02 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:31:49 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:32:42 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:33:53 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:34:46 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 283.32815885543823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 18 07:35:45 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:36:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:37:25 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:38:35 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:39:28 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 281.10852694511414 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 18 07:40:26 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:41:13 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:42:06 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:43:16 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:44:08 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 280.62600231170654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 18 07:45:07 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:45:54 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:46:47 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 18 07:47:58 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:48:51 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.8560390472412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 18 07:49:50 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 18 07:50:37 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Mon Mar 18 07:51:30 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Mon Mar 18 07:52:40 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 18 07:53:33 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 282.6395356655121 seconds. 
Discarding model... 

Training complete taking 7080.201662540436 total seconds. 
Now scoring model... 
Scoring complete taking 0.7433962821960449 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641349602324,), 'R2_train': -0.03877556182668673, 'MAE_train': 12.53180210339704, 'MSE_test': 182.38628763166471, 'R2_test': -0.09939060158675028, 'MAE_test': 11.257106544005005}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
