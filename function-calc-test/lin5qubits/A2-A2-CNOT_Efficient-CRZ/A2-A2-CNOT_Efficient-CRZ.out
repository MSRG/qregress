/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:51:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:09 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:11 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:22 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 846.5265970230103 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:12 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:06 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:11 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 830.1965746879578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:01 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:21 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:56 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:28 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:07 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 841.7212400436401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:07 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:26 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:00 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:30 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 832.9892535209656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:56 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:14 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:15 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:48 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 826.591019153595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:46 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:06 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:44 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:16 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:56 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 843.4935793876648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:50 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:10 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:18 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:52 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.2515761852264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:51 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:06 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:43 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:08 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:40 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 823.4759180545807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:34 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:58 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:36 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:11 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:53 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 849.9392659664154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:46 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:09 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:03:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:13 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:53 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.7806425094604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:17 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:54 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:25 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:58 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 843.482807636261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:56 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:19 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:32:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:35:28 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:38:10 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 850.7600374221802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:08 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:43:28 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:46:05 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:38 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:20 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 847.15558385849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:55:13 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:10 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:03:40 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:22 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 846.4204363822937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:23 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:11:44 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:14:21 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:50 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:28 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 847.51172041893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:23:25 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:25:48 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:28:23 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:31:52 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:30 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 836.3816106319427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:22 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:42 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:17 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:50 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:41 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 855.2867481708527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:39 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:54:01 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:37 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:00:09 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:00 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 861.7706527709961 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:02 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:22 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:03 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:17:13 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 850.2965066432953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:20:07 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:22:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:29 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 828.4626984596252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:14 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:50 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:18 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 854.2206840515137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:15 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:50:38 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:21 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:59:18 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.1146986484528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:02:23 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:55 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:40 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:18 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 885.1027717590332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:20 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:21:57 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:27 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:28:03 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 841.7689638137817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:13 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:39:47 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:42:30 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 867.2689235210419 seconds. 
Discarding model... 

Training complete taking 21143.971536636353 total seconds. 
Now scoring model... 
Scoring complete taking 2.100876808166504 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641349602324,), 'R2_train': -0.03877556182668673, 'MAE_train': 12.53180210339704, 'MSE_test': 182.38628763166471, 'R2_test': -0.09939060158675028, 'MAE_test': 11.257106544005005}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
