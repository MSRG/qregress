/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:23 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:36:58 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:06 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:19 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1556.9109098911285 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:20 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:03:00 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:10 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:19 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:49 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1559.8713681697845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:20 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:29:03 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:11 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:20 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:45 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1557.0767440795898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:18 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:54:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:57 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:18 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:03 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1562.5609257221222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:21 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:20:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:02 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:20 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:02 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:09 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1577.6643764972687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:36 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:47:08 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:14 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:23 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1532.7728698253632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:06:11 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:12:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:31 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:43 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:23 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:08 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1585.7307879924774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:35 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:39:14 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:19 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:45:23 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:53:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1537.3333611488342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:14 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:05:12 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:18 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:11:21 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:07 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:00 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1578.3413915634155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:24:31 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:31:04 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:09 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:14 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:57 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:45:56 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1584.1727256774902 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:50:56 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:57:27 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:33 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:37 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:18 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:55 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1526.8883237838745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:16:23 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:22:55 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 22:24:00 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:04 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:26 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1529.996855020523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:41:54 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:48:37 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 22:49:42 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:54:47 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:30 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:03:08 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1540.881575345993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:07:34 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:14:15 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:20 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:20:25 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:09 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:29:18 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1575.1641516685486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:48 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:40:46 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 23:41:51 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:46:57 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:37 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:55:19 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1558.3186469078064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:59:46 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:06:17 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:07:22 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:25 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:08 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1529.282236814499 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:25:16 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:31:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:33:02 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:38:08 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:39:49 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:56 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1568.4701890945435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:24 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:57:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:03 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:04:10 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:05:53 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:12:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1550.576459646225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:17:15 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:23:47 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:53 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:05 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:23 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1612.6577224731445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:44:09 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:50:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 01:51:57 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:57:04 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:58:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1581.319281578064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:10:29 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:17:08 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 02:18:13 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:23:30 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:13 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:32:18 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1577.1431934833527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:36:47 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:43:19 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 02:44:25 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:49:32 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:51:15 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:57:54 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1541.2439880371094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:02:27 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:09:28 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 03:10:33 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:15:52 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:17:48 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:24:27 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1593.2799520492554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:29:00 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:36:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 03:37:06 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:42:31 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:19 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:50:59 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1586.7712337970734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:55:27 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:02:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 04:03:07 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:08:17 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:09:58 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:36 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1537.3153541088104 seconds. 
Discarding model... 

Training complete taking 39041.74767756462 total seconds. 
Now scoring model... 
Scoring complete taking 0.8357174396514893 seconds. 
Saved predicted values as A2_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (27.60112075639812,), 'R2_train': 0.8662818333107439, 'MAE_train': 3.711572352293895, 'MSE_test': 30.963562541948228, 'R2_test': 0.8133574070052433, 'MAE_test': 3.5833578004922813}. 
Saved model results as A2_Full-Pauli-CRZ_results.json. 
