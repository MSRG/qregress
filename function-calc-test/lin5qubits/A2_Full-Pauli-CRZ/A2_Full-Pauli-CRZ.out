/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:23 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:36:58 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:06 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:19 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1556.9109098911285 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:20 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:03:00 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:10 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:19 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:49 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1559.8713681697845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:20 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:29:03 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:11 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:20 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:01 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:45 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1557.0767440795898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:18 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:54:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:57 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:18 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:03 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1562.5609257221222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:21 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:20:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:02 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:20 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:02 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:09 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1577.6643764972687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:36 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:47:08 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:14 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:23 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1532.7728698253632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:06:11 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:12:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:31 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:43 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:23 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:08 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1585.7307879924774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:35 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:39:14 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:19 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:45:23 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:53:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1537.3333611488342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:14 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:05:12 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:18 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:11:21 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:07 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:00 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1578.3413915634155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:24:31 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:31:04 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:09 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:14 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:57 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:45:56 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1584.1727256774902 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:50:56 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:57:27 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:33 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:37 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:18 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:55 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1526.8883237838745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:16:23 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:22:55 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 22:24:00 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:04 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:26 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1529.996855020523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:41:54 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:48:37 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 22:49:42 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:54:47 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:30 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:03:08 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1540.881575345993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:07:34 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:14:15 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:20 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:20:25 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:09 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:29:18 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1575.1641516685486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:48 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:40:46 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Sun Mar 24 23:41:51 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:46:57 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:37 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:55:19 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1558.3186469078064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:59:46 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:06:17 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:07:22 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:25 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:08 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:46 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1529.282236814499 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:25:16 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:31:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:33:02 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:38:08 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:39:49 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:56 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1568.4701890945435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:24 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:57:56 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:03 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:04:10 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:05:53 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:12:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1550.576459646225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:17:15 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:23:47 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:53 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:05 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:23 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1612.6577224731445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:44:09 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:50:52 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 01:51:57 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:57:04 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:58:46 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:47 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1581.319281578064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:10:29 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:17:08 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 02:18:13 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:23:30 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:13 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:32:18 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1577.1431934833527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:36:47 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:43:19 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 02:44:25 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:49:32 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:51:15 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:57:54 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1541.2439880371094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:02:27 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:09:28 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 03:10:33 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:15:52 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:17:48 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:24:27 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1593.2799520492554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:29:00 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:36:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 03:37:06 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:42:31 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:19 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:50:59 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1586.7712337970734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:55:27 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:02:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Mon Mar 25 04:03:07 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:08:17 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:09:58 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:36 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1537.3153541088104 seconds. 
Discarding model... 

Training complete taking 39041.74767756462 total seconds. 
Now scoring model... 
Scoring complete taking 0.8357174396514893 seconds. 
Saved predicted values as A2_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (27.60112075639812,), 'R2_train': 0.8662818333107439, 'MAE_train': 3.711572352293895, 'MSE_test': 30.963562541948228, 'R2_test': 0.8133574070052433, 'MAE_test': 3.5833578004922813}. 
Saved model results as A2_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:46 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:00 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 11:49:41 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:53 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:56:00 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:57:43 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:04:28 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1557.7993829250336 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:58 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:15:32 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 12:16:39 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:44 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:23:25 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:04 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1542.29527759552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:34:40 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:41:20 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:28 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:47:34 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:49:17 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:56:18 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1567.953959941864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:49 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:07:40 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:47 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:52 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:34 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:22:11 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1552.803718805313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:41 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:33:12 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 13:34:20 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:24 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:05 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:53 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1539.5841648578644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:52:25 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:58:57 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 14:00:02 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:05:08 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:06:50 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:30 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1532.101393699646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:54 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:24:26 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 14:25:30 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:30:37 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:25 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:39:09 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1549.6732060909271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:43:42 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:50:16 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 14:51:22 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:56:26 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:06 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:04:48 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1545.9155011177063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:29 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:15:57 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 15:17:15 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:22:19 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:24:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:30:45 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1547.6122543811798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:35:15 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:41:47 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 15:42:54 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:48:02 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:49:42 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:56:20 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1542.8290946483612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:01:00 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:07:33 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 16:08:41 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:13:50 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:15:32 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:22:06 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1532.9490795135498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:26:33 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:33:05 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 16:34:10 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:16 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:41:17 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:48:00 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1553.924660205841 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:52:25 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:59:18 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 17:00:23 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:05:26 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:07:08 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:48 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1550.3607971668243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:18:15 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:25:23 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 17:26:33 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:31:34 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:33:15 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:39:55 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1602.479606628418 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:45:02 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:51:37 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 17:52:42 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:57:46 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:59:27 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:06:27 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1556.7491629123688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:10:55 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:17:34 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 18:18:40 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:24:22 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:26:07 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:32:56 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1598.3134899139404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:37:35 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:44:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 18:45:10 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:50:14 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:51:54 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:58:36 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1532.2089579105377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:03:07 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:09:46 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 19:10:51 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:15:55 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:17:38 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:24:14 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1533.8505935668945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:28:40 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:35:09 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 19:36:14 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:41:18 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:43:04 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:49:53 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1544.0163567066193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:54:23 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:00:53 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 20:01:59 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:07:03 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:08:45 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:15:25 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1527.7191398143768 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:19:52 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:26:29 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 20:27:34 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:32:35 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:34:17 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:41:02 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1539.462346792221 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:45:30 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:52:18 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 20:53:24 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:58:57 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:00:37 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:07:19 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1578.6409847736359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:11:51 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:18:21 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 21:19:26 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:24:32 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:26:12 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:32:50 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1529.0518624782562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:37:20 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:44:01 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 21:45:05 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:50:07 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:51:50 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:58:29 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1537.469626903534 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:02:57 2024]  Iteration number: 0 with current cost as 0.1058119292873369 and parameters 
[-2.97731414  1.87102973 -2.08630687 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.27181127  1.14432444
  1.44190366 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:09:27 2024]  Iteration number: 50 with current cost as 0.04980326073531717 and parameters 
[-4.09846047  1.5658298  -1.45286688 -0.11655942  0.55388101 -2.77009914
  3.06855656  2.18960793  1.18553446 -1.06650277 -0.06244126  1.14432987
  1.60999603 -1.87355922  0.72965765  2.88576015 -0.54535957 -0.47524022
 -2.02653973  0.72896322  1.60512168  2.83080031 -1.26456667 -0.25137552]. 
Working on 0.4 fold... 
[Thu Apr  4 22:10:32 2024]  Iteration number: 0 with current cost as 0.1520652366983095 and parameters 
[-3.19436037  1.91819414 -2.08663793 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.31729536  1.14432445
  1.67709987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:16:05 2024]  Iteration number: 0 with current cost as 0.1157591384803876 and parameters 
[-3.10671578  1.90234764 -2.08368629 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.3041314   1.14432445
  1.58246756 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:18:03 2024]  Iteration number: 0 with current cost as 0.11506291848343225 and parameters 
[-3.09246515  1.88258421 -2.08316689 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.28549355  1.14432446
  1.5697275  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:24:40 2024]  Iteration number: 0 with current cost as 0.12801732892664347 and parameters 
[-3.12345774  1.90778201 -2.08621104 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.30787436  1.14432446
  1.59999537 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 1582.7370278835297 seconds. 
Discarding model... 

Training complete taking 38778.50383543968 total seconds. 
Now scoring model... 
Scoring complete taking 0.8283486366271973 seconds. 
Saved predicted values as A2_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (27.60112075639812,), 'R2_train': 0.8662818333107439, 'MAE_train': 3.711572352293895, 'MSE_test': 30.963562541948228, 'R2_test': 0.8133574070052433, 'MAE_test': 3.5833578004922813}. 
Saved model results as A2_Full-Pauli-CRZ_results.json. 
