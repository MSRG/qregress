/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:28 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:50 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:14 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:07 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:15:56 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3233.7127487659454 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:05 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:48 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:39 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:17 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:14 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3264.5631861686707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:32 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:09 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:45:28 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:43 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:51 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3359.8468697071075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:36 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:20 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:45 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:50:08 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:59:38 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3201.579912185669 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:01 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:22:26 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:34:15 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:01 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:54:01 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3297.379423379898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:07:55 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:54 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:59 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:40:03 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:06 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3373.200520515442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:04 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:13:55 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:26:13 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:04 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:46:04 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3361.6990442276 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:00:10 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:10:19 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:21:55 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:32:00 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:13 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3368.3489503860474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:56:29 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:06:02 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:18:04 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:27:49 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:38:00 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3322.193078517914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:56 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:02:13 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:14:32 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:24:36 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:34:38 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3418.5708565711975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:48:46 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:59:02 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:10:54 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:20:54 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:30:44 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3367.6630425453186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:05 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:54:58 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:06:37 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:16:22 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:26:18 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3324.4885256290436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:40:20 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:49:47 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:01:22 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:10:55 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:21:10 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3291.537752389908 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:35:22 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:45:45 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:58:02 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:07:48 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:17:57 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3440.397626876831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:32:40 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:43:13 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:55:47 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:06:13 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:16:30 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3508.8475608825684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:31:19 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:41:42 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:54:20 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:04:55 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:15:41 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3547.870492219925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 08:30:23 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:40:45 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:52:44 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:02:41 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:12:44 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3385.3160297870636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 09:26:29 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:37:00 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:49:04 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:58:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:08:56 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3384.7419056892395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:22:59 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:33:04 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:45:10 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:55:32 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:06:03 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3437.540449619293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:20:22 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:30:10 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:41:56 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 11:51:54 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:01:39 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3331.4524726867676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 12:15:59 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 12:25:53 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 12:38:14 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 12:48:24 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:58:43 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3436.773994922638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 13:13:23 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 13:23:33 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 13:36:22 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 13:46:46 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:57:22 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3533.0309584140778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 14:12:01 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 14:22:24 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 14:34:04 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 14:42:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 14:52:03 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3193.3093132972717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 15:04:48 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 15:14:02 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 15:25:03 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 15:35:13 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 15:45:23 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3265.2107634544373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 15:59:56 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 16:10:12 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 16:22:23 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 16:32:32 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 16:42:51 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3452.2368693351746 seconds. 
Discarding model... 

Training complete taking 84101.51306414604 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.5091984272003174 seconds. 
Saved predicted values as M-M-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (138.55034071437586,), 'R2_train': 0.32877009893869913, 'MAE_train': 10.90998099702426, 'MSE_test': 127.74699642063811, 'R2_test': 0.2299648780098158, 'MAE_test': 9.746227604530995}. 
Saved model results as M-M-CNOT_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:24 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:56:12 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:07:04 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:16:21 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:06 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3021.53853225708 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:45 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:29 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:57:10 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:28 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:25 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3019.3550901412964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:28:06 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:25 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:48:39 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:36 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:07:07 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3098.34996008873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:19:44 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:28:52 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:39:49 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:49:04 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:58:04 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3088.1366856098175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:11:13 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:20:22 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:11 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:24 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:49:39 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3091.3741466999054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:02:34 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:11:25 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:22:19 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:31:24 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:40:17 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3010.761148929596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:52:54 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:02:05 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:13:19 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:22:19 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:08 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3044.4455919265747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:43:44 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:52:45 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:03:38 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:12:38 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:21:32 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3045.47442817688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:34:20 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:43:20 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:54:10 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:03:13 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:12:01 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3019.2838509082794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:24:32 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:33:17 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:43:51 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:52:40 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:01:36 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 2984.8118724823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:14:16 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:23:36 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:52 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:43:56 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:52:57 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3081.9202229976654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:05:49 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:15:24 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:26:30 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:35:40 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:44:50 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3117.431919813156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:57:43 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:07:01 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:18:29 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:27:58 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:37:31 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3168.088367700577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:42 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:00:21 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:11:44 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:21:07 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:30:43 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3195.193617105484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:43:59 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:53:16 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:04:08 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:13:22 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:22:39 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3091.581017255783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:35:18 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:44:20 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:55:19 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:04:26 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:13:31 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3047.5419659614563 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:26:14 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:35:29 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:46:39 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:55:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:05:19 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3129.726050376892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:18:17 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:27:29 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:38:45 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:47:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:57:14 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3107.799940109253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:10:15 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:19:34 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:30:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:39:53 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:49:04 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3117.149715423584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:02:04 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:11:13 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:22:13 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:31:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:41:26 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3146.212259054184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:54:42 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:04:06 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:15:09 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:24:22 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:33:31 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3105.0760271549225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:46:10 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:55:28 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:06:28 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:15:39 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:25:02 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3099.5338695049286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:38:02 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:47:24 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:59:00 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:08:26 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:17:56 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3168.5843274593353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:30:42 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:40:14 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:52:00 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:01:31 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:11:15 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3226.7766451835632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:24:45 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:33:58 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:44:49 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:53:46 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:03:04 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3099.1099441051483 seconds. 
Discarding model... 

Training complete taking 77325.25788807869 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.7046866416931152 seconds. 
Saved predicted values as M-M-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (138.55034071437586,), 'R2_train': 0.32877009893869913, 'MAE_train': 10.90998099702426, 'MSE_test': 127.74699642063811, 'R2_test': 0.2299648780098158, 'MAE_test': 9.746227604530995}. 
Saved model results as M-M-CNOT_Full-CRX_results.json. 
