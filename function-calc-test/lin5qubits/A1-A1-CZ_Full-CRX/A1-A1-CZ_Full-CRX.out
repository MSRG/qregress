test.sh: line 9: python: command not found
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_train.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_train.bin into X and y data. 
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_test.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_test.bin into X and y data. 
Training model with dataset /home/grierjones/qregress/function-calc-test/linear/linear_train.bin 
 at time Fri Mar 15 11:41:46 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 11:43:03 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 11:45:36 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 11:47:23 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 11:49:40 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 11:51:58 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.1390998363495 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 11:54:15 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 11:56:46 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 11:58:32 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:00:48 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:03:04 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 665.5214321613312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 12:05:20 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 12:07:51 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 12:09:36 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:11:54 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:14:10 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 667.6163716316223 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 12:16:27 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 12:18:58 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 12:20:43 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:23:00 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:25:16 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 665.0147924423218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 12:27:32 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 12:30:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 12:31:51 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:34:07 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:36:23 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 667.0809915065765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 12:38:39 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 12:41:10 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 12:42:56 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:45:11 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:47:28 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 664.4435141086578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 12:49:43 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 15 12:52:14 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 12:53:59 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 12:56:15 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 12:58:31 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 662.9242300987244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 13:00:46 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:03:16 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 13:05:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 13:07:17 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 13:09:34 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 662.9890439510345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 13:11:50 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:14:20 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 13:16:06 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 13:18:23 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 13:20:39 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 665.1657285690308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 13:22:56 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:25:29 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.6 fold... 
[Fri Mar 15 13:27:16 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 13:29:33 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 13:31:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.2310240268707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 13:34:07 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:36:39 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 13:38:27 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 13:40:45 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 13:43:04 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 674.217280626297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 13:45:22 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:47:56 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 13:49:43 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 13:52:00 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 13:54:19 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 674.8977842330933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 13:56:37 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 13:59:11 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:00:59 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Fri Mar 15 14:03:17 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 14:05:36 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 676.3812148571014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 14:07:54 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 14:10:26 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:12:13 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 14:14:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 14:16:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.7479338645935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 14:19:04 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 14:21:34 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:23:19 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 14:25:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 14:27:53 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 664.9518768787384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 14:30:10 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 14:32:43 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:34:30 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 14:36:47 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 1.0 fold... 
[Fri Mar 15 14:39:04 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.7643702030182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 14:41:22 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 14:43:55 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:45:42 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 14:48:00 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 14:50:18 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 673.2680253982544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 14:52:35 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 14:55:07 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 14:56:54 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 14:59:11 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:01:28 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 670.5761308670044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 15:03:45 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 15:06:17 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 15:08:04 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 15:10:22 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:12:40 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 671.6679542064667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 15:14:57 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 15:17:29 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 15:19:16 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 15:21:32 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:23:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 670.1294121742249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 15:26:07 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 15:28:40 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 15:30:27 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 15:32:46 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:35:04 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 674.3513355255127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 15:37:22 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 15:39:57 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 15:41:44 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 15:44:02 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:46:20 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 675.5644936561584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 15:48:37 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 15 15:51:11 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 15:52:58 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 15:55:17 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 15:57:35 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 675.557788848877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 15:59:53 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 16:02:27 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 16:04:14 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 16:06:32 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 16:08:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 674.5378017425537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 16:11:07 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 16:13:40 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 16:15:28 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 16:17:48 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 16:20:10 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 681.9432129859924 seconds. 
Discarding model... 

Training complete taking 16766.683161258698 total seconds. 
Now scoring model... 
Scoring complete taking 0.922271728515625 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (206.93879274915722,), 'R2_train': -0.0025489989166755933, 'MAE_train': 12.57727151830348, 'MSE_test': 191.37228833007708, 'R2_test': -0.15355654159227172, 'MAE_test': 12.013220427830294}. 
Saved model results as A1-A1-CZ_Full-CRX_results.json. 
