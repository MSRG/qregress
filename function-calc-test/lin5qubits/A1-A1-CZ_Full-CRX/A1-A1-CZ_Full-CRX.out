/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:24 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:23 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:59 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:12 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:22 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:27 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2053.5103068351746 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:45 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:48 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:27 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:55 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:09 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2154.0275337696075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:37 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:56 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:40 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:02 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:09 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2156.803309202194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:57 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:19 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:49 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:14 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:43 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2191.8361251354218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:20:03 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:28 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:24 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 20:41:59 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:10 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2178.747588157654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:06 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:34 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 21:23:33 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2060.834601163864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:42 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:23 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:45 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:05 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:09 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2074.542544364929 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:05:09 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:12:50 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:18:38 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 22:25:49 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 22:33:09 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2099.5056958198547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:14 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:48:14 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:54 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 23:00:56 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 23:08:11 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2118.1622824668884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:15:30 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:27 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:28:58 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:11 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 23:43:52 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2140.0004501342773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:23 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:45 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:27 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 00:12:39 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:07 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2166.938685655594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:26 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:35:22 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:41:06 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 00:48:15 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 00:55:05 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2099.7772827148438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:02:38 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:10:38 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:16:21 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 01:23:49 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:06 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2177.394701242447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:38:30 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:26 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:52:12 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 01:59:30 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 02:07:15 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2163.168974161148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:54 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:23:09 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:29:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 02:36:24 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 02:43:23 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2160.6619782447815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:50:22 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:58:17 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:03:51 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 03:10:50 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 03:17:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2065.03804564476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:25:08 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:33:30 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:39:21 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:46 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 03:53:44 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2158.5114991664886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:01:11 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:01 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:14:32 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 04:21:41 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 04:29:20 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2130.126462459564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:36:41 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:44:31 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:49:59 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 04:56:29 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:02 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2001.1984853744507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:09:30 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:17:24 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:22:44 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 05:29:07 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 05:35:39 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1952.7337124347687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:42:03 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:49:15 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:54:19 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 06:00:42 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 06:07:10 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1892.0515336990356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:13:33 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:20:35 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:25:58 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 06:33:05 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 06:39:59 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1989.618724346161 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:47:07 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:54:50 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:00:18 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 07:06:50 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 07:13:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2016.0532217025757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:20:28 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:28:16 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:33:23 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 07:40:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 07:46:53 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1979.5932068824768 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:53:35 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:01:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:06:39 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 08:13:41 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 08:20:25 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2023.465810060501 seconds. 
Discarding model... 

Training complete taking 52204.30341267586 total seconds. 
Now scoring model... 
Scoring complete taking 2.33638334274292 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (206.93879274915722,), 'R2_train': -0.0025489989166755933, 'MAE_train': 12.57727151830348, 'MSE_test': 191.37228833007708, 'R2_test': -0.15355654159227172, 'MAE_test': 12.013220427830294}. 
Saved model results as A1-A1-CZ_Full-CRX_results.json. 
