/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:09 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:12 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:32:15 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:33:17 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:34:22 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.77204513549805 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:24 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:26 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:29 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:31 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:34 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.4884593486786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:36 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:38 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:41 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:44 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:47 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.8463809490204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:49 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:53 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:57 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:00 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:02 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.1580882072449 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:51:05 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:07 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:09 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:12 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:17 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 318.04367089271545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:23 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:30 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:32 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:34 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:00:37 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 317.6206793785095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:39 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:41 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:44 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:47 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:49 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.445764541626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:53 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:55 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:57 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:59 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:02 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.2229907512665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:04 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:07 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:09 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:11 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:13 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 318.6363878250122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:24 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:30 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:33 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:36 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:44 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 332.11158657073975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:56 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:24:07 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:18 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:25 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:27 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 334.97274804115295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:30 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:32 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:42 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:31:45 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:48 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 322.0536859035492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:50 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:54 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:57 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:00 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:38:03 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.1626887321472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:05 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:07 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:10 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:12 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:15 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.83975076675415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:17 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:45:20 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:22 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:26 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:28 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.9614565372467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:31 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:33 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:35 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:38 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:40 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.6571145057678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:43 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:45 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:47 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:50 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:52 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.26902866363525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:58 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:01 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:03 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:06 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.57704424858093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:08 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:11 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:14 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:17 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:19 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 315.3936309814453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:22 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:26 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:29 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:13:32 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:34 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.72768473625183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:37 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:40 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:42 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:45 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:19:48 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.3780212402344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:50 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:53 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:22:58 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:00 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:25:02 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.58372473716736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:05 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:08 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:11 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:13 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:15 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.56858825683594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:18 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:21 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:33:24 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:28 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:30 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.43109107017517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:32 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:34 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:35 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:37 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:39 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 308.8140470981598 seconds. 
Discarding model... 

Training complete taking 7890.73773932457 total seconds. 
Now scoring model... 
Scoring complete taking 2.2811334133148193 seconds. 
Saved predicted values as IQP_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.369243939905919,), 'R2_train': 0.9885218082671368, 'MAE_train': 1.1378144288432182, 'MSE_test': 1.7527885580737752, 'R2_test': 0.9894345167482835, 'MAE_test': 1.177123249982144}. 
Saved model results as IQP_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:00 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:30:03 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:31:05 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:32:09 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:33:18 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 321.98008584976196 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:34:20 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:35:24 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:36:27 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:37:29 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:38:32 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.98834776878357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:39:35 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:37 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:41:40 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:42:42 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:43:46 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.56176233291626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:44:48 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:45:59 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:47:06 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:48:09 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:49:11 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 325.58320140838623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 11:50:13 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:16 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:52:26 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:53:30 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:54:32 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 344.15856170654297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:55:59 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:57:07 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:58:12 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:59:15 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:00:18 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 322.98851680755615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:01:21 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:02:24 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:03:28 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:04:30 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:33 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.3046178817749 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:35 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:07:38 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:08:42 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:09:45 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:10:48 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.6701748371124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:11:50 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:12:54 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:58 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:00 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:16:04 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 324.66768407821655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:17:15 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:18:18 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:19:22 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:31 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:34 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 320.2669503688812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:37 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:55 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:24:59 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:26:20 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:27:23 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 350.3025794029236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:28:26 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:39 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:30:42 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:31:45 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:32:48 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 325.12510776519775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:51 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:00 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:03 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:06 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:09 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 321.54140162467957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:39:12 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:15 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:18 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:42:33 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:36 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 326.1569356918335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:44:40 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:45:43 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:46:49 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:52 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:55 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 319.41430020332336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:00 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:02 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:05 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:09 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:29 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 332.51405596733093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:55:32 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:39 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:57:42 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:45 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:49 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 321.73415184020996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:52 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:55 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:03:00 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:04:02 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:06 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.71358156204224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:06:09 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:07:11 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:08:14 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:09:17 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:10:19 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 315.1525285243988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:22 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:12:25 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:28 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:14:33 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:36 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.71224761009216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:16:39 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:17:42 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:18:45 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:48 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:20:51 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 327.7108006477356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:22:12 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:15 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:24:26 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:25:29 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:15 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 373.1960437297821 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:28:18 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:20 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:23 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:25 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:32:28 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.8328003883362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:33:33 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:34:35 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:35:37 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:36:39 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:42 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.1211187839508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:38:52 2024]  Iteration number: 0 with current cost as 0.09769982316331624 and parameters 
[-3.69726586  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:40:00 2024]  Iteration number: 0 with current cost as 0.11346798294822101 and parameters 
[-3.58747417  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:04 2024]  Iteration number: 0 with current cost as 0.10839238531432757 and parameters 
[-3.61222037  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:42:06 2024]  Iteration number: 0 with current cost as 0.10728114286920104 and parameters 
[-3.62186158  2.23743463 -2.12427963 -0.11653103  0.55388707 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:09 2024]  Iteration number: 0 with current cost as 0.111333381396274 and parameters 
[-3.61308652  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 325.1788651943207 seconds. 
Discarding model... 

Training complete taking 8112.577474355698 total seconds. 
Now scoring model... 
Scoring complete taking 0.8833918571472168 seconds. 
Saved predicted values as IQP_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.369243939905919,), 'R2_train': 0.9885218082671368, 'MAE_train': 1.1378144288432182, 'MSE_test': 1.7527885580737752, 'R2_test': 0.9894345167482835, 'MAE_test': 1.177123249982144}. 
Saved model results as IQP_Modified-Pauli-CRZ_results.json. 
