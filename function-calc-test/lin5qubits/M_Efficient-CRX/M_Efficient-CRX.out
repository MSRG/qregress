/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:58 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:02 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:02 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:02 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:36 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 967.7787852287292 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:47:59 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 17:50:50 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 17:54:54 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:51 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 18:00:23 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 940.5944800376892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:03:43 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:39 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:44 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:44 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:13 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 949.7040913105011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:29 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:27 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:32 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:28 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:02 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 953.0239539146423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:24 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:19 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:24 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:19 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:49 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 941.2588143348694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:05 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 18:54:08 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:14 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:18 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 19:03:53 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 961.4349737167358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:05 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:19 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:28 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 19:17:19 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 19:19:47 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 957.8715801239014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:05 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:01 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:57 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:52 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:28 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 940.5374140739441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:43 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:47 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:03 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:01 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:42 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 977.4219002723694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:00 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:51 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:47 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:50 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:19 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 934.348491191864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:44 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 20:13:50 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:58 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:59 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:43 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 983.898362159729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:01 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 20:30:04 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:06 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:05 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:41 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 957.309029340744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:03 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:03 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 20:50:15 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 20:53:16 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:58 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 981.8781609535217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:26 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:32 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:51 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:05 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:54 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 1016.2088270187378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:16:28 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:28 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 21:23:39 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:40 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:28 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 997.7131464481354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:05 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 21:36:11 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:20 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:27 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 21:45:52 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 972.2937407493591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:15 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:38 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 21:57:07 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 22:00:22 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:01 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 1042.2249732017517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:33 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:36 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 22:13:52 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:47 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 22:19:12 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 959.0859203338623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:22:26 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 22:25:23 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:21 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 22:32:15 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:39 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 927.2856209278107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:49 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:37 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:33 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:23 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:52 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 920.0155873298645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:12 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:04 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:01 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 23:03:13 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 23:05:48 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 950.296050786972 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:09:05 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 23:12:00 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 23:15:52 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:43 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 23:21:16 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 927.3671288490295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:24:30 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 23:27:15 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 23:31:09 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 23:33:57 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 23:36:33 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 917.4531395435333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:39:47 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 23:42:39 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Sun Mar 24 23:46:35 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Sun Mar 24 23:49:20 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:51 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 918.6733005046844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:08 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:58 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Mon Mar 25 00:01:51 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Mon Mar 25 00:04:41 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Mon Mar 25 00:07:07 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 912.8949403762817 seconds. 
Discarding model... 

Training complete taking 23908.57347011566 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.6230599880218506 seconds. 
Saved predicted values as M_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (623.8500271847597,), 'R2_train': -2.0223440077103136, 'MAE_train': 22.44904286068611, 'MSE_test': 528.8326405487767, 'R2_test': -2.187704746783231, 'MAE_test': 20.86029481346508}. 
Saved model results as M_Efficient-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:32:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:34:17 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 11:37:00 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:48 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 11:43:33 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 11:45:57 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 884.3653645515442 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:49:03 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:46 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 11:55:31 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:14 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 12:00:37 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 881.09645819664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:03:40 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 12:06:20 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 12:10:03 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 12:12:51 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 12:15:19 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 889.91770362854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:18:33 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 12:21:25 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:20 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:07 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:35 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 907.6090104579926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:43 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:29 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 12:40:15 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:08 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:37 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 905.8419573307037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:44 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:39 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 12:55:29 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:16 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 13:00:48 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 911.7541196346283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:01 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 13:06:49 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:37 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:30 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:56 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 910.8528501987457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:04 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 13:21:51 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 13:25:51 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:45 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 13:31:16 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 913.7710912227631 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:24 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:13 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:59 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 13:43:54 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 13:46:27 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 916.310825586319 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:36 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 13:52:21 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 13:56:10 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 13:58:59 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 14:01:31 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 902.1300084590912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:43 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:39 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:30 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 14:14:23 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 14:17:03 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 939.3630092144012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:20:30 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 14:23:25 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 14:27:19 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 14:30:10 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:41 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 930.0048577785492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:44 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 14:38:26 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 14:42:19 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 14:45:11 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 14:47:39 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 895.8470287322998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:50:45 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:28 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 14:57:14 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 15:00:06 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 15:02:30 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 889.9079878330231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:05:31 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 15:08:19 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 15:12:12 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 15:14:56 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 15:17:21 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 894.451963186264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:20:25 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 15:23:07 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 15:26:45 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 15:29:26 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 15:31:51 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 868.7891550064087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:34:53 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 15:37:33 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:11 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 15:43:49 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:08 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 852.9242329597473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:49:13 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 15:51:54 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:48 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 15:58:31 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:54 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 887.321783542633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:04:00 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 16:06:41 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 16:10:26 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 16:13:12 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 16:15:33 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 877.0923171043396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:18:29 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 16:21:12 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 16:24:54 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 16:27:33 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 16:29:53 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 863.6754128932953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:32:56 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 16:35:35 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:24 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 16:42:05 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 16:44:30 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 874.4649751186371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:47:26 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 16:50:06 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 16:53:47 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 16:56:28 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 16:58:50 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 868.7918827533722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:02:01 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 17:04:40 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 17:08:21 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 17:11:03 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:32 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 873.8440914154053 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:16:30 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 17:19:11 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 17:22:54 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 17:25:35 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 17:27:59 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 868.3428456783295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:31:00 2024]  Iteration number: 0 with current cost as 0.0797617206657834 and parameters 
[-4.85400879  2.23743452 -2.12427964 -0.11653103  0.55388702 -2.77010915
  3.06858486  2.18960151  1.18551998 -1.0664832   0.60271516  1.14432445
  1.31029887 -1.87354668]. 
Working on 0.4 fold... 
[Thu Apr  4 17:33:44 2024]  Iteration number: 0 with current cost as 0.9516194088061364 and parameters 
[-7.95280889  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.87354617]. 
Working on 0.6 fold... 
[Thu Apr  4 17:37:30 2024]  Iteration number: 0 with current cost as 0.9892562710298453 and parameters 
[-7.9186438   2.23743464 -2.12427932 -0.11653071  0.55388739 -2.77010897
  3.06858498  2.18960177  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 0.8 fold... 
[Thu Apr  4 17:40:10 2024]  Iteration number: 0 with current cost as 1.0342590897902582 and parameters 
[-8.28308212  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271573  1.14432508
  1.3102993  -1.87354617]. 
Working on 1.0 fold... 
[Thu Apr  4 17:42:36 2024]  Iteration number: 0 with current cost as 0.983255889694549 and parameters 
[-7.72371319  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552061 -1.06648308  0.60271542  1.14432508
  1.31029899 -1.87354617]. 
Training complete taking 877.8128535747528 seconds. 
Discarding model... 

Training complete taking 22286.28462910652 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2297861576080322 seconds. 
Saved predicted values as M_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (623.8500271847597,), 'R2_train': -2.0223440077103136, 'MAE_train': 22.44904286068611, 'MSE_test': 528.8326405487767, 'R2_test': -2.187704746783231, 'MAE_test': 20.86029481346508}. 
Saved model results as M_Efficient-CRX_results.json. 
