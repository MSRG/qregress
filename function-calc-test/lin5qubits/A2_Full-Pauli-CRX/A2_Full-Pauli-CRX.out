/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:51:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:23 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:27 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:05:03 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:59 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:24 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:22:12 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:36 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2137.989541053772 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:02 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:06 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:40:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:18 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:41 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:57:39 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:03 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2127.4457907676697 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:28 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:35 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:15:44 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:41 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:06 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:32:51 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:12 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2106.4503836631775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:43 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:48 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:51:27 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 19:56:26 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:59:57 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 20:08:49 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:10 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2158.2982738018036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:33 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:41 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 20:27:06 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 20:32:02 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:35:26 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 20:44:19 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 20:46:41 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2132.2661221027374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:05 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:53:07 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:02:27 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:26 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:47 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:19:40 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:03 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2126.626802921295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:32 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:36 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:38:15 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:08 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:35 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:55:46 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:23 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2174.298187017441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:46 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:51 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 22:14:11 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:11 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:41 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 22:31:30 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:03 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2151.6630709171295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:38 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:42 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 22:49:52 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 22:54:46 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:10 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:07:05 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 23:09:33 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2120.4097616672516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:12:58 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:16:25 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:25:46 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Sun Mar 24 23:30:45 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:34:08 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:42:57 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:20 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2144.403345108032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:42 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:51:45 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:01:22 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 00:06:35 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:10:01 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:19:09 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:29 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2190.2013323307037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:25:12 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:16 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:37:45 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 00:42:53 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:46:19 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:55:10 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 00:57:36 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2146.500872373581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:01:00 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:04:05 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 01:13:19 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 01:18:15 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:21:36 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 01:30:30 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 01:32:53 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2120.371528863907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:36:19 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:20 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 01:48:43 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 01:54:08 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:34 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 02:06:39 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 02:09:02 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2165.987762928009 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:12:26 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:15:29 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 02:24:54 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 02:29:57 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:23 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 02:42:07 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 02:44:29 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2127.314850330353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:47:53 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:10 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:00:20 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 03:05:17 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:09:03 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:17:56 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 03:20:16 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2145.4528110027313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:23:39 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 03:26:45 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:36:07 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:01 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:24 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:53:10 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 03:55:33 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2118.837168455124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:58:57 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:02:04 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 04:11:12 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:12 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:19:38 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 04:28:43 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 04:31:07 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2132.0962052345276 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:34:29 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:30 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 04:46:37 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 04:51:41 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:55:36 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:05:06 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 05:07:39 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2194.066204547882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:11:04 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:14:07 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:23:17 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 05:28:13 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:31:42 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:40:41 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 05:43:04 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2123.855367422104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:46:27 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:49:40 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:59:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 06:04:16 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:07:47 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:16:49 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 06:19:11 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2171.58673620224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:22:39 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 06:25:42 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:34:47 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 06:39:48 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:43:12 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:52:13 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 06:55:11 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2154.5765924453735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:58:33 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 07:01:36 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 07:10:43 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 07:15:47 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:19:12 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 07:27:58 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 07:30:20 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2108.138251543045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:33:41 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 07:36:42 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 07:45:53 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 07:50:51 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:54:14 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 08:03:07 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 08:05:29 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2117.1628053188324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:08:59 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 08:12:07 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 08:21:38 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Mon Mar 25 08:26:31 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 08:29:54 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 08:38:52 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Mon Mar 25 08:41:16 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2151.517373085022 seconds. 
Discarding model... 

Training complete taking 53547.5185046196 total seconds. 
Now scoring model... 
Scoring complete taking 0.9929025173187256 seconds. 
Saved predicted values as A2_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (27.61296064674166,), 'R2_train': 0.8662244730156852, 'MAE_train': 3.713096917624143, 'MSE_test': 30.748108715798402, 'R2_test': 0.8146561225754806, 'MAE_test': 3.5692210550880246}. 
Saved model results as A2_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:27:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:30 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:27 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 12:39:27 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:13 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:32 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 12:56:00 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:18 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2043.2407221794128 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:01:33 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 13:04:31 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 13:13:15 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 13:18:08 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:21:22 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 13:29:47 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 13:32:02 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2024.1268167495728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:17 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:11 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 13:46:55 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 13:51:36 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:50 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 14:03:15 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 14:05:31 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2008.2545742988586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:08:44 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:11:43 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 14:20:33 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:18 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:28:37 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 14:37:05 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 14:39:31 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2041.4292829036713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:46 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:45:40 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 14:54:23 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 14:59:20 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:02:34 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 15:11:21 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 15:13:37 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2044.6463124752045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:16:50 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 15:19:53 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 15:29:02 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 15:33:47 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:37:14 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 15:45:40 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 15:47:56 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2059.7286636829376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:51:10 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 15:54:05 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:02:52 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 16:07:35 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:48 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:19:14 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 16:21:31 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2015.382752418518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:24:46 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 16:27:47 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:36:47 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 16:41:44 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:44:59 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:53:28 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 16:55:45 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2055.7738506793976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:59:01 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:01:58 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 17:10:57 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 17:15:40 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:18:55 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 17:27:21 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 17:29:38 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2030.1660776138306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:32:52 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:48 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 17:44:32 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 17:49:14 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:52:28 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 18:00:53 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 18:03:10 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2012.1565234661102 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:06:24 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 18:09:19 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 18:18:03 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 18:22:46 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:26:00 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 18:34:26 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:43 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2038.648354291916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:40:37 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 18:43:40 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 18:52:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 18:57:24 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:00:39 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 19:09:05 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 19:11:21 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2053.35608959198 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:14:37 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 19:17:40 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 19:26:25 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 19:31:08 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:34:33 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 19:43:13 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 19:45:29 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2048.9677686691284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:48:46 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 19:51:42 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:00:26 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 20:05:09 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:08:39 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:17:07 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 20:19:24 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2034.7680537700653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:22:40 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:25:35 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:34:29 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 20:39:36 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:42:51 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:51:15 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 20:53:32 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2046.7324438095093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:56:47 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:59:42 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 21:08:27 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 21:13:10 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:16:25 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 21:24:50 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 21:27:06 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2014.9283759593964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:30:21 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:33:15 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 21:42:00 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 21:46:42 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:49:57 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 21:58:26 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 22:00:47 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2019.6128866672516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:04:01 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:06:57 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 22:15:42 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 22:20:27 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:23:43 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 22:32:10 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 22:34:26 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2019.9250085353851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:37:41 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:40:36 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 22:49:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 22:54:03 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:57:18 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:05:42 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 23:07:59 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2012.9771184921265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:11:14 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:14:10 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:23:03 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Thu Apr  4 23:27:45 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:31:03 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:39:53 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Thu Apr  4 23:42:09 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2050.203708410263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:45:24 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:48:20 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:57:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Fri Apr  5 00:02:06 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:05:26 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 00:13:54 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Fri Apr  5 00:16:10 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2039.6175458431244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:19:23 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:22:26 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 00:31:21 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Fri Apr  5 00:36:03 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:39:19 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 00:47:44 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Fri Apr  5 00:50:01 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2030.7915058135986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:15 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:56:11 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 01:04:57 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Fri Apr  5 01:09:38 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:12:54 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 01:21:37 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Fri Apr  5 01:23:56 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2035.194910287857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:27:10 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:30:14 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 01:39:04 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Fri Apr  5 01:43:57 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:47:11 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 01:55:55 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Fri Apr  5 01:58:12 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2057.58212685585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:01:27 2024]  Iteration number: 0 with current cost as 0.11678073324100832 and parameters 
[-2.81357337  2.13824775 -2.10179959 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.44057494  1.14432445
  1.1279041  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:04:23 2024]  Iteration number: 0 with current cost as 0.0956169657819254 and parameters 
[-2.83278333  2.17061555 -2.10547761 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.48026432  1.14432445
  1.17099977 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 02:13:13 2024]  Iteration number: 50 with current cost as 0.04166702021878999 and parameters 
[-3.23458804  1.57445476 -0.76510495 -0.11647673  0.55397488 -2.77005239
  3.0686902   2.18967541  1.18558375 -1.06637304  0.02270673  1.14453939
  1.56680405 -1.87348563  0.72978146  2.88587114 -0.54534887 -0.47525211
 -2.02658895  0.72901747  1.60518003  2.83077312 -1.26464509 -0.25137257]. 
Working on 0.6 fold... 
[Fri Apr  5 02:17:57 2024]  Iteration number: 0 with current cost as 0.09878187757510767 and parameters 
[-2.82416417  2.16080633 -2.10359567 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46454259  1.14432445
  1.15376436 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:12 2024]  Iteration number: 0 with current cost as 0.10913781969723677 and parameters 
[-2.82282555  2.1516392  -2.10396382 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.45863434  1.14432445
  1.14781607 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 02:29:40 2024]  Iteration number: 50 with current cost as 0.04158091252606293 and parameters 
[-2.32363525  1.53308328 -4.514185   -0.1165482   0.55387874 -2.77012923
  3.06856793  2.18957937  1.18552898 -1.06651051  0.09441066  1.14431312
  1.48823947 -1.87355902  0.72965915  2.88579034 -0.5453772  -0.47524869
 -2.02654323  0.72897329  1.6051345   2.83076869 -1.26454435 -0.25137675]. 
Working on 1.0 fold... 
[Fri Apr  5 02:31:57 2024]  Iteration number: 0 with current cost as 0.10387431434027387 and parameters 
[-2.82589122  2.16153221 -2.10395751 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.46718694  1.14432445
  1.15659563 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2025.3027110099792 seconds. 
Discarding model... 

Training complete taking 50863.51580476761 total seconds. 
Now scoring model... 
Scoring complete taking 0.9920737743377686 seconds. 
Saved predicted values as A2_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (27.61296064674166,), 'R2_train': 0.8662244730156852, 'MAE_train': 3.713096917624143, 'MSE_test': 30.748108715798402, 'R2_test': 0.8146561225754806, 'MAE_test': 3.5692210550880246}. 
Saved model results as A2_Full-Pauli-CRX_results.json. 
