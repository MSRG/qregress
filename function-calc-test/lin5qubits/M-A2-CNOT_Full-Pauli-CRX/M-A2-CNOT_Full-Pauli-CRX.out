/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:27 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:54 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:10 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:41 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:24 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2554.946843147278 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:29 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:59 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:33 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:08 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:05:51 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2547.4024047851562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:54 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:28 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:00 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:51 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:48:54 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2586.1549212932587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:02 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:43 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:14:11 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 20:22:40 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:31:32 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2564.9499180316925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:39:47 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:34 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:03 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:48 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:40 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2590.0244522094727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:22:57 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:31 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:17 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:52 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:32 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2558.723185300827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:05:34 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:02 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:30 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:57 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:42 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2528.749016523361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:44 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:11 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:42 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:13:15 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:21:55 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2531.7561585903168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:56 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:25 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:46:59 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:55:32 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:15 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2544.6550517082214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:12:21 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:20:51 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:29:18 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:37:43 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:26 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2528.0118770599365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:29 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:02:55 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:11:20 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:46 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:33 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2527.453162431717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:36:36 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:45:05 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:53:34 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:02:06 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:45 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2536.7713294029236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:18:53 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:27:18 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:35:46 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:44:10 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:52:50 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2521.44073843956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:00:54 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:09:16 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:17:41 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:26:08 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:34:49 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2523.206362724304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:42:58 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:51:23 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:59:49 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:08:16 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:57 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2527.4964106082916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:25:05 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:33:35 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:41:57 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:50:23 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:59:03 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2517.9616050720215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:07:03 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:15:41 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:24:08 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 05:32:35 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:41:10 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2527.783878326416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:49:11 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:57:39 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 06:06:10 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:14:50 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:23:39 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2546.1053578853607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:31:37 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:40:29 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 06:48:58 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:57:24 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:06:04 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2549.8057010173798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:14:05 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:32 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 07:31:16 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 07:39:47 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:48:28 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2540.3914048671722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:56:27 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:04:54 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 08:13:20 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 08:21:48 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 08:30:30 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2523.238466978073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 08:38:31 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:47:01 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 08:55:31 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:04:05 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:12:49 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2539.4031369686127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 09:20:50 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:29:34 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 09:38:01 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:46:27 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:55:05 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2535.226996898651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:03:05 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:11:32 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 10:19:58 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 10:28:25 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 10:37:03 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2528.8453018665314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 10:45:14 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:53:59 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 11:02:27 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 11:11:14 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 11:20:07 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2575.4150092601776 seconds. 
Discarding model... 

Training complete taking 63555.920071840286 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0840129852294922 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (90.58042160461207,), 'R2_train': 0.5611682575570447, 'MAE_train': 8.413096585237101, 'MSE_test': 139.60000431572922, 'R2_test': 0.15851715214396866, 'MAE_test': 9.467649466204854}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:23:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:28 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:31:52 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:40:20 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 12:48:41 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:42 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2547.8757281303406 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:06:10 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:40 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:58 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:11 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 13:39:41 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2495.420449256897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:47:30 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:55:51 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:04:33 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:13:17 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:21:58 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2540.1469292640686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:49 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:38:21 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:37 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:55:07 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:58 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2539.464148759842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:12:10 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:20:27 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:28:44 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 15:37:04 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:45:31 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2472.056363582611 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:53:22 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:01:48 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:10:06 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 16:18:24 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 16:26:56 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2483.921380519867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:34:46 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:44:01 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:21 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 17:00:45 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:09:16 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2555.3051342964172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:17:21 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:25:58 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:34:21 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 17:42:42 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:51:17 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2511.3662555217743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:59:13 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:07:40 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:15:59 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:24:17 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 18:32:48 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2488.651449918747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:40:41 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:48:59 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:57:17 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 19:05:32 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:14:04 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2475.2698760032654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:21:57 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:30:22 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:39:09 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 19:47:27 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:55:55 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2511.3118867874146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:03:48 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:12:06 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:20:26 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 20:28:52 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:37:37 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2502.685231924057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:45:31 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:53:51 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:02:09 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:10:28 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 21:19:01 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2482.495005607605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:26:53 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:35:08 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:43:28 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:51:46 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:00:18 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2478.163829565048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:08:11 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:16:31 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:24:46 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:33:06 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:41:40 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2489.6332864761353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:49:41 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:58:28 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:06:59 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:15:22 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:24:14 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2547.6563420295715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:32:09 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:40:27 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:48:45 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:57:03 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:05:33 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2479.7545330524445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:13:29 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:21:49 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 00:30:09 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:38:30 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:47:02 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2484.693238258362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:54:53 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:03:14 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 01:11:35 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 01:19:54 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:28:27 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2487.69554066658 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:36:20 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:44:37 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 01:52:55 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:01:15 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:09:46 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2479.474395751953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:17:40 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:25:56 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 02:34:13 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:42:32 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:51:05 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2499.118645429611 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:59:19 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:07:37 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 03:15:58 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:24:13 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 03:32:55 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2532.827816963196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:41:32 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:49:48 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 03:58:19 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:38 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:15:07 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2487.8879134655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:23:00 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:31:21 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 04:39:38 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:47:57 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:56:28 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2477.902334690094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:04:18 2024]  Iteration number: 0 with current cost as 0.4208552602093614 and parameters 
[-3.04519413  2.43447713 -2.17868224 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.65906235  1.14432445
  1.48101302 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:12:36 2024]  Iteration number: 0 with current cost as 0.36868550457226545 and parameters 
[-3.03622057  2.35997012 -2.18342362 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.68346599  1.14432445
  1.49056522 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 05:21:12 2024]  Iteration number: 0 with current cost as 0.3712150300221597 and parameters 
[-3.04555622  2.41020331 -2.18035726 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960146  1.18551998 -1.06648308  0.67189784  1.14432446
  1.48965495 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 05:29:37 2024]  Iteration number: 0 with current cost as 0.37621661860844435 and parameters 
[-3.05730234  2.3964541  -2.18626971 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.68988301  1.14432445
  1.50859262 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 05:38:12 2024]  Iteration number: 0 with current cost as 0.39043627534417424 and parameters 
[-3.03969369  2.38619551 -2.18082606 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.67321733  1.14432445
  1.48580383 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2509.9152517318726 seconds. 
Discarding model... 

Training complete taking 62560.69458270073 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.208345651626587 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (90.58042160461207,), 'R2_train': 0.5611682575570447, 'MAE_train': 8.413096585237101, 'MSE_test': 139.60000431572922, 'R2_test': 0.15851715214396866, 'MAE_test': 9.467649466204854}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRX_results.json. 
