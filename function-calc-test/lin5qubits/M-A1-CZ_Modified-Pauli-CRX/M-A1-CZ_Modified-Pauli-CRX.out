/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:02 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:39 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:49 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:12 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:28 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1571.485009431839 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:46 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:58 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:24 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:38 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1605.3546369075775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:30 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:39:17 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:32 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:01 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1629.6974318027496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:40 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:41 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:59 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:04 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:25 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1570.07821559906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:51 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:55 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:24 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:22 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:44 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1580.0929701328278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:11 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:53:15 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:54 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:06 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:38 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1611.2350010871887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:03 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:16 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:25:43 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:31:10 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:54 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.6333413124084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:25 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:33 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:08 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:12 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:56 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1625.7297177314758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:48 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:29 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:40 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1616.957095861435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:26 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:26 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:47 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:46 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:01 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1560.1784753799438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:27 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:07:21 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:47 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:17:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:23:00 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1550.5943455696106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:28:16 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:19 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:46 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:43:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:03 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1572.570993900299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:30 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:29 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:54 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:55 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:15:20 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1579.1993200778961 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:52 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:56 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:31:27 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:31 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:41:58 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1595.544973373413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:47:24 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:34 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:02 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:03 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:08:29 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1593.6025063991547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:13:59 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:04 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:24:37 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:29:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:35:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1589.6283767223358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:40:28 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:45:41 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:51:22 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:56:28 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:54 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1603.3168077468872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:11 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:13 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:17:38 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:22:39 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:04 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1578.9253404140472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:33:28 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:23 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:53 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:48:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:54:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1567.6206810474396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:59:42 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:04:42 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:10:04 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:15:02 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:20:24 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1568.1699242591858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:25:46 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:52 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:36:25 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:41:24 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:46:45 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1587.99374127388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:52:17 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:57:26 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:02:50 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:07:54 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:13:21 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1594.3052062988281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:48 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:24:00 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:29:30 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:34:40 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:40:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1603.1173827648163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:50:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:56:07 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:01:15 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:06:55 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.6221542358398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:12:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:17:35 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:22:58 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:28:15 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:34:06 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1635.8163039684296 seconds. 
Discarding model... 

Training complete taking 39853.47143268585 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0289084911346436 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (47.22246715097087,), 'R2_train': 0.7712229952652303, 'MAE_train': 4.0791890855164255, 'MSE_test': 69.8065298581968, 'R2_test': 0.5792192283091213, 'MAE_test': 5.272717961664344}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:17:16 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:17:51 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:06 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:46 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:03 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:39:40 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1648.227213382721 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:19 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:50:33 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:11 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:26 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:13 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1653.0417675971985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:51 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:18:07 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:23:46 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:10 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:47 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1652.3458642959595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:40:26 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:45:42 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:51:45 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:02 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:42 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1675.2178103923798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:08:20 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:13:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:15 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:28 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:30:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.5746562480927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:42 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:40:54 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:46:34 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:51:47 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:57:25 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1650.6552109718323 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:13 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:08:25 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:14:00 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:19:29 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:25:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1650.8020389080048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:30:44 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:35:58 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:38 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:46:53 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:52:36 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1647.9060785770416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:58:11 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:03:37 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:09:20 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:14:34 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:20:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1662.7878074645996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:25:55 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:31:09 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:36:46 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:42:11 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:47:58 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1657.786440372467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:53:33 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:58:46 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:04:25 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:09:38 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:15:23 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1647.7244045734406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:21:06 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:26:18 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:31:55 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:37:09 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:42:45 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.6280448436737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:48:21 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:53:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:59:09 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:04:23 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:10:01 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1637.3468465805054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:15:39 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:20:53 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:26:28 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:31:42 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:37:19 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1636.8982264995575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:42:56 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:48:10 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:53:47 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:58:59 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:04:38 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1639.3405928611755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:10:15 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:15:30 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:14 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:26:33 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:32:09 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1648.5198638439178 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:37:45 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:43:00 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:48:53 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:54:13 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:59:52 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1666.272268295288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:05:29 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:10:43 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:16:18 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:21:33 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:27:09 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1636.3460159301758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:32:47 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:38:05 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:43:40 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:48:53 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:54:36 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1647.68004322052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:00:14 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:05:28 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:11:03 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:16:22 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:22:00 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.8642053604126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:27:38 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:32:53 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:38:30 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:43:46 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:49:27 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1646.7777552604675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:55:05 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:00:20 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:06:01 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:11:17 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:16:54 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.8249156475067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:22:28 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:42 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:33:23 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:38:39 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:44:18 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1659.5174400806427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:08 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:55:24 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:01:15 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:06:29 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:12:05 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1653.9085071086884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:17:42 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:22:55 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:29 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:33:43 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:39:22 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1659.9191899299622 seconds. 
Discarding model... 

Training complete taking 41251.91503381729 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2695269584655762 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (47.22246715097087,), 'R2_train': 0.7712229952652303, 'MAE_train': 4.0791890855164255, 'MSE_test': 69.8065298581968, 'R2_test': 0.5792192283091213, 'MAE_test': 5.272717961664344}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRX_results.json. 
