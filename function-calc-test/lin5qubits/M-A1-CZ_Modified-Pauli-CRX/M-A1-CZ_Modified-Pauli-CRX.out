/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:02 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:39 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:49 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:12 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:28 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1571.485009431839 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:46 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:58 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:24 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:38 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1605.3546369075775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:30 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:39:17 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:32 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:01 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1629.6974318027496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:40 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:41 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:59 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:04 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:25 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1570.07821559906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:51 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:55 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:24 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:22 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:44 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1580.0929701328278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:11 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:53:15 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:54 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:06 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:38 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1611.2350010871887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:03 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:16 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:25:43 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:31:10 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:54 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.6333413124084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:25 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:33 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:08 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:12 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:56 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1625.7297177314758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:48 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:29 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:40 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1616.957095861435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:26 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:26 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:47 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:46 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:01 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1560.1784753799438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:27 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:07:21 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:47 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:17:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:23:00 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1550.5943455696106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:28:16 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:19 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:46 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:43:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:03 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1572.570993900299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:30 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:29 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:54 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:55 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:15:20 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1579.1993200778961 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:52 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:56 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:31:27 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:31 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:41:58 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1595.544973373413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:47:24 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:34 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:02 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:03 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:08:29 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1593.6025063991547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:13:59 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:04 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:24:37 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:29:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:35:07 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1589.6283767223358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:40:28 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:45:41 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:51:22 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:56:28 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:54 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1603.3168077468872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:11 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:13 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:17:38 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:22:39 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:04 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1578.9253404140472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:33:28 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:23 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:53 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:48:45 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:54:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1567.6206810474396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:59:42 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:04:42 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:10:04 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:15:02 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:20:24 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1568.1699242591858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:25:46 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:52 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:36:25 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:41:24 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:46:45 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1587.99374127388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:52:17 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:57:26 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:02:50 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:07:54 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:13:21 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1594.3052062988281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:48 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:24:00 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:29:30 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:34:40 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:40:08 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1603.1173827648163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:50:36 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:56:07 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:01:15 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:06:55 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.6221542358398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:12:31 2024]  Iteration number: 0 with current cost as 0.5199497379174822 and parameters 
[-4.51894601  2.45016801 -2.4716     -0.11653091  0.55388715 -2.7701089
  3.06858506  2.18960153  1.18552006 -1.06648301  2.73568401  1.14432453
  1.31029906 -1.87354673  0.7296508   2.88578427 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:17:35 2024]  Iteration number: 0 with current cost as 0.24260904916793719 and parameters 
[-3.26306372  2.27950601 -2.20934871 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  1.14642166  1.14432446
  1.31029899 -1.87354679  0.7296508   2.8857842  -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:22:58 2024]  Iteration number: 0 with current cost as 0.23099772192793172 and parameters 
[-3.26166241  2.27755787 -2.20735794 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551999 -1.06648308  1.11614534  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:28:15 2024]  Iteration number: 0 with current cost as 0.24013198427128116 and parameters 
[-3.27202672  2.27629221 -2.21233239 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  1.12387301  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:34:06 2024]  Iteration number: 0 with current cost as 0.2619352187614449 and parameters 
[-3.24436969  2.2742942  -2.20289793 -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  1.10702177  1.14432445
  1.31029899 -1.8735468   0.72965079  2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1635.8163039684296 seconds. 
Discarding model... 

Training complete taking 39853.47143268585 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0289084911346436 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (47.22246715097087,), 'R2_train': 0.7712229952652303, 'MAE_train': 4.0791890855164255, 'MSE_test': 69.8065298581968, 'R2_test': 0.5792192283091213, 'MAE_test': 5.272717961664344}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRX_results.json. 
