/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:49 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:15 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:01 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:24 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.7033035755157 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:05 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:27 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:25 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:46 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:59 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1775.9851174354553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:43 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:05 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:00 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:25 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1755.6000516414642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:58:57 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:34 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:10:19 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:35 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:57 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1780.324364900589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:28:39 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:44 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:46:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:32 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1772.262221813202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:09 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:30 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:09:18 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:28 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1742.0751712322235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:15 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:34 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:42 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:50 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1750.6310141086578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:24 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:30 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:14:00 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:17 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1765.1235077381134 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:52 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:12 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:35 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:01 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1828.1958801746368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:15 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:01:40 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:13:39 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:19:59 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.8027122020721 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:32 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:30:52 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:36:37 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:55 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:04 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1743.8476073741913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:36 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:22 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:27 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:33 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:42 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1781.9075002670288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:24:18 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:29:41 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:35:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:37 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:54 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1775.2780771255493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:53:54 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:21 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:08 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:11:18 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1746.4621455669403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:01 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:23 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:10 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:40:19 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:33 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1743.917694568634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:05 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:28 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:14 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:25 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:36 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1754.682344198227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:20 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:27:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:33:15 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:39:25 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:45:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1790.2286355495453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:10 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:56:30 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:02:17 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:08:30 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:14:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1746.7857422828674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:20:16 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:22 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:37:30 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:43:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1735.5618770122528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:49:13 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:54:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:00:21 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:06:31 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:12:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1742.1267313957214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:14 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:23:33 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:29:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:35:46 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:41:56 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1763.571660041809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:47:36 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:53:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:58:47 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:05:05 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:11:17 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1751.8410835266113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:16:50 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:22:09 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:28:04 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:34:17 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:40:28 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1752.9429075717926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:46:02 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:51:23 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:57:06 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:03:41 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:09:52 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1765.4636664390564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:15:29 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:20:57 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:26:43 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:32:57 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:39:07 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1762.899664402008 seconds. 
Discarding model... 

Training complete taking 44041.22332072258 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2353627681732178 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (76.94208880674692,), 'R2_train': 0.6272414027211133, 'MAE_train': 6.030523386608814, 'MSE_test': 93.30365765471907, 'R2_test': 0.437582914531251, 'MAE_test': 6.526579210094285}. 
Saved model results as M-M-CZ_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:26 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:55 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:54:43 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:00:54 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:07:08 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1759.4680242538452 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:12:45 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:18:09 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:23:55 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:06 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:36:18 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1746.8349795341492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:41:51 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:14 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:53:10 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:59:28 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:43 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1778.3179807662964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:31 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:16:56 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:44 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:58 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:16 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1761.5126056671143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:40:51 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:16 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:08 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:58:18 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1760.2819702625275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:10:12 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:31 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:28 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:27:57 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:34:09 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1770.7242813110352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:39:43 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:45:19 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:51:04 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:15 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:27 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1788.470201253891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:30 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:15:01 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:20:48 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:27:15 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:33:27 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1786.4754581451416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:39:18 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:44:39 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:50:40 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:56:54 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:03:04 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1760.1928429603577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:08:37 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:13:57 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:42 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:26:08 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:32:20 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.5564346313477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:37:54 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:43:14 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:48:59 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:55:09 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:01:21 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1741.3217768669128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:06:55 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:12:15 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:18:02 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:24:40 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:30:59 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1779.6554157733917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:36:35 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:42:02 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:47:47 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:54:08 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:00:17 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1758.6298162937164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:05:55 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:11:29 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:17:17 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:23:30 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:30:02 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1790.1825473308563 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:35:44 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:41:06 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:46:54 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:53:05 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:59:15 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1744.6135325431824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:04:48 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:10:07 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:15:57 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:22:22 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:28:36 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1762.3128323554993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:34:11 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:39:38 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:45:23 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:51:48 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:58:16 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1780.6037349700928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:03:52 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:09:19 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:15:07 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:21:16 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:27:27 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1750.8484752178192 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:33:03 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:38:25 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:44:14 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:50:31 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:56:40 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1753.1027908325195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:02:15 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:07:39 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:13:26 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:19:38 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:25:47 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1744.574073791504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:31:20 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:36:51 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:42:38 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:48:55 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:55:09 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1770.2161598205566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:00:50 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:06:17 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:12:04 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:18:17 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:24:28 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1751.6525361537933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:30:01 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:33 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:41:17 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:31 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:53:44 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.1849148273468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:59:18 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:04:40 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:10:26 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:16:44 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:22:54 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1755.6944224834442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:28:33 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:34:26 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:15 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:46:27 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:52:37 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1782.0491938591003 seconds. 
Discarding model... 

Training complete taking 44090.48116517067 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2349941730499268 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (76.94208880674692,), 'R2_train': 0.6272414027211133, 'MAE_train': 6.030523386608814, 'MSE_test': 93.30365765471907, 'R2_test': 0.437582914531251, 'MAE_test': 6.526579210094285}. 
Saved model results as M-M-CZ_Modified-Pauli-CRX_results.json. 
