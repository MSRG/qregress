/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:49 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:15 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:01 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:24 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.7033035755157 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:05 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:27 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:25 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:46 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:59 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1775.9851174354553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:43 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:05 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:00 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:25 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1755.6000516414642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:58:57 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:34 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:10:19 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:35 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:57 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1780.324364900589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:28:39 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:44 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:46:12 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:32 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1772.262221813202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:09 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:30 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:09:18 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:28 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1742.0751712322235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:15 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:34 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:42 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:50 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1750.6310141086578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:24 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:30 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:14:00 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:17 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1765.1235077381134 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:52 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:12 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:35 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:01 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1828.1958801746368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:15 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:01:40 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:13:39 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:19:59 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1756.8027122020721 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:32 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:30:52 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:36:37 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:55 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:04 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1743.8476073741913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:36 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:22 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:27 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:33 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:42 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1781.9075002670288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:24:18 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:29:41 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:35:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:37 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:54 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1775.2780771255493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:53:54 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:21 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:08 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:11:18 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1746.4621455669403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:01 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:23 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:10 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:40:19 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:33 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1743.917694568634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:05 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:28 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:14 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:25 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:36 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1754.682344198227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:20 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:27:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:33:15 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:39:25 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:45:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1790.2286355495453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:10 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:56:30 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:02:17 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:08:30 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:14:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1746.7857422828674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:20:16 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:22 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:37:30 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:43:38 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1735.5618770122528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:49:13 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:54:36 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:00:21 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:06:31 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:12:41 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1742.1267313957214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:18:14 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:23:33 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:29:29 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:35:46 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:41:56 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1763.571660041809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:47:36 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:53:00 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:58:47 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:05:05 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:11:17 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1751.8410835266113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:16:50 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:22:09 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:28:04 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:34:17 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:40:28 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1752.9429075717926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:46:02 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:51:23 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:57:06 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:03:41 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:09:52 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1765.4636664390564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:15:29 2024]  Iteration number: 0 with current cost as 0.330143243325126 and parameters 
[-3.92256529  2.45566371 -2.17891504 -0.11653095  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552006 -1.06648308  1.64200739  1.14432445
  1.31029902 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:20:57 2024]  Iteration number: 0 with current cost as 0.4225171348333484 and parameters 
[-4.11765282  2.48139399 -2.20862744 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960141  1.18552006 -1.06648308  2.09012301  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:26:43 2024]  Iteration number: 0 with current cost as 0.40836499268167353 and parameters 
[-4.09633429  2.52699314 -2.18332985 -0.11653099  0.55388704 -2.77010897
  3.06858495  2.18960141  1.18552002 -1.06648312  2.00897177  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:32:57 2024]  Iteration number: 0 with current cost as 0.403241505669211 and parameters 
[-4.14996819  2.48511876 -2.21020349 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18552002 -1.06648308  1.94611651  1.14432441
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:39:07 2024]  Iteration number: 0 with current cost as 0.26968457887503405 and parameters 
[-3.72290955  2.40477391 -2.17947231 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552004 -1.06648311  1.56242531  1.14432445
  1.31029899 -1.87354677  0.72965078  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1762.899664402008 seconds. 
Discarding model... 

Training complete taking 44041.22332072258 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2353627681732178 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (76.94208880674692,), 'R2_train': 0.6272414027211133, 'MAE_train': 6.030523386608814, 'MSE_test': 93.30365765471907, 'R2_test': 0.437582914531251, 'MAE_test': 6.526579210094285}. 
Saved model results as M-M-CZ_Modified-Pauli-CRX_results.json. 
