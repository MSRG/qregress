/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:25 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:28 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:53 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:54 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:32 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1666.8887584209442 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:10 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:03:12 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:38 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:38 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:13 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1661.8314752578735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:53 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:54 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:27 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:26 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:06 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1675.5170803070068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:48 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:10 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:11 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:48 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1660.0252714157104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:29 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:24 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 19:33:41 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:38 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:10 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.274522781372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:50 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 19:53:49 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:08 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:06 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:38 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.5358793735504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:14 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:20 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:03 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:00 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:33 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1676.5228946208954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:10 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:08 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:56:24 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:23 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:09 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1655.1232287883759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 21:24:03 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:02 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:37 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.432956457138 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:11 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:21 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 21:51:46 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:45 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:02:16 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1662.0172367095947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:52 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 22:11:56 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:12 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:10 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:43 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1647.1617381572723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:19 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:17 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 22:46:35 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:29 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:01 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.008466720581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:47 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:09 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 23:19:13 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:44 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1654.1702947616577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:19 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 23:34:17 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:35 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:35 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 23:52:12 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1662.5414462089539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:57:00 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:57 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 00:09:17 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:20 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 00:19:58 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1653.4236710071564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:24:33 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:29:30 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 00:36:51 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:03 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:39 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1661.9783356189728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:17 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:14 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:04:31 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:28 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:03 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.3387598991394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:19:39 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:37 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:55 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:51 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 01:42:24 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.4728498458862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:02 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:01 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:27 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:04:25 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:01 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1655.3939633369446 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:37 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 02:19:47 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 02:27:09 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:17 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 02:37:52 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1673.992380619049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:32 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 02:47:29 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 02:54:52 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:59:50 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 03:05:23 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.1162133216858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:09:57 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 03:14:52 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 03:22:11 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:12 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 03:33:02 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1666.3738770484924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:37:42 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 03:42:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 03:50:07 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 03:55:10 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:00:45 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1660.9125747680664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:24 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:21 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 04:17:39 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:36 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:08 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1642.319568157196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:32:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 04:45:02 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 04:49:59 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:55:30 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1638.4801180362701 seconds. 
Discarding model... 

Training complete taking 41380.85525012016 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0283408164978027 seconds. 
Saved predicted values as M-M-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (97.64789816067427,), 'R2_train': 0.5269287056005588, 'MAE_train': 8.422954687714341, 'MSE_test': 108.54136299524139, 'R2_test': 0.34573286232255607, 'MAE_test': 8.789224388295931}. 
Saved model results as M-M-CZ_HWE-CNOT_results.json. 
