/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:28 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:27 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:28 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 655.3609457015991 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:23 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:43 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:29 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:32 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:50 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 666.5380244255066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:34 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:59 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:39 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:42 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:01 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 673.8880400657654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:50 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:13 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:55 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:55 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:09 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 668.3339192867279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:52 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:16 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:55 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:58 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:13 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 664.3218448162079 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:58 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:19 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:05 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:46 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 688.252827167511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:29 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:30 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:46 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 663.0984876155853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:27 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:57 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:44 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:52 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:08 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 678.522159576416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:01 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:26 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:05 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:08 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:30 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 683.9136664867401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:17 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:35 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:36 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 666.9373459815979 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:19 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:39 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:40 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:00 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 683.3511047363281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:46 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:04 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:45 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:50 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:13 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 671.4153139591217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:01 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:20 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:04 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:06 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:23 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 670.4891378879547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:10 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:31 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:22 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:06:34 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.4562232494354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:09:14 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:34 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:25 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:27 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:45 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 671.7965207099915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:20:27 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:58 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:34 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:36 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:44 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 656.496887922287 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:20 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:46 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:31 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:40 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:21 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 700.014340877533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:09 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:26 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:11 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:12 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:51:28 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 662.721272945404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:54:08 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:28 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:13 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:11 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:29 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 662.1806318759918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:05:20 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:39 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.7529783248901 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:16:33 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:58 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:42 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:39 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:57 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 677.3459823131561 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:41 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:10 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:59 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:09 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:21 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 684.5922842025757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:12 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:40:35 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:38 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:38 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 699.9298319816589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:50:54 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:13 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:55:02 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:58 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:28 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 684.4848065376282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:09 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 22:03:31 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:24 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:40 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 673.2949955463409 seconds. 
Discarding model... 

Training complete taking 16852.49025774002 total seconds. 
Now scoring model... 
Scoring complete taking 2.461081027984619 seconds. 
Saved predicted values as A2-A2-CZ_ESU2_predicted_values.csv
Model scores: {'MSE_train': (297.79299261890327,), 'R2_train': -0.4427071051698612, 'MAE_train': 12.784712056259519, 'MSE_test': 277.12212436485027, 'R2_test': -0.670440596026348, 'MAE_test': 11.805573531587578}. 
Saved model results as A2-A2-CZ_ESU2_results.json. 
