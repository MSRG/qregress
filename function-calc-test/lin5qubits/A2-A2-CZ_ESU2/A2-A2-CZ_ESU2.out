/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:28 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:27 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:28 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 655.3609457015991 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:23 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:43 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:29 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:32 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:50 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 666.5380244255066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:34 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:59 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:39 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:42 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:01 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 673.8880400657654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:50 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:13 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:55 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:55 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:09 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 668.3339192867279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:52 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:16 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:55 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:58 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:13 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 664.3218448162079 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:58 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:19 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:05 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:46 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 688.252827167511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:29 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:30 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:46 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 663.0984876155853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:27 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:57 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:44 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:52 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:08 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 678.522159576416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:01 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:26 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:05 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:08 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:30 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 683.9136664867401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:17 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:35 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:36 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 666.9373459815979 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:19 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:39 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:40 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:00 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 683.3511047363281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:46 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:04 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:45 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:50 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:13 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 671.4153139591217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:01 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:20 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:04 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:06 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:23 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 670.4891378879547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:10 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:31 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:22 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:23 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:06:34 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.4562232494354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:09:14 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:34 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:25 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:27 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:45 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 671.7965207099915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:20:27 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:58 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:34 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:36 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:44 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 656.496887922287 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:20 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:46 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:31 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:40 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:21 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 700.014340877533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:09 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:26 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:11 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:12 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 20:51:28 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 662.721272945404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:54:08 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:28 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:13 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:11 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:29 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 662.1806318759918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:05:20 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:39 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.7529783248901 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:16:33 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:58 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:42 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:39 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:57 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 677.3459823131561 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:41 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:10 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:59 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:09 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:21 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 684.5922842025757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:12 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:40:35 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:38 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:38 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 699.9298319816589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:50:54 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:13 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 21:55:02 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:58 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:28 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 684.4848065376282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:09 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Sun Mar 24 22:03:31 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:21 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:24 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:40 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 673.2949955463409 seconds. 
Discarding model... 

Training complete taking 16852.49025774002 total seconds. 
Now scoring model... 
Scoring complete taking 2.461081027984619 seconds. 
Saved predicted values as A2-A2-CZ_ESU2_predicted_values.csv
Model scores: {'MSE_train': (297.79299261890327,), 'R2_train': -0.4427071051698612, 'MAE_train': 12.784712056259519, 'MSE_test': 277.12212436485027, 'R2_test': -0.670440596026348, 'MAE_test': 11.805573531587578}. 
Saved model results as A2-A2-CZ_ESU2_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:38:56 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:20 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 11:43:11 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 11:45:20 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 11:47:40 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 694.5212330818176 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:50:24 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:49 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 11:54:32 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 11:56:37 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 11:58:58 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 678.6030964851379 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:01:45 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:03:08 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 12:06:00 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 12:08:09 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 12:10:29 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 687.4651889801025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:13:13 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:14:35 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 12:17:24 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 12:19:30 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:50 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 682.4146227836609 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:24:29 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:25:47 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:36 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:39 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 12:32:57 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 667.8436031341553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:37 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:58 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:33 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:32 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:47 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 650.9905519485474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:46:30 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:50 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:33 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:32 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:45 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 658.3008313179016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:29 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 12:58:53 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:41 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:45 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 13:06:04 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 675.6015660762787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:48 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 13:10:11 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:56 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:11 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 13:17:31 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 688.1215269565582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:20:16 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 13:21:42 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:24:29 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 13:26:34 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 13:28:54 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 685.58034324646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:31:43 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 13:33:06 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:35:51 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 13:37:55 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 13:40:17 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 681.660649061203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:02 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 13:44:26 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:47:12 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 13:49:16 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 13:51:33 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 674.8583948612213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:54:24 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 13:55:53 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 13:58:46 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:00:49 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 14:03:09 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 696.1582398414612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:52 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:21 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 14:10:11 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:12:25 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 14:14:48 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 700.2070093154907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:36 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 14:19:04 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:57 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:03 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:22 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 691.9375841617584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:09 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 14:30:33 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 14:33:22 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:35:30 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:50 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 688.6750712394714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:40:41 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 14:42:06 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 14:44:57 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:47:04 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 14:49:25 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 693.9935691356659 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:14 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:37 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 14:56:26 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:37 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:01:05 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 702.8480525016785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:50 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:11 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 15:07:55 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:58 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:12:13 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 664.7037925720215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:14:56 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 15:16:17 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:02 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:06 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:23:23 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.5643584728241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:12 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 15:27:33 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 15:30:17 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 15:32:21 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:34:39 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 674.6880347728729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:37:35 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 15:38:57 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:46 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 15:43:57 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:15 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 695.392083644867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:49:00 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 15:50:23 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 15:53:10 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 15:55:15 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 15:57:37 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 681.3870944976807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:00:20 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 16:01:39 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 16:04:24 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 16:06:30 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 16:08:49 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 672.4573440551758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:11:38 2024]  Iteration number: 0 with current cost as 0.3306152226302728 and parameters 
[-4.55106017  2.23743119 -2.12427964 -0.11653103  0.55388364 -2.77011241
  3.06858498  2.18959801  1.18552343 -1.06648653]. 
Working on 0.4 fold... 
[Thu Apr  4 16:13:03 2024]  Iteration number: 0 with current cost as 0.7426600868432873 and parameters 
[-0.16456592  2.23743448 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324]. 
Working on 0.6 fold... 
[Thu Apr  4 16:15:45 2024]  Iteration number: 0 with current cost as 0.46871654136845775 and parameters 
[-1.72522141  2.23743464 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316]. 
Working on 0.8 fold... 
[Thu Apr  4 16:17:42 2024]  Iteration number: 0 with current cost as 0.47286224336328664 and parameters 
[-1.78648354  2.23743464 -2.12427964 -0.11653103  0.55388701 -2.77010911
  3.06858498  2.18960138  1.18551998 -1.06648322]. 
Working on 1.0 fold... 
[Thu Apr  4 16:19:58 2024]  Iteration number: 0 with current cost as 0.47006467472300045 and parameters 
[-1.60328955  2.23743454 -2.12427954 -0.11653093  0.55388708 -2.77010907
  3.06858489  2.18960155  1.18552008 -1.06648318]. 
Training complete taking 669.5789713859558 seconds. 
Discarding model... 

Training complete taking 17030.553553819656 total seconds. 
Now scoring model... 
Scoring complete taking 2.3178417682647705 seconds. 
Saved predicted values as A2-A2-CZ_ESU2_predicted_values.csv
Model scores: {'MSE_train': (297.79299261890327,), 'R2_train': -0.4427071051698612, 'MAE_train': 12.784712056259519, 'MSE_test': 277.12212436485027, 'R2_test': -0.670440596026348, 'MAE_test': 11.805573531587578}. 
Saved model results as A2-A2-CZ_ESU2_results.json. 
