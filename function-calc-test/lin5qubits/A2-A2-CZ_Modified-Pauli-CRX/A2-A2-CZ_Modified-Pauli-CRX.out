/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:04 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:35 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:22 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:37 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2432.6251304149628 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:40 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:56 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:35 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:49 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:41:50 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.5642273426056 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:48 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:07 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:51 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:10 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:10 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2407.5320720672607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:55 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:42 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:00 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:23 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:23 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2485.1465570926666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:20 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:31 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:08 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:31 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:43:37 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2408.240451812744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:28 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:45 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:25 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:23:48 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2417.8486297130585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:49 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:05 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:41 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:50 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:48 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2397.321219444275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:13:45 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:21:59 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:54 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:38:35 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:45 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2450.2119433879852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:34 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:02:48 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:10:30 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:45 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2404.127531528473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:38 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:01 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:50:43 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:58:55 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:56 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2407.1565175056458 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:14:46 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:22:57 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:43 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:39:04 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:45:30 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2435.0323827266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:35 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:04:02 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:11:58 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:09 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:26:10 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2439.2037217617035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:36:00 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:44:17 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:54 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:00:21 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:06:35 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2425.29350233078 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:26 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:00 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:32:50 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:41:30 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:47:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2462.083827018738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:57:27 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:05:46 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:13:37 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:22:07 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:28:14 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2439.528915643692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:38:07 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:46:21 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:59 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:02:18 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:31 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.164631843567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:18:18 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:34:23 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:42:35 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:48:36 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2405.908566713333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:58:24 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:06:47 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:14:24 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:22:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2411.0916600227356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:36 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:46:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:55:06 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:03:19 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:09:29 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2481.2047357559204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:19:57 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:28:16 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:35:57 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:44:11 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:50:12 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.4078521728516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:00:10 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:08:26 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:16:18 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:24:48 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:30:49 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2428.8847649097443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:40:37 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:48:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:56:27 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:04:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:10:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2398.1699619293213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:20:36 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:28:48 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:36:21 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:44:37 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:50:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2405.3573632240295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:00:41 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:09:19 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:16:53 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 09:25:04 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:31:03 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.012710094452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:40:54 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:49:06 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:56:44 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 10:04:57 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 10:10:59 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2418.0408976078033 seconds. 
Discarding model... 

Training complete taking 60609.16141462326 total seconds. 
Now scoring model... 
Scoring complete taking 1.2336456775665283 seconds. 
Saved predicted values as A2-A2-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (205.9990105339206,), 'R2_train': 0.0020039305102743654, 'MAE_train': 12.461906276603795, 'MSE_test': 203.35328958026008, 'R2_test': -0.22577578758433448, 'MAE_test': 12.206233862302794}. 
Saved model results as A2-A2-CZ_Modified-Pauli-CRX_results.json. 
