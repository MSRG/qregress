/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:04 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:35 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:22 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:37 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2432.6251304149628 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:40 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:56 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:35 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:49 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:41:50 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.5642273426056 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:48 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:07 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:51 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:10 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:10 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2407.5320720672607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:55 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:42 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:00 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:23 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:23 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2485.1465570926666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:20 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:31 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:08 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:31 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:43:37 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2408.240451812744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:28 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:45 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:25 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:23:48 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2417.8486297130585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:49 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:05 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:41 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:50 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:48 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2397.321219444275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:13:45 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:21:59 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:54 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:38:35 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:45 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2450.2119433879852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:34 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:02:48 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:10:30 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:45 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2404.127531528473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:38 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:01 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:50:43 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:58:55 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:56 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2407.1565175056458 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:14:46 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:22:57 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:43 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:39:04 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:45:30 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2435.0323827266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:35 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:04:02 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:11:58 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:09 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:26:10 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2439.2037217617035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:36:00 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:44:17 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:54 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:00:21 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:06:35 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2425.29350233078 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:26 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:00 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:32:50 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:41:30 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:47:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2462.083827018738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:57:27 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:05:46 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:13:37 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:22:07 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:28:14 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2439.528915643692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:38:07 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:46:21 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:59 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:02:18 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:31 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.164631843567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:18:18 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:34:23 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:42:35 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:48:36 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2405.908566713333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:58:24 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:06:47 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:14:24 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:22:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2411.0916600227356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:36 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:46:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:55:06 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:03:19 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:09:29 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2481.2047357559204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:19:57 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:28:16 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:35:57 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:44:11 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:50:12 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.4078521728516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:00:10 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:08:26 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:16:18 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:24:48 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:30:49 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2428.8847649097443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:40:37 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:48:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:56:27 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:04:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:10:41 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2398.1699619293213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:20:36 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:28:48 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:36:21 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:44:37 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:50:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2405.3573632240295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:00:41 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:09:19 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:16:53 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 09:25:04 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:31:03 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.012710094452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:40:54 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:49:06 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:56:44 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 10:04:57 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 10:10:59 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2418.0408976078033 seconds. 
Discarding model... 

Training complete taking 60609.16141462326 total seconds. 
Now scoring model... 
Scoring complete taking 1.2336456775665283 seconds. 
Saved predicted values as A2-A2-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (205.9990105339206,), 'R2_train': 0.0020039305102743654, 'MAE_train': 12.461906276603795, 'MSE_test': 203.35328958026008, 'R2_test': -0.22577578758433448, 'MAE_test': 12.206233862302794}. 
Saved model results as A2-A2-CZ_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:34 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:45:50 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:34 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:01:45 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:07:46 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2412.5415642261505 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:17:39 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:25:50 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:33:26 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:43 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:47:43 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2394.8945801258087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:33 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:44 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:21 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:21:32 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:28 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2380.7866535186768 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:37:15 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:45:39 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:53:55 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:02:06 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:08:20 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2464.1356387138367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:18:18 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:26:27 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:33:59 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:42:17 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:48:40 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2431.1183812618256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:58:49 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:07:02 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:14:35 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:22:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:28:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2381.5846881866455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:38:30 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:46:49 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:23 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:02:36 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:08:44 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2400.5830132961273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:18:31 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:26:41 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:34:16 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:42:26 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:48:27 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2383.0574474334717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:58:14 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:06:25 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:14:01 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:22:11 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:22 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2395.8929600715637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:38:10 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:46:37 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:54:12 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:02:25 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:09:01 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2438.385483264923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:18:48 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:27:01 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:34:34 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:42:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:48:43 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2383.719360113144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:58:33 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:06:46 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:14:21 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:22:31 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:28:40 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2404.9828481674194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:38:43 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:46:58 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:54:33 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:02:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:08:38 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2386.4557406902313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:18:23 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:26:36 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:25 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:42:39 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:48:38 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2398.4702739715576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:58:23 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:06:39 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:14:25 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:23:10 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:29:10 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2436.4232025146484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:01 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:47:10 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:54:56 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:03:17 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:09:18 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2407.140429019928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:19:12 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:26 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:35:03 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:43:16 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:49:18 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2406.7100415229797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:59:13 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:07:38 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:15:30 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:23:40 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:29:43 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2419.756146669388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:39:32 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:48:03 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:55:44 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:03:54 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:09:52 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2406.2494831085205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:19:38 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:28:00 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:35:45 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:44:01 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:49:59 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2419.252025604248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:59:58 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:08:30 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:16:05 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:24:44 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:30:54 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2451.9499151706696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:40:50 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:49:27 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:57:14 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:05:35 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:11:42 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2446.3814249038696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:21:37 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:29:46 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:37:19 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:45:29 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:51:30 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2384.4349942207336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:01:20 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:10:12 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:18:00 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:26:45 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:32:56 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2484.332382917404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:42:44 2024]  Iteration number: 0 with current cost as 0.47135810647765936 and parameters 
[-3.27903931  2.37836306 -2.1782082  -0.11653104  0.55388706 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664831   1.05998395  1.14432445
  1.31029899 -1.8735468   0.72965077  2.88578418 -0.54534334 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:50:58 2024]  Iteration number: 0 with current cost as 0.4383836296828129 and parameters 
[-3.17940239  2.30339831 -2.17917971 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  1.00332401  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:59:07 2024]  Iteration number: 0 with current cost as 0.4918741844203418 and parameters 
[-3.11760023  2.2863818  -2.17150546 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648309  0.93127466  1.14432446
  1.31029898 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:07:55 2024]  Iteration number: 0 with current cost as 0.47579778261759487 and parameters 
[-3.13784857  2.29081186 -2.17560695 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.94949954  1.14432445
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:13:52 2024]  Iteration number: 0 with current cost as 0.4844827455680756 and parameters 
[-3.13595569  2.29526083 -2.17093747 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960147  1.18552    -1.06648308  0.94307188  1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2454.804741382599 seconds. 
Discarding model... 

Training complete taking 60374.04562330246 total seconds. 
Now scoring model... 
Scoring complete taking 1.2368595600128174 seconds. 
Saved predicted values as A2-A2-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (205.9990105339206,), 'R2_train': 0.0020039305102743654, 'MAE_train': 12.461906276603795, 'MSE_test': 203.35328958026008, 'R2_test': -0.22577578758433448, 'MAE_test': 12.206233862302794}. 
Saved model results as A2-A2-CZ_Modified-Pauli-CRX_results.json. 
