/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:06 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:13 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:40 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:59 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2411.857063770294 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:59 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:16 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:28:03 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:23 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:08 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2386.6062712669373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:45 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:18 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:49 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:26 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2419.217806339264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:05 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:28 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:42 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:52 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2365.1709530353546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:31 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:16:57 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:27:03 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:27 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:42:08 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2358.5001895427704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:48 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:01 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:11 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:24 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:50 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2317.422255039215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:25 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:34:49 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:50 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:52:12 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:22 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2375.919346809387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:03 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:26 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:35 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:54 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2365.287621974945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:27 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:53:43 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:03:56 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:19:04 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2353.8314394950867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:26:42 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:32:55 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:55 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:50:06 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:57 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2336.666961669922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:05:38 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:12:11 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:22:22 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:29:37 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:37:18 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2355.859765291214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:53 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:51:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:01:04 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:08:29 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:56 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2320.0644063949585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:23:33 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:29:46 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:11 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:47:54 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:56:02 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2431.314536333084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:04:05 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:10:18 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:20:20 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:27:37 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:10 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2323.0116953849792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:49:02 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:59:09 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:06:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:14:02 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2334.7044093608856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:21:42 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:28:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:38:12 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:45:31 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:52:56 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2336.5658605098724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:00:39 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:07:25 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:17:27 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:24:43 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:32:10 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2347.594261407852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:39:47 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:46:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:56:07 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:03:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:10:59 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2327.928037881851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:18:36 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:24:50 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:35:09 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:42:44 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:50:11 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2354.2653353214264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:57:48 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:04:10 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:14:23 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:21:40 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:29:09 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2338.0118136405945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:36:46 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:43:23 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:53:46 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:01:02 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:08:25 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2356.013996362686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:16:03 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:32:39 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:39:55 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:47:25 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2339.563533782959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:55:02 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:01:15 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:11:15 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:18:30 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:25:57 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2312.861347913742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:33:35 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:39:50 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:49:52 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:57:08 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:04:34 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2312.4258041381836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:12:08 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:18:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:25 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 09:35:40 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:43:44 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2380.64159488678 seconds. 
Discarding model... 

Training complete taking 58861.307995557785 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1070077419281006 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (106.05452996922868,), 'R2_train': 0.4862013958875683, 'MAE_train': 9.312492209376204, 'MSE_test': 154.82129384713758, 'R2_test': 0.06676605137779534, 'MAE_test': 10.181632067275974}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:18 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:43:37 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:54 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:01:10 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:08:37 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2338.3530538082123 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:17 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:22:34 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:32:43 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:59 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:47:49 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2349.927190542221 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:55:26 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:44 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:11:48 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:06 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:26:42 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2332.3596954345703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:23 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:40:36 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:37 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:59 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:05:27 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2328.373894929886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:05 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:19:27 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:29:37 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:13 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:41 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2353.536687850952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:19 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:58:35 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:08:38 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:01 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:23:30 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2328.242779970169 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:31:07 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:37:39 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:48:27 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:55:44 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:03:26 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2397.842273235321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:11:06 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:17:21 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:31 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:50 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:42:15 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2330.3278152942657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:49:55 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:56:19 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:06:21 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:13:42 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:21:12 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2338.303740978241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:28:55 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:15 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:45:16 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:52:51 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:00:18 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2354.664463043213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:08:13 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:14:30 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:24:46 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:32:02 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:39:56 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2370.514415025711 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:47:41 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:53:55 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:04:16 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:11:37 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:19:09 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2360.0495595932007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:26:59 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:33:26 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:43:28 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:50:43 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:58:10 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2347.950399875641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:06:07 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:12:53 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:23:03 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:30:24 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:37:53 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2363.13929438591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:45:30 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:51:41 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:01:47 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:09:31 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:17:06 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2354.770978450775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:24:45 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:31:19 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:41:25 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:48:43 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:56:11 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2343.871739387512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:03:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:10:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:20:07 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:27:27 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:35:00 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2333.8804404735565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:42:42 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:49:01 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:59:09 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:06:30 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:27 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2385.0579538345337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:22:29 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:28:43 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:39:07 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:46:26 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:53:55 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2342.741728782654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:01:32 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:07:46 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:18:06 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:25:23 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:33:01 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2350.9113352298737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:40:41 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:46:57 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:57:00 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:04:14 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:11:42 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2332.840313911438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:19:35 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:25:49 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:36:21 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:43:38 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:51:15 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2382.969369649887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:59:17 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:30 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:15:40 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:22:56 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:30:25 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2331.1490790843964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:38:15 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:44:38 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:55:23 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:02:43 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:10:11 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2382.068984270096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:17:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:24:08 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:34:19 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:41:44 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:49:11 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2337.68501830101 seconds. 
Discarding model... 

Training complete taking 58771.53411889076 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.8325676918029785 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (106.05452996922868,), 'R2_train': 0.4862013958875683, 'MAE_train': 9.312492209376204, 'MSE_test': 154.82129384713758, 'R2_test': 0.06676605137779534, 'MAE_test': 10.181632067275974}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRX_results.json. 
