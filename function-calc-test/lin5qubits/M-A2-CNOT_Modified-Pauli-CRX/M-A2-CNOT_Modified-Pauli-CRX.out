/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:06 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:13 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:40 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:59 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2411.857063770294 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:59 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:16 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:28:03 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:23 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:08 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2386.6062712669373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:45 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:18 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:49 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:26 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2419.217806339264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:05 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:28 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:42 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:52 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2365.1709530353546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:31 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:16:57 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:27:03 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:27 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:42:08 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2358.5001895427704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:48 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:01 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:11 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:24 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:50 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2317.422255039215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:25 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:34:49 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:50 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:52:12 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:22 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2375.919346809387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:03 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:26 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:35 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:54 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2365.287621974945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:27 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:53:43 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:03:56 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:19:04 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2353.8314394950867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:26:42 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:32:55 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:55 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:50:06 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:57 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2336.666961669922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:05:38 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:12:11 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:22:22 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:29:37 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:37:18 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2355.859765291214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:53 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:51:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:01:04 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:08:29 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:56 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2320.0644063949585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:23:33 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:29:46 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:11 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:47:54 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:56:02 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2431.314536333084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:04:05 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:10:18 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:20:20 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:27:37 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:10 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2323.0116953849792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:50 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:49:02 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:59:09 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:06:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:14:02 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2334.7044093608856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:21:42 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:28:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:38:12 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:45:31 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:52:56 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2336.5658605098724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:00:39 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:07:25 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:17:27 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:24:43 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:32:10 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2347.594261407852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:39:47 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:46:03 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:56:07 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:03:25 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:10:59 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2327.928037881851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:18:36 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:24:50 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:35:09 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:42:44 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:50:11 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2354.2653353214264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:57:48 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:04:10 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:14:23 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:21:40 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:29:09 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2338.0118136405945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:36:46 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:43:23 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:53:46 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:01:02 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:08:25 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2356.013996362686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:16:03 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 07:32:39 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 07:39:55 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 07:47:25 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2339.563533782959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:55:02 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:01:15 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:11:15 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:18:30 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 08:25:57 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2312.861347913742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:33:35 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:39:50 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 08:49:52 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 08:57:08 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:04:34 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2312.4258041381836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:12:08 2024]  Iteration number: 0 with current cost as 0.2799162571634812 and parameters 
[-4.41759715  1.81201558 -2.37189699 -0.11653103  0.55388699 -2.77010914
  3.0685849   2.18960145  1.18551998 -1.06648317  2.71589357  1.14432445
  1.31029899 -1.87354671  0.7296508   2.88578419 -0.54534318 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:18:22 2024]  Iteration number: 0 with current cost as 0.3835471610131548 and parameters 
[-3.41294666  2.00146655 -2.20776738 -0.1165311   0.55388708 -2.77010905
  3.06858495  2.18960145  1.18551998 -1.06648316  1.85368949  1.14432441
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:25 2024]  Iteration number: 0 with current cost as 0.3767606116350196 and parameters 
[-3.5656345   2.01415459 -2.23519092 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18552002 -1.06648312  1.71669496  1.14432441
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 09:35:40 2024]  Iteration number: 0 with current cost as 0.3861115412843048 and parameters 
[-3.51317206  2.00564644 -2.23108352 -0.11653114  0.55388704 -2.77010905
  3.06858495  2.18960145  1.18552002 -1.06648316  1.74847951  1.14432434
  1.31029895 -1.87354684  0.72965069  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 09:43:44 2024]  Iteration number: 0 with current cost as 0.40865790570041527 and parameters 
[-3.43054519  2.00058329 -2.22317402 -0.11653106  0.55388708 -2.77010905
  3.06858498  2.18960145  1.18551998 -1.06648316  1.74992995  1.14432438
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 2380.64159488678 seconds. 
Discarding model... 

Training complete taking 58861.307995557785 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1070077419281006 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (106.05452996922868,), 'R2_train': 0.4862013958875683, 'MAE_train': 9.312492209376204, 'MSE_test': 154.82129384713758, 'R2_test': 0.06676605137779534, 'MAE_test': 10.181632067275974}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRX_results.json. 
