/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:38 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:16 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:23 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:44 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:07 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1272.3941869735718 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:45 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:18 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:31 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:55 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:20 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1276.2749106884003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:02 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:37 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:58 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:28 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:59 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1298.4560267925262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:44 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:29 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:48 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:50:16 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:44 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1304.3101823329926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:58:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:17 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:39 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:14 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:38 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1315.276850938797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:21 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:08 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:20 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:54 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1337.5239427089691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:42 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:45:21 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:44 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:26 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:00:54 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.713958978653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:42 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:54 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:19 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:22:57 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1324.7052998542786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:44 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:18 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:42 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:40:09 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:45 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1305.173567533493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:46 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:27 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:55 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:22 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:49 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1327.827168226242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:35 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:19:29 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:00 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:28:44 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1311.359873533249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:42 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:10 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:42 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.4423446655273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:32 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:57:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:48 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1331.5686025619507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:16:35 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:19:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:47 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1309.793855190277 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:38:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:03 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:38 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1316.4311075210571 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:21 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:27 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:03 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:28 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1311.9442727565765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:22:22 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:05 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:31:33 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:13 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:39 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1328.621229171753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:44:26 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:47:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:53:44 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:58:18 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:02:47 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1328.7144253253937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:32 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:09:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:15:45 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:16 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:25:02 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1333.3230686187744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:52 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:50 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:32 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:02 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1319.0633997917175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:50:49 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:53:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:59:59 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:04:38 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:09:16 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1333.7404642105103 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:11 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:15:51 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:22:19 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:27:01 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:40 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1345.8870141506195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:35:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:44:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:49:05 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:53:41 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1319.4751751422882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:57:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:00:06 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:06:29 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:11:02 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:15:34 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1313.2617807388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:19:18 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:21:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:28:19 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:51 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:37:16 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1301.3380167484283 seconds. 
Discarding model... 

Training complete taking 32903.62142133713 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.7548296451568604 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.7315077379052,), 'R2_train': -0.5635258411587694, 'MAE_train': 16.553766460279537, 'MSE_test': 301.15156108794497, 'R2_test': -0.8152855689562502, 'MAE_test': 16.30447737035846}. 
Saved model results as M-M-CZ_Efficient-CRZ_results.json. 
