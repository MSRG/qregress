/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:38 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:16 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:23 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:44 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:07 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1272.3941869735718 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:45 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:18 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:31 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:55 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:20 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1276.2749106884003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:02 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:37 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:58 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:28 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:59 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1298.4560267925262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:44 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:29 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:48 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:50:16 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:44 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1304.3101823329926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:58:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:17 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:39 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:14 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:38 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1315.276850938797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:21 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:08 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:20 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:54 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1337.5239427089691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:42 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:45:21 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:44 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:26 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:00:54 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.713958978653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:42 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:54 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:19 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:22:57 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1324.7052998542786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:44 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:18 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:42 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:40:09 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:45 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1305.173567533493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:46 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:27 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:55 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:22 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:49 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1327.827168226242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:35 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:19:29 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:00 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:28:44 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1311.359873533249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:42 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:10 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:42 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.4423446655273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:32 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:57:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:48 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1331.5686025619507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:16:35 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:19:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:47 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1309.793855190277 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:38:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:03 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:38 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1316.4311075210571 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:21 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:27 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:03 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:28 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1311.9442727565765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:22:22 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:05 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:31:33 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:13 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:39 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1328.621229171753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:44:26 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:47:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:53:44 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:58:18 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:02:47 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1328.7144253253937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:32 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:09:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:15:45 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:16 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:25:02 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1333.3230686187744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:52 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:50 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:32 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:02 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1319.0633997917175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:50:49 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:53:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:59:59 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:04:38 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:09:16 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1333.7404642105103 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:11 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:15:51 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:22:19 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:27:01 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:40 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1345.8870141506195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:35:31 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:44:34 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:49:05 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:53:41 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1319.4751751422882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:57:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:00:06 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:06:29 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:11:02 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:15:34 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1313.2617807388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:19:18 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:21:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:28:19 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:51 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:37:16 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1301.3380167484283 seconds. 
Discarding model... 

Training complete taking 32903.62142133713 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.7548296451568604 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.7315077379052,), 'R2_train': -0.5635258411587694, 'MAE_train': 16.553766460279537, 'MSE_test': 301.15156108794497, 'R2_test': -0.8152855689562502, 'MAE_test': 16.30447737035846}. 
Saved model results as M-M-CZ_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:39:38 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:42:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:49:04 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:53:51 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:58:32 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1376.0319967269897 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:05:16 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:12 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:16:54 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:40 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1394.35595536232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:46 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:32 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:35:42 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:27 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:18 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1416.542315006256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:49:10 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:58:16 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:45 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:24 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1322.4629321098328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:09 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:13:58 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:20:48 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:25:38 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:30:28 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1382.4377493858337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:19 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:08 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:43:53 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:53 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:35 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1390.5551855564117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:57:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:00:12 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:50 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:11:18 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:15:55 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1338.7167239189148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:19:51 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:22:35 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:28:57 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:33:30 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:56 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1322.3594179153442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:41:45 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:19 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:30 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:54:56 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:59:17 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1275.9451413154602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:02:53 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:33 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:12:00 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:28 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:21:08 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1311.9443593025208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:24:57 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:27:38 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:34:02 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:38:32 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:00 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1312.2160212993622 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:46:40 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:49:22 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:51 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:00:21 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:58 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1317.5050659179688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:08:35 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:11:10 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:17:40 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:22:03 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:26:27 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1291.0029180049896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:30:16 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:33:04 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:35 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:44:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:48:40 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1331.6038687229156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:52:19 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:54:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:01:07 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:05:32 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:10:01 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1280.3530282974243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:13:39 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:16:12 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:22:37 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:27:02 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:27 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1286.8521897792816 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:35:13 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:37:56 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:44:19 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:48:41 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:18 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1309.6358878612518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:57:00 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:59:36 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:05:51 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:10:12 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:14:43 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1284.0329275131226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:18:27 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:21:05 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:27:14 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:31:40 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:06 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1285.6819715499878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:39:48 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:42:22 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:48:30 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:52:51 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:57:11 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1262.0889937877655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:00:48 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:03:19 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:09:29 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:13:48 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:18:10 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1258.6169826984406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:21:52 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:24:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:30:36 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:34:51 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:39:11 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1261.8651928901672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:42:47 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:45:27 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:51:33 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:55:52 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:00:15 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1264.0298056602478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:03:53 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:06:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:12:41 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:17:10 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:21:48 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1298.0589616298676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:25:33 2024]  Iteration number: 0 with current cost as 0.5502979017751133 and parameters 
[-1.74536142  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.6027151   1.14432445
  1.31029921 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:28:05 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743448 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432429
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:23 2024]  Iteration number: 0 with current cost as 0.1976313409092198 and parameters 
[64.41734463  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77012935
  3.06857479  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:38:50 2024]  Iteration number: 0 with current cost as 0.20134476341335295 and parameters 
[ 1.53284675  2.23743313 -2.12427964 -0.11653178  0.55388708 -2.77011122
  3.06858273  2.1896007   1.18552073 -1.06648383  0.60271435  1.1443237
  1.31029824 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:43:11 2024]  Iteration number: 0 with current cost as 0.508676380228498 and parameters 
[-1.61121049  2.23743464 -2.1242793  -0.11653103  0.55388708 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648308  0.60271527  1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1278.0880415439606 seconds. 
Discarding model... 

Training complete taking 32852.984280109406 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.5570425987243652 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.7315077379052,), 'R2_train': -0.5635258411587694, 'MAE_train': 16.553766460279537, 'MSE_test': 301.15156108794497, 'R2_test': -0.8152855689562502, 'MAE_test': 16.30447737035846}. 
Saved model results as M-M-CZ_Efficient-CRZ_results.json. 
