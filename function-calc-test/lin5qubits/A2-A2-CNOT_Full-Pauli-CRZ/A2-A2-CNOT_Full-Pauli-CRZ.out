/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:05 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:31 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:19 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:30 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:21 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1924.7097086906433 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:02:09 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:25 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:56 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:06 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:00 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1926.9578149318695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:34:17 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:28 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:45 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:59 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.88538813591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:47 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:04 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:18:11 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1873.7585124969482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:37:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:11 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:18 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:31 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:25 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1872.9968621730804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:08:14 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:22 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:31 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:49 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:49 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1884.056087255478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:39:39 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:49 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:09 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:25 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:18 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.750769853592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:13 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:29 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:23:39 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:59 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:50 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1889.224758386612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:42:39 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:48 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:55:02 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:01:15 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:09 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1889.2204308509827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:08 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:20:16 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:26:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:32:33 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:22 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1863.524650812149 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:45:12 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:22 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:34 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:03:44 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:10:35 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1873.8165369033813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:24 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:22:36 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:28:50 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:35:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:42:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1900.4066078662872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:05 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:17 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:00:27 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:06:36 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:13:26 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1866.354547739029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:12 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:25:40 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:31:51 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:01 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:44:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.4291574954987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:50:43 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:56:51 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:00 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:10 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:16:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1868.0301487445831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:51 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:28:13 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:34:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:38 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:47:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1912.0360040664673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:53:42 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:59:57 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:06:03 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:12:09 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:19:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1866.4636554718018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:24:50 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:56 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:37:07 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:43:34 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:51 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1912.9210872650146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:56:41 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:02:54 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:09:04 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:15:13 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:22:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1869.5780668258667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:52 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:34:09 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:40:40 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:50 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:53:56 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1911.1745524406433 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:59:42 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:05:51 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:12:02 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:18:12 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:25:13 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1878.534143447876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:31:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:31 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:37 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:50:10 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:57:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1920.1718068122864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:03:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:09:49 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:15:57 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:22:03 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:50 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1898.7170782089233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:34:40 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:41:15 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:47:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:53:54 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:01:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1934.1935501098633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:07:08 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:13:21 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 06:19:32 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:25:41 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:32:31 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1903.160837173462 seconds. 
Discarding model... 

Training complete taking 47312.07443404198 total seconds. 
Now scoring model... 
Scoring complete taking 0.9834396839141846 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (102.82277698041185,), 'R2_train': 0.5018581544906382, 'MAE_train': 9.204393674647502, 'MSE_test': 130.68886215951196, 'R2_test': 0.21223185878756112, 'MAE_test': 9.835769608290548}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRZ_results.json. 
