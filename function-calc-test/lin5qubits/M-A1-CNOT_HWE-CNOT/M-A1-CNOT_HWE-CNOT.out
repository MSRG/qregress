/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:59 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:11 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:12 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:26 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:09 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 17:42:15 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 888.8647081851959 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:44:58 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:57 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:13 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:01 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:07 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 891.453239440918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:51 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:56 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:23 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:06 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:14 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 909.1594505310059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:59 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:00 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:18 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:02 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:32 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 916.3252992630005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:16 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:19 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:34 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:19 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:24 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 893.2823538780212 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:08 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:08 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:25 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:08 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 18:57:13 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 899.6864664554596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:08 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:08 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:39 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:30 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 19:12:37 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 915.2103111743927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:25 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 19:18:31 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:49 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:33 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:42 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 901.5455894470215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:26 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:27 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:43 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:36 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:38 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 895.6981461048126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:22 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:19 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:32 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:13 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 19:57:14 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 874.9340925216675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:57 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 20:02:55 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:08 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:53 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:56 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 883.3625934123993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:38 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:37 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:51 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:53 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 20:26:56 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 898.7879993915558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:29:37 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:36 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:56 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:38 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:42 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 907.416531085968 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:59 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:01 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:15 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:53:59 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:03 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 902.5571825504303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:47 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 21:03:02 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:15 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:09:00 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:06 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 912.300790309906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:01 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:00 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:12 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:23:55 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:00 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 882.2472057342529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:29:42 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:43 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 21:35:59 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:42 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 21:41:46 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 887.256973028183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:31 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 21:47:28 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 21:50:41 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:22 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:26 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 878.4548182487488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:59:08 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:06 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:19 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:10 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:37 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 951.1138396263123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:00 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 22:18:34 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:49 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:37 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 22:27:39 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 922.7599396705627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:30:22 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:20 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 22:36:35 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:39:17 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 22:42:21 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 881.3971290588379 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:45:04 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 22:48:02 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 22:51:18 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:54:01 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:04 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 900.4014976024628 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:09 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:28 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:44 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:26 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:29 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 911.9532763957977 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:15:15 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 23:18:13 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 23:21:26 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:24:09 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 23:27:12 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 898.2653012275696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:30:13 2024]  Iteration number: 0 with current cost as 0.22597063888133323 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.05251573  0.56163898 -2.55903759
  3.07143982  2.34692641  1.09100203 -1.12034253  0.65480099  1.09523017
  1.28105143 -1.83545015  0.96413285]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:22 2024]  Iteration number: 0 with current cost as 0.1812913649244408 and parameters 
[-2.90318345  2.23743463 -2.12427964  0.03872364  0.55644432 -2.58350111
  3.08192839  2.35161316  1.07025113 -1.11045813  0.62209319  1.0970024
  1.27090489 -1.84481885  0.96462542]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:37 2024]  Iteration number: 0 with current cost as 0.19541986815088097 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03252136  0.56313732 -2.58016822
  3.07365487  2.33882509  1.09179708 -1.11837648  0.64074894  1.09322327
  1.276766   -1.84260746  0.9528836 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:39:20 2024]  Iteration number: 0 with current cost as 0.1979994800488246 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03973177  0.55783228 -2.58011227
  3.0779539   2.349577    1.07826255 -1.11487397  0.63485186  1.09563114
  1.27272126 -1.84427323  0.96048649]. 
Working on 1.0 fold... 
[Sun Mar 24 23:42:23 2024]  Iteration number: 0 with current cost as 0.1951350616102997 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.03939294  0.55892874 -2.57876999
  3.07608145  2.3475148   1.08266498 -1.11213026  0.63632194  1.09942134
  1.27358493 -1.84225975  0.96479786]. 
Training complete taking 892.1036052703857 seconds. 
Discarding model... 

Training complete taking 22496.54065680504 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.8752856254577637 seconds. 
Saved predicted values as M-A1-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (41.83843979243316,), 'R2_train': 0.7973068008520587, 'MAE_train': 5.00997475043641, 'MSE_test': 60.15996538025604, 'R2_test': 0.6373669238533528, 'MAE_test': 5.271974889571611}. 
Saved model results as M-A1-CNOT_HWE-CNOT_results.json. 
