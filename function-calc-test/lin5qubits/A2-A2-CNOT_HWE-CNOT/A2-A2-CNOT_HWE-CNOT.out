/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:12 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:45 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:08 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 17:41:58 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 17:45:27 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1126.680820941925 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:00 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:32 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:45 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:13 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.4404709339142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:44 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:15 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:40 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:30 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:59 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1138.375005722046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:42 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:16 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:48 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:23 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1154.5572423934937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:58 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:38 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:28 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:17 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:45 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1161.2340440750122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:18 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:08:58 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:31 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:17:38 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:06 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1161.8902337551117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:40 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:13 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:09 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1154.430151462555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:29 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:52:04 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:20 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1138.956886291504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:06:25 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:10:53 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:43 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:18:09 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.6692683696747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:40 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:25:12 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:33 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:19 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:44 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1118.9774935245514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:18 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:05 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:48:26 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:19 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:46 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1153.8375074863434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:34 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:03:07 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:26 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:56 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1136.9001369476318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:29 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:22:01 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:26:24 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:32 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1174.3513491153717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:04 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:37 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:45:59 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:51 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:17 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.111295223236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:48 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:00:19 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:40 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:27 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:55 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1119.7025475502014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:28 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:19:04 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:26 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1122.6258940696716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:12 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:44 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:04 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:45:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:21 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1122.4642915725708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:52:53 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:25 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:48 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:04:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:08:00 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1118.9202892780304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:32 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:04 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:19:27 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:23:17 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:26:43 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1121.7877569198608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:30:15 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:46 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:38:07 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:55 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:22 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1120.2209069728851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:26 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:56:48 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:00:36 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:01 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1119.313508272171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:07:33 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:11:06 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:15:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:19:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:23:03 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1168.476798772812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:02 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:30:34 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:55 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:43 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:10 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1120.8814606666565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:45:42 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:49:17 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:53:42 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:32 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:59 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1127.544951915741 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:04:30 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 01:08:02 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:25 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 01:16:14 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 01:19:40 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1121.7545483112335 seconds. 
Discarding model... 

Training complete taking 28380.106197834015 total seconds. 
Now scoring model... 
Scoring complete taking 0.9165096282958984 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.651447002735345,), 'R2_train': 0.8660380195375295, 'MAE_train': 4.422136207468062, 'MSE_test': 36.07814606615533, 'R2_test': 0.782527649293965, 'MAE_test': 4.853306916793701}. 
Saved model results as A2-A2-CNOT_HWE-CNOT_results.json. 
