/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:12 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:45 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:08 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 17:41:58 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 17:45:27 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1126.680820941925 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:00 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:32 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:45 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:13 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.4404709339142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:44 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:15 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:40 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:30 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:59 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1138.375005722046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:42 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:16 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:48 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:23 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1154.5572423934937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:58 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:38 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:28 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:17 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:45 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1161.2340440750122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:18 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:08:58 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:31 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:17:38 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:06 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1161.8902337551117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:40 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:13 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:09 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1154.430151462555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:29 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:52:04 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:20 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1138.956886291504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:06:25 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:10:53 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:43 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:18:09 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.6692683696747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:40 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:25:12 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:33 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:19 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:44 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1118.9774935245514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:18 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:05 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:48:26 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:19 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:46 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1153.8375074863434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:34 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:03:07 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:26 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:56 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1136.9001369476318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:29 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:22:01 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:26:24 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:32 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1174.3513491153717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:04 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:37 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:45:59 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:51 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:17 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1125.111295223236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:48 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:00:19 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:40 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:27 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:55 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1119.7025475502014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:28 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:19:04 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:26 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1122.6258940696716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:12 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:44 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:04 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 22:45:54 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 22:49:21 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1122.4642915725708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:52:53 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:25 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:48 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:04:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:08:00 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1118.9202892780304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:32 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:04 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:19:27 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:23:17 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:26:43 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1121.7877569198608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:30:15 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:46 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:38:07 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:55 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:22 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1120.2209069728851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:26 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:56:48 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:00:36 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:01 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1119.313508272171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:07:33 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:11:06 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:15:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:19:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:23:03 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1168.476798772812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:02 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:30:34 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:55 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:43 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:10 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1120.8814606666565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:45:42 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 00:49:17 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:53:42 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:32 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:59 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1127.544951915741 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:04:30 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Mon Mar 25 01:08:02 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:25 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Mon Mar 25 01:16:14 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Mon Mar 25 01:19:40 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1121.7545483112335 seconds. 
Discarding model... 

Training complete taking 28380.106197834015 total seconds. 
Now scoring model... 
Scoring complete taking 0.9165096282958984 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.651447002735345,), 'R2_train': 0.8660380195375295, 'MAE_train': 4.422136207468062, 'MSE_test': 36.07814606615533, 'R2_test': 0.782527649293965, 'MAE_test': 4.853306916793701}. 
Saved model results as A2-A2-CNOT_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:02 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 11:32:38 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:37:15 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 11:41:05 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 11:44:36 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1147.9713439941406 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:48:09 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:44 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:56:12 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 12:00:10 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 12:04:01 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1184.8682544231415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:07:57 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 12:11:47 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:16:52 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:45 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 12:24:17 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1199.0324456691742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:55 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 12:31:33 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:03 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:00 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:28 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1167.8793983459473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:23 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:19 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:32 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 13:00:25 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 13:03:54 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1203.1968686580658 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:26 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 13:11:03 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:43 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:36 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:23 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1184.249350309372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:27:09 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:50 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:35:18 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 13:39:09 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:42 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1157.1909756660461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:46:27 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 13:50:02 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:54:29 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 13:58:49 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:20 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1167.8059475421906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 14:09:50 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:14:16 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 14:18:47 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 14:22:15 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1196.2009370326996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:50 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 14:29:24 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:08 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 14:38:03 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 14:41:30 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1152.019654750824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:03 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 14:48:38 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:53:12 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:10 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:52 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1164.8825197219849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:04:27 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 15:08:05 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:12:34 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:24 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 15:19:53 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1140.889232635498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:23:28 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 15:27:25 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:57 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 15:35:51 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 15:39:21 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1166.3621718883514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:42:54 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 15:46:28 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:50:55 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 15:55:05 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 15:58:32 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1151.7445776462555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:02:08 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 16:05:42 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:10:13 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 16:14:03 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 16:17:36 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1143.517127752304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:21:11 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 16:24:46 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:29:11 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 16:33:03 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 16:36:32 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1136.2128791809082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:40:07 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 16:43:41 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:48:04 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 16:51:56 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 16:55:25 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1133.8869333267212 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:58:59 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 17:02:35 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:00 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:58 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 17:14:26 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1140.9276411533356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:18:00 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 17:21:35 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:02 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 17:29:55 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 17:33:25 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1137.8404803276062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:36:58 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 17:40:44 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:45:06 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 17:48:58 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 17:52:34 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1160.170134305954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:56:20 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:14 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:04:44 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 18:08:49 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 18:12:18 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1171.9475445747375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:15:52 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 18:19:27 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:23:57 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 18:27:53 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 18:31:27 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1151.6151225566864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:35:02 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 18:38:47 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:43:12 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 18:47:03 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 18:50:37 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1164.3711211681366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:54:26 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 18:58:02 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:02:49 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 19:07:34 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 19:11:11 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1219.8516008853912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:14:48 2024]  Iteration number: 0 with current cost as 0.30618100024585426 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.14648172  0.50984717 -2.8752408
  3.1126154   2.06589617  1.18006361 -1.1768278   0.61422036  1.01397557
  1.27861613 -1.92650567  0.64632658]. 
Working on 0.4 fold... 
[Thu Apr  4 19:18:21 2024]  Iteration number: 0 with current cost as 0.2094357119591878 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.13841014  0.32530835 -3.15871619
  3.5123362   1.41342422  0.86254566 -1.50736516  0.65057353  0.62410647
  1.09704761 -2.23253937  0.15973105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:22:58 2024]  Iteration number: 0 with current cost as 0.2006112346823123 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18701207  0.33067875 -3.2073364
  3.39500684  1.56307308  0.97959106 -1.54798427  0.67047314  0.5810212
  1.12897285 -2.16740675  0.28582378]. 
Working on 0.8 fold... 
[Thu Apr  4 19:26:50 2024]  Iteration number: 0 with current cost as 0.20743036939086953 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12605484  0.31220194 -3.16499604
  3.44441417  1.52678987  0.91507848 -1.55869876  0.6776197   0.57024943
  1.09504221 -2.20461491  0.26642554]. 
Working on 1.0 fold... 
[Thu Apr  4 19:30:23 2024]  Iteration number: 0 with current cost as 0.2927693537159176 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.13819366  0.51074257 -2.86407425
  3.12995048  2.05697007  1.15526216 -1.16118591  0.61229524  1.03236116
  1.27533704 -1.93262604  0.63541417]. 
Training complete taking 1152.5440702438354 seconds. 
Discarding model... 

Training complete taking 29097.179840564728 total seconds. 
Now scoring model... 
Scoring complete taking 0.8538000583648682 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.651447002735345,), 'R2_train': 0.8660380195375295, 'MAE_train': 4.422136207468062, 'MSE_test': 36.07814606615533, 'R2_test': 0.782527649293965, 'MAE_test': 4.853306916793701}. 
Saved model results as A2-A2-CNOT_HWE-CNOT_results.json. 
