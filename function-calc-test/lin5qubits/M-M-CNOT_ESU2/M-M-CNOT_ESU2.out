/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:56:50 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:13 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:03 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:07 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:26 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:28 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 866.1654200553894 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:32 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:25 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:24 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:39 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:42 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 851.6557307243347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:43 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:31 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:32 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:49 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:47 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 843.1029813289642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:46 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:34 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:33 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:49 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 18:51:56 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 854.9512016773224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:06 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:03 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:11 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:34 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:47 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 892.6039931774139 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:57 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:59 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:06 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:25 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:33 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 883.4569787979126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:41 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:38 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:47 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:14 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:33 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 901.7009930610657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:35 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:45:46 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:18 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:28 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 894.6350247859955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:54:37 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:40 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:00:47 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:07 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 20:06:19 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 895.3389286994934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:09:35 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 20:13:40 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:15:50 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:16 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:31 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 918.5800008773804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:59 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:02 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:12 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:38 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:53 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 914.6149153709412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:08 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:09 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:46:12 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:36 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 20:51:53 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 900.317764043808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:55:05 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 20:59:00 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:15 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:03:40 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:51 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 895.9729309082031 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:03 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:04 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:17 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:18:39 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:48 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 896.3329086303711 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:05 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:15 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:25 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:53 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:05 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 925.1603245735168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:24 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:24 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:41 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:03 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 21:52:22 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 905.707925081253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:55:31 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:33 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:01:39 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:03:58 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 22:07:16 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 893.2850027084351 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:10:21 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:24 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:16:34 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:18:56 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 22:22:05 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 891.0139710903168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:17 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:14 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:31:25 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:33:48 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 22:36:57 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 892.2867770195007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:05 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 22:44:08 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:46:15 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:34 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 22:51:44 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 888.9191629886627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:57 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 22:58:53 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:56 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:03:17 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 23:06:21 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 872.6580190658569 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:09:28 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 23:13:26 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:15:30 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:17:50 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:57 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 877.4415338039398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:24:04 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 23:28:02 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:30:13 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:32:35 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 23:35:45 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 889.7398705482483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:54 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 23:42:59 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:45:02 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:26 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Sun Mar 24 23:50:36 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 888.1548676490784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:53:46 2024]  Iteration number: 0 with current cost as 0.4316002603868079 and parameters 
[11.23095105  2.23743337 -2.12427964 -0.11653103  0.55388581 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648435]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:42 2024]  Iteration number: 0 with current cost as 0.2083101878850545 and parameters 
[ 1.14482891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:59:51 2024]  Iteration number: 0 with current cost as 0.21565691284601785 and parameters 
[ 1.36873352  2.23743432 -2.12427995 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:11 2024]  Iteration number: 0 with current cost as 0.42971802263024067 and parameters 
[-0.11549455  2.23743426 -2.12427964 -0.11653103  0.55388689 -2.77010935
  3.06858479  2.18960145  1.18551998 -1.06648346]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:20 2024]  Iteration number: 0 with current cost as 0.45843826430371304 and parameters 
[-0.08905163  2.23743444 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552018 -1.06648308]. 
Training complete taking 884.717209815979 seconds. 
Discarding model... 

Training complete taking 22218.515348672867 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.7990543842315674 seconds. 
Saved predicted values as M-M-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (230.58975310169777,), 'R2_train': -0.11712996418595378, 'MAE_train': 13.57130779527258, 'MSE_test': 188.5963532374056, 'R2_test': -0.13682372142729782, 'MAE_test': 11.862091226822262}. 
Saved model results as M-M-CNOT_ESU2_results.json. 
