/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:28 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:16 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:31 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 17:56:58 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:50 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1898.082347869873 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:07 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:45 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:58 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:26 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1885.944445848465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:32 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:17 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:30 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:56 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:45 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1888.2352349758148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:58 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:25:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:13 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1882.7580692768097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:59 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:15 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:48 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:37 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1890.1325635910034 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:51 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:24:28 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:28:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:09 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:08 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1896.0869925022125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:30 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:14 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:28 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:10:49 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1894.2559685707092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:37:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:42:29 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1897.5198383331299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:40 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:25 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:00 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:51 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1890.538812160492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:21:12 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:31:19 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:35:38 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:07 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:58 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1931.9252212047577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:03 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:18 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:46 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:42 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1900.5977125167847 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:03 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:34:43 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:38:58 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:44:28 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1886.6890840530396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:56:29 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:06:08 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:28 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:16:02 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:59 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1916.6910517215729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:30 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:31 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:01 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:48:37 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:53:35 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1945.1488955020905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:00:50 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:10:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:15:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:33 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:25:33 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1921.737848997116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:58 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:42:55 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:47:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:52:59 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:58:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1961.5493218898773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:15:41 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:20:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:37 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:37 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1944.2216539382935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:38:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:48:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:52:49 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:58:23 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:03:24 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1979.0002720355988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:11:01 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:21:04 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:29 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 03:31:03 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:36:06 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1950.3809158802032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:43:36 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:53:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:58:12 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:03:59 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:09:00 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1980.416908979416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:16:33 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:30:55 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:36:35 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:41:43 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1960.0759885311127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:49:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:59:23 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:03:50 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:09:29 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:14:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 1985.7982342243195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:22:19 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:32:30 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:36:59 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:42:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:48:04 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2007.3494336605072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:55:54 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:06:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:10:55 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:21:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2013.2108821868896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:26 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:39:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:44:20 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:50:05 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:55:18 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2016.2077040672302 seconds. 
Discarding model... 

Training complete taking 48324.55598902702 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.0676088333129883 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRX_results.json. 
