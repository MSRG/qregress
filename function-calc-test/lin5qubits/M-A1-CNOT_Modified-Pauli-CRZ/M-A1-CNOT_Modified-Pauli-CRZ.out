/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:32:53 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:34:44 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:35:27 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.82936668395996 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:36:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:53 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:41:28 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.6556942462921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:49 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:49 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:55 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:46 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:30 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.7910735607147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:50 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:56 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:52:48 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:39 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.3097608089447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:01 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:00 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:08 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:59 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:44 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.04971194267273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:13 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:12 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:19 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:13 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:57 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.83530354499817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:18 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:17 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:23 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:15 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:02 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.7286138534546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:35 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:32 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:31 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:16 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.64192819595337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:38 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:20:37 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:45 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:39 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:23 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.4926953315735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:44 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:26:43 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:49 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:41 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:25 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.58758521080017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:52 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:51 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:01 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:37 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 370.6273374557495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:57 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:56 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:02 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:53 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:39 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.98055839538574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:59 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:59 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:06 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:57 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:44 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 364.5590469837189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:03 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:09 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:15 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:12 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:56 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 384.9750108718872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:31 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:30 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:38 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:27 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:13 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 364.0933666229248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:33 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:34 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:06:54 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:42 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 395.16211557388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:09 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:10 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:16 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:13:12 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:58 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.8115735054016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:19 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:20 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:26 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:17 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:00 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.7535970211029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:22 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:22 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:29 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:20 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:04 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.2727105617523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:25 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:24 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:31 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:25 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:13 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.63901376724243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:36 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:34 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:41 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:29 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:14 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.4437048435211 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:39:33 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:31 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:37 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:28 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:12 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.49071288108826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:48 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:47 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:52 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:43 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:50:26 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 373.67755341529846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:47 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:45 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:42 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:25 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.3538203239441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:46 2024]  Iteration number: 0 with current cost as 0.46074512897749886 and parameters 
[-1.08295672  2.23743464 -2.12427964 -0.11653053  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:45 2024]  Iteration number: 0 with current cost as 0.37623898671970385 and parameters 
[-1.15776279  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010897
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029899 -1.87354655  0.72965055  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.5001342552128882 and parameters 
[-0.07997905  2.23743521 -2.12427935 -0.11653103  0.55388708 -2.77010897
  3.06858527  2.18960174  1.18552027 -1.0664828   0.60271539  1.14432474
  1.31029927 -1.87354623  0.72965052  2.88578419 -0.54534306 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:01:51 2024]  Iteration number: 0 with current cost as 0.3985886622875897 and parameters 
[-1.12518698  2.23743464 -2.12427928 -0.11653067  0.55388708 -2.77010897
  3.06858516  2.18960181  1.18552016 -1.06648291  0.60271546  1.1443248
  1.31029934 -1.87354645  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:36 2024]  Iteration number: 0 with current cost as 0.40254991577958255 and parameters 
[-1.11690592  2.23743479 -2.12427948 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 370.6228094100952 seconds. 
Discarding model... 

Training complete taking 9190.385673999786 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.8425660133361816 seconds. 
Saved predicted values as M-A1-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.35371916474514,), 'R2_train': -0.24194773296428518, 'MAE_train': 14.396656389443962, 'MSE_test': 224.0725410873362, 'R2_test': -0.3506675800243102, 'MAE_test': 12.795027007942936}. 
Saved model results as M-A1-CNOT_Modified-Pauli-CRZ_results.json. 
