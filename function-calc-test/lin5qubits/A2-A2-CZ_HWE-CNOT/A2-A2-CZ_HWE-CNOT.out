/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:01 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:57 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:02 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:13 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:04 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1215.6089053153992 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:17 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:14 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:18 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:30 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:20 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1213.2928054332733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:30 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:22 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:25 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:34 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:33 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1217.4602708816528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:48 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:44 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:48 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:00 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:53 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1227.306547164917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:16 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:13 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:18 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:29 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:21 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1228.8410325050354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:44 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:35 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:39 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 19:23:52 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:41 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1207.0554893016815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:52 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:41 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:52 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:01 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:52 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1224.4349427223206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:52:16 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:08 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 20:00:10 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:33 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:24 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1228.2914497852325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:47 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:04 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:07 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:20 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:09 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1235.1211128234863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:22 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:31 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:39 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 20:45:49 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:38 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1245.1025085449219 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:54:05 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:55 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:57 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:07 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:57 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1208.3036677837372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:11 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:05 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:09 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:18 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:30:12 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1211.7928795814514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:34:25 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:28 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:43 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:54 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:41 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1232.123931646347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:57 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:53 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 22:03:17 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 22:07:30 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:18 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1231.3763904571533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:28 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 22:19:17 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:22 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:33 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:21 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1237.137589931488 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:05 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:52 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 22:43:59 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:18 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:09 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1236.2501475811005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:56:42 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:31 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:33 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 23:08:41 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:34 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1202.1062099933624 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:43 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 23:20:56 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 23:25:09 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:17 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:06 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1246.2096962928772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:37:30 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Sun Mar 24 23:41:22 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Sun Mar 24 23:45:44 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Sun Mar 24 23:50:10 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:54:06 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1245.5593764781952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:58:15 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 00:02:08 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 00:06:19 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 00:10:30 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:19 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1233.1935646533966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:03 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 00:22:59 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 00:27:32 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:40 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:35:30 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1253.0201613903046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:39:55 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 00:43:43 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 00:48:10 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 00:52:31 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:56:35 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1263.181496143341 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:00:45 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 01:04:33 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 01:08:35 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 01:12:45 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:16:56 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1245.6463243961334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:31 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 01:26:07 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:17 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 01:34:25 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:38:14 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1254.356927871704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:42:25 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:19 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Mon Mar 25 01:50:22 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Mon Mar 25 01:54:31 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:58:21 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1206.8229837417603 seconds. 
Discarding model... 

Training complete taking 30749.598655223846 total seconds. 
Now scoring model... 
Scoring complete taking 0.9557716846466064 seconds. 
Saved predicted values as A2-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (64.1785717075567,), 'R2_train': 0.689076359427176, 'MAE_train': 5.924845653404416, 'MSE_test': 72.97719750134138, 'R2_test': 0.56010703378566, 'MAE_test': 5.657818039851248}. 
Saved model results as A2-A2-CZ_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:51 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 11:32:44 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 11:36:51 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 11:41:04 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:44:55 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1213.939114332199 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:49:05 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 11:52:57 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 11:57:08 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 12:01:56 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:53 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1262.9980454444885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:10:08 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:59 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:05 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 12:22:13 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:26:02 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1203.4337403774261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:11 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:01 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:03 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 12:42:13 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:46:04 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1204.6023650169373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:15 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 12:54:04 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 12:58:11 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:22 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:06:17 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1217.8157393932343 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:34 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:24 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 13:18:30 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:02 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:26:53 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1232.34499001503 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:31:06 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:07 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:36 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 13:43:45 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:44 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1270.7957997322083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:52:18 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:20 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:23 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:35 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:08:27 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1249.3860049247742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:07 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 14:17:11 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:22 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:44 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:29:36 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1239.2531714439392 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:46 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 14:37:38 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 14:41:44 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 14:45:59 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:49:55 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1230.3402872085571 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:16 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 14:58:09 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:14 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 15:06:25 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:10:28 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1224.273033618927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:14:41 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 15:18:40 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 15:22:48 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 15:27:24 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:31:15 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1260.0637230873108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:35:41 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 15:39:31 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:35 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 15:47:44 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:51:32 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1207.5390207767487 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:56:03 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 16:00:31 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 16:04:38 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 16:08:59 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:50 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1276.0978028774261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:17:20 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 16:21:11 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 16:25:21 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 16:29:34 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:33:58 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1266.1913480758667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:38:10 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 16:41:58 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 16:46:24 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 16:50:35 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:54:27 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1229.9097826480865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:58:41 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 17:02:30 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 17:06:32 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:47 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:14:37 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1211.4635601043701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:18:52 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 17:22:47 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:51 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 17:30:59 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:34:50 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1208.934199810028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:39:00 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 17:42:53 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 17:46:58 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 17:51:09 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:54:59 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1209.5287611484528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:59:09 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 18:02:59 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 18:07:04 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 18:11:17 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:15:53 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1274.6835808753967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:20:25 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 18:24:25 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 18:28:47 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 18:33:00 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:52 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1247.2574565410614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:41:11 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 18:45:03 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 18:49:06 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 18:53:28 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:57:20 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1217.8962762355804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:01:30 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 19:05:20 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 19:09:23 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 19:13:34 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:17:25 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1207.1805748939514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:21:37 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 19:25:29 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 19:29:37 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 19:34:09 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:38:20 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1283.7520830631256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:43:01 2024]  Iteration number: 0 with current cost as 0.21405389774195638 and parameters 
[-2.90318347  2.23743462 -2.12427964 -0.18554374  0.83854752 -2.3993393
  3.15242238  2.70241534  0.74894146 -0.36703811  1.2765593   2.20405428
  1.44255983 -1.8868139   0.23854534]. 
Working on 0.4 fold... 
[Thu Apr  4 19:47:37 2024]  Iteration number: 0 with current cost as 0.18827673728327643 and parameters 
[-2.90318349  2.23743462 -2.12427964 -0.17387029  0.81520085 -2.42267778
  3.16988743  2.74433059  0.69496182 -0.58576451  1.22821738  1.92343057
  1.58122245 -1.7021328   0.43461288]. 
Working on 0.6 fold... 
[Thu Apr  4 19:51:42 2024]  Iteration number: 0 with current cost as 0.18615094106252258 and parameters 
[-2.90318345  2.2374346  -2.12427965 -0.17225448  0.81788548 -2.41651597
  3.1672023   2.71990323  0.7135022  -0.4785967   1.17252673  2.03610815
  1.51845787 -1.78555682  0.34647469]. 
Working on 0.8 fold... 
[Thu Apr  4 19:56:13 2024]  Iteration number: 0 with current cost as 0.16745914554987062 and parameters 
[-2.90318345  2.23743462 -2.12427964 -0.22205241  0.82500329 -2.46376831
  3.17929644  2.6769649   0.71710584 -0.47936783  1.20953358  2.04674254
  1.48197854 -1.80105757  0.4133364 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:00:03 2024]  Iteration number: 0 with current cost as 0.19150438963974817 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.15006132  0.81427258 -2.39615696
  3.16866776  2.72332737  0.70902581 -0.52221022  1.21699541  1.9970697
  1.51852651 -1.78711489  0.34066661]. 
Training complete taking 1277.480483531952 seconds. 
Discarding model... 

Training complete taking 30927.162835597992 total seconds. 
Now scoring model... 
Scoring complete taking 1.0243194103240967 seconds. 
Saved predicted values as A2-A2-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (64.1785717075567,), 'R2_train': 0.689076359427176, 'MAE_train': 5.924845653404416, 'MSE_test': 72.97719750134138, 'R2_test': 0.56010703378566, 'MAE_test': 5.657818039851248}. 
Saved model results as A2-A2-CZ_HWE-CNOT_results.json. 
