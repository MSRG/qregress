/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:16 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:20 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:56 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:01 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:07 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.867847442627 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:13 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:48 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:59 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 951.0813715457916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:05 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:16 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:44 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:58 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.3387682437897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:02 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:24:07 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:39 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:30:44 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:46 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 949.0750617980957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:17 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:19 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:21 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 936.3725271224976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:39 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:05 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:09 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:05:11 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 947.9692182540894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:13 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:19 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:41 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:17:43 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:52 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 939.6157720088959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:54 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:58 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:27 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:29 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:51 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 958.629693031311 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:39:53 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:55 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:21 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:27 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:28 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 937.499832868576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:30 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:34 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:00 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:07 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:13 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 945.7164642810822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:15 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:46 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:48 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:55 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 947.2758507728577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:03 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:57 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.0765478610992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:01 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:04 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:49:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:38 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:56:01 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 964.6557650566101 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:05 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:09 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:05:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:08:36 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:40 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 938.5914702415466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:44 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:51 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:33 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:35 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:37 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 955.2793440818787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:39 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:42 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:07 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:09 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:43:10 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 933.1115040779114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:12 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:14 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:40 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:42 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:44 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 935.0134971141815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:47 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:50 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:16 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:19 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:21 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 937.2274088859558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:23 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:20:24 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:50 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:26:52 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:55 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 943.6994829177856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:08 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:15 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:40 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:42 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:44 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 941.0236067771912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:49 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:55:28 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:34 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:01:49 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 961.6887121200562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:51 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:11:38 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:46 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:48 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 959.1316909790039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:53 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:21 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:30:24 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:49 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 959.7449340820312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:36:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:57 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:26 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:28 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:29 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 942.244068145752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:31 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:35 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:59:00 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:03 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:07 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 940.0379917621613 seconds. 
Discarding model... 

Training complete taking 23696.969702243805 total seconds. 
Now scoring model... 
Scoring complete taking 1.003828763961792 seconds. 
Saved predicted values as IQP_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.355242500405745,), 'R2_train': 0.988589640542409, 'MAE_train': 1.1430239729859633, 'MSE_test': 1.788691417882077, 'R2_test': 0.9892181009905208, 'MAE_test': 1.1600935528937666}. 
Saved model results as IQP_Full-Pauli-CRX_results.json. 
