/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:56:50 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:02 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:05 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:01 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:32 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1656.0055372714996 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:57 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:39 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:46 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:33 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:03 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1645.1677997112274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:00 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:03:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:54 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:26 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1670.922236442566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:13 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:50 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:59 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:04 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:38 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1672.8482103347778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:06 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:41 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:38 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:28 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:08 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1643.215057849884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:29 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:26:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:31:51 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:17 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1623.7770097255707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:33 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:03 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:14 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:04:41 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1645.6704127788544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:59 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:35 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:33 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:20 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:43 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1620.622328042984 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:59 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:38 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:47:40 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:29 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:57 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1633.164823770523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:12 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:48 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:45 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:20:32 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:26:01 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1625.2819592952728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:17 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:55 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:41:53 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:40 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:53:06 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1622.4361248016357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:20 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:00 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:08:58 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:46 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:13 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1629.775845527649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:30 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:30:06 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:02 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:47 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:14 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1621.2111387252808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:31 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:08 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:06 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:52 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:22 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1628.3709259033203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:39 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:24:15 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:11 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:36:01 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:41:30 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1628.1339452266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:46:47 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:51:28 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:29 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:03:17 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:08:44 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1633.367655992508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:02 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:18:38 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:36 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:24 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:51 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1640.6694412231445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:22 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:45:58 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:56 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:45 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:03:16 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1642.9276044368744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:45 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:21 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:19 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:25:08 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:33 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1624.1512372493744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:35:49 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:40:25 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:46:22 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:52:11 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:57:40 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.2440996170044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:00 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:59 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:07 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:19:53 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:25:19 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1660.0869250297546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:30:36 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:35:13 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:12 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:59 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:52:27 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1631.993748664856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:57:48 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:02:22 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:08:19 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:14:10 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:19:37 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1626.9824573993683 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:24:55 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:29:31 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:29 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:41:16 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:46:40 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1621.6662969589233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:51:57 2024]  Iteration number: 0 with current cost as 0.3946272296631361 and parameters 
[-2.82278494  2.0256901  -2.40223636 -0.11653103  0.55388708 -2.77010901
  3.06858495  2.18960141  1.18551995 -1.06648312  1.67589967  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:56:32 2024]  Iteration number: 0 with current cost as 0.3329604614022771 and parameters 
[-2.99138249  1.96502368 -2.3306498  -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648316  2.00049886  1.14432445
  1.31029899 -1.87354673  0.72965073  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:02:30 2024]  Iteration number: 0 with current cost as 0.3345007694083931 and parameters 
[-2.95591232  1.98219276 -2.37226945 -0.11653103  0.55388712 -2.77010897
  3.06858502  2.18960149  1.18552002 -1.06648312  1.91767049  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578416 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:08:20 2024]  Iteration number: 0 with current cost as 0.3321590371089672 and parameters 
[-2.95929422  1.989361   -2.3831704  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648312  1.9106503   1.14432445
  1.31029906 -1.87354673  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:13:46 2024]  Iteration number: 0 with current cost as 0.34649450653514635 and parameters 
[-2.94362125  1.98017033 -2.35602868 -0.11653095  0.55388708 -2.77010894
  3.06858502  2.18960149  1.18552006 -1.06648308  1.90922735  1.14432449
  1.31029906 -1.87354673  0.7296508   2.88578419 -0.5453432  -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1627.4993879795074 seconds. 
Discarding model... 

Training complete taking 40903.19436621666 total seconds. 
Now scoring model... 
Scoring complete taking 1.108515977859497 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (111.40195566903135,), 'R2_train': 0.4602949130532119, 'MAE_train': 9.491056955666982, 'MSE_test': 128.96323767794826, 'R2_test': 0.22263360204103688, 'MAE_test': 9.769200931316783}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRX_results.json. 
