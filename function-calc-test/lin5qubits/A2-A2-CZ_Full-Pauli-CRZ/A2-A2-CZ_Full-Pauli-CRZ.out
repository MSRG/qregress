/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:31 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:30 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:59 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:30 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:48 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1627.7457308769226 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:39 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:37 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:12 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:43 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:04 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.5204215049744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:40 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:38 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:33 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:06 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:28 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1637.490881204605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:59 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:16 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:48 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:29 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:49 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1657.3829128742218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:19:35 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:53 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:32 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:07 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:37 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1653.0556654930115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:08 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:08 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:40 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:11 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:41 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1622.6872556209564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:02 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:31 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:23 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1600.4996523857117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:50 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:57 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:00 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:34 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:50 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1648.6310622692108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:08:19 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:15 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:30:42 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1671.8089570999146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:12 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:25 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:53 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:52:23 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:50 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1643.438021659851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:34 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:21 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:51 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:20:22 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1658.8585393428802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:14 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:08 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:02 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:59 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:53:27 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1663.0942380428314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:57 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:52 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:22 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:53 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:12 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1609.0585358142853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:47 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:30:48 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:19 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:49 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:18 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.122226715088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:47 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:38 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:11 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:42 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:16 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1619.883229970932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:46 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:24:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:22 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:35:51 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:41:06 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1636.6043152809143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:47:02 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:52:17 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:03:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:08:32 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1625.7432794570923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:08 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:05 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:45 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:46 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:36:07 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1648.214281320572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:35 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:46 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:52:25 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:55 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:03:11 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1631.6768872737885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:48 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:47 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:17 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:24:44 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:02 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1601.4606580734253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:35:31 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:40:31 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:45:59 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:51:28 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:46 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1614.8146967887878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:02:24 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:19 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:12:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:18:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:28 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1592.0169706344604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:28:56 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:33:49 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:39:17 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:45 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:50:03 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1603.649492263794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:55:41 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:00:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:06:09 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:11:38 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:54 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1615.5400257110596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:22:36 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:27:27 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:32:58 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:29 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:43:47 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1599.507904291153 seconds. 
Discarding model... 

Training complete taking 40723.50825834274 total seconds. 
Now scoring model... 
Scoring complete taking 1.1121869087219238 seconds. 
Saved predicted values as A2-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (163.36687292681077,), 'R2_train': 0.2085423291927031, 'MAE_train': 11.01907158920767, 'MSE_test': 161.7538667619928, 'R2_test': 0.024977791929262638, 'MAE_test': 10.776672522047338}. 
Saved model results as A2-A2-CZ_Full-Pauli-CRZ_results.json. 
