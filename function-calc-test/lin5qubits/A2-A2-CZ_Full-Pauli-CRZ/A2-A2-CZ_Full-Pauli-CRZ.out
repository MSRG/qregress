/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:31 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:30 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:59 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:30 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:48 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1627.7457308769226 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:39 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:37 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:12 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:43 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:04 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.5204215049744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:40 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:38 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:33 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:06 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:28 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1637.490881204605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:59 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:16 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:48 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:29 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:49 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1657.3829128742218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:19:35 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:53 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:32 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:07 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:37 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1653.0556654930115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:08 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:08 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:40 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:11 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:41 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1622.6872556209564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:02 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:31 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:23 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1600.4996523857117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:50 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:57 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:00 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:34 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:50 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1648.6310622692108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:08:19 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:15 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:30:42 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1671.8089570999146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:12 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:25 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:53 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:52:23 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:50 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1643.438021659851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:34 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:21 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:51 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:20:22 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1658.8585393428802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:14 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:08 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:02 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:59 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:53:27 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1663.0942380428314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:57 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:52 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:22 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:53 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:12 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1609.0585358142853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:47 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:30:48 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:19 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:49 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:18 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.122226715088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:47 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:38 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:11 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:42 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:16 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1619.883229970932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:46 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:24:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:22 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:35:51 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:41:06 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1636.6043152809143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:47:02 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:52:17 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:03:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:08:32 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1625.7432794570923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:08 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:05 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:45 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:46 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:36:07 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1648.214281320572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:35 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:46 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:52:25 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:55 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:03:11 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1631.6768872737885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:48 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:47 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:17 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:24:44 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:30:02 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1601.4606580734253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:35:31 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:40:31 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:45:59 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:51:28 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:46 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1614.8146967887878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:02:24 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:19 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:12:47 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:18:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:28 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1592.0169706344604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:28:56 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:33:49 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:39:17 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:45 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:50:03 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1603.649492263794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:55:41 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:00:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:06:09 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:11:38 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:54 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1615.5400257110596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:22:36 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:27:27 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:32:58 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:29 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:43:47 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1599.507904291153 seconds. 
Discarding model... 

Training complete taking 40723.50825834274 total seconds. 
Now scoring model... 
Scoring complete taking 1.1121869087219238 seconds. 
Saved predicted values as A2-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (163.36687292681077,), 'R2_train': 0.2085423291927031, 'MAE_train': 11.01907158920767, 'MSE_test': 161.7538667619928, 'R2_test': 0.024977791929262638, 'MAE_test': 10.776672522047338}. 
Saved model results as A2-A2-CZ_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:24 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 11:34:23 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:39:55 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:45:26 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 11:50:45 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1618.3959946632385 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:56:19 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:01:15 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:07:07 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:12:42 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:18:10 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1649.7701318264008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:47 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:34:11 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:49 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:08 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1613.6717586517334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:40 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:55:35 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:05 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:38 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:58 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1612.3537967205048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:17:33 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:22:29 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:28:08 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:41 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:39:26 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1647.7784447669983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:00 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:49:56 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:55:27 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:00:58 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:06:15 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1604.6777851581573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:11:46 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:16:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:22:09 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:27:37 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:33:17 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.7206103801727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:38:45 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:41 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:49:14 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:54:48 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:12 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1614.0485289096832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:05:39 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:10:38 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:16:26 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:54 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:27:18 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1631.7483875751495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:32:51 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:37:45 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:14 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:48:41 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:53:59 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1595.583333015442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:59:28 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:04:21 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:09:48 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:15:17 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:20:34 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1598.3842253684998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:26:05 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:31:20 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:36:48 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:42:16 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:47:33 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1613.9322214126587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:53:01 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:57:54 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:03:24 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:09:07 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:14:26 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1616.01704788208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:19:55 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:24:53 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:30:20 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:35:58 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:41:19 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.7479405403137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:46:56 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:51:59 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:57:27 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:03:18 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:08:36 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1629.0937404632568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:14:06 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:19:07 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:24:44 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:30:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:35:38 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1635.4688794612885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:41:21 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:46:13 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:51:43 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:57:14 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:02:30 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1599.1591165065765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:08:00 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:12:55 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:18:23 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:23:53 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:29:25 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1620.3639495372772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:35:00 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:39:53 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:45:23 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:50:52 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:56:11 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1601.0642676353455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:01:42 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:06:34 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:12:03 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:17:32 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:23:01 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1606.6496014595032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:28:27 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:33:23 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:39:04 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:44:34 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:49:59 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1625.9137389659882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:55:34 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:00:27 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:05:54 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:11:24 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:16:39 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1592.6460301876068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:22:06 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:27:00 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:32:29 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:37:59 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:43:29 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1623.331695318222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:49:10 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:24 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:59:56 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:05:26 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:10:43 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1626.1895582675934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:16:16 2024]  Iteration number: 0 with current cost as 0.3471024290414729 and parameters 
[-3.00403741  2.86782109 -2.04522357 -0.11653103  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551997 -1.06648308  1.1074843   1.14432445
  1.37447883 -1.87354682  0.72965078  2.88578417 -0.54534337 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:21:16 2024]  Iteration number: 0 with current cost as 0.35585717525065735 and parameters 
[-3.08149693  2.31286072 -2.11109594 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67002268  1.14432445
  1.50140815 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:26:57 2024]  Iteration number: 0 with current cost as 0.39705388119620844 and parameters 
[-3.00687863  2.32206722 -2.10356805 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.67494418  1.14432445
  1.42159469 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:32:36 2024]  Iteration number: 0 with current cost as 0.3921962655959692 and parameters 
[-3.00822304  2.31808224 -2.10360891 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66973108  1.14432445
  1.4219854  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:38:04 2024]  Iteration number: 0 with current cost as 0.39079955875103156 and parameters 
[-3.02288927  2.32221059 -2.10749849 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.67478926  1.14432445
  1.43783666 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Training complete taking 1648.1655714511871 seconds. 
Discarding model... 

Training complete taking 40465.87823486328 total seconds. 
Now scoring model... 
Scoring complete taking 1.0963506698608398 seconds. 
Saved predicted values as A2-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (163.36687292681077,), 'R2_train': 0.2085423291927031, 'MAE_train': 11.01907158920767, 'MSE_test': 161.7538667619928, 'R2_test': 0.024977791929262638, 'MAE_test': 10.776672522047338}. 
Saved model results as A2-A2-CZ_Full-Pauli-CRZ_results.json. 
