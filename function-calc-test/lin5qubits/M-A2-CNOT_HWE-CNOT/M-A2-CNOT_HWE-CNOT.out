/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:29:55 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:37 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 17:39:42 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:16 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 17:46:10 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:07 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:51 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 17:57:02 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1667.7750301361084 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:42 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:08 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 18:07:21 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:54 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 18:13:49 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:45 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:50 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 18:25:02 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1679.6422955989838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:42 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 18:35:11 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:42 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 18:41:37 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:36 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:49 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 18:53:00 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1677.8622453212738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:40 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:03 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:03:06 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:37 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 19:09:30 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:26 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 19:15:07 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 19:20:22 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1640.6362526416779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:00 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:28 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:30:34 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:04 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 19:36:55 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:51 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:31 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 19:47:40 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1638.340512752533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:19 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:43 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:57:50 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:19 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:04:15 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:11 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:00 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 20:15:13 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1655.7791893482208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:54 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:17 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 20:25:21 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 20:26:52 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:32:00 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:57 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:39 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 20:42:50 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1654.0464322566986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:30 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 20:52:57 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:27 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:59:19 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:15 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 21:04:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 21:10:08 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1639.008819103241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:48 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 21:15:22 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 21:20:27 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:03 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 21:26:57 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:27:53 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:34 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 21:37:42 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1653.1751918792725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:21 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:44 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 21:47:46 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:15 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 21:54:47 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:47 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:29 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 22:05:47 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1685.1246762275696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:26 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 22:10:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 22:16:17 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 22:17:48 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 22:22:43 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:23:41 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:21 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 22:33:31 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1664.8610525131226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:11 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:38 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 22:43:43 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 22:45:12 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 22:50:05 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:01 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:01:06 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1654.710768699646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:46 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:09 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 23:11:11 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 23:12:42 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 23:17:32 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:29 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 23:23:12 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:28:29 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1643.1927783489227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:09 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:34 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 23:38:40 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:10 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 23:45:07 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:12 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:10 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:56:46 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1696.2775056362152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:57:25 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:53 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 00:06:57 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 00:09:02 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 00:13:56 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:53 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 00:19:35 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 00:24:45 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1678.849779367447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:25:24 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:29:49 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 00:34:53 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 00:36:22 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 00:41:13 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:09 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:50 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 00:51:59 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1634.1849024295807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:38 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:04 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:02:08 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:38 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 01:08:31 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:27 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 01:14:14 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 01:19:23 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1644.793791770935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:20:03 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:25 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:29:35 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:06 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 01:35:59 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:59 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:56 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 01:47:16 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1671.5284123420715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:56 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:30 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:57:33 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:04 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:03:58 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:04:54 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 02:09:32 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 02:14:41 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1646.7757518291473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:15:21 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 02:19:45 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 02:24:52 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:21 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:31:15 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:11 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 02:36:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 02:42:10 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1647.8735620975494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:50 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 02:47:13 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 02:52:15 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 02:53:45 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:58:38 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:59:34 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:04:36 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 03:09:45 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1665.2469515800476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:10:34 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 03:15:00 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 03:20:03 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 03:21:33 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 03:26:26 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:23 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:32:04 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 03:37:12 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1636.895102739334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:37:52 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 03:42:17 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 03:47:22 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 03:48:51 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 03:53:45 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:42 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:59:24 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:04:37 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1645.7897918224335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:17 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:42 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 04:14:47 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:18 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 04:21:10 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:06 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 04:26:47 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:31:59 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1641.6764261722565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:32:38 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:04 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 04:42:09 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:39 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 04:48:33 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:49:29 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:13 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:59:23 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1642.9373161792755 seconds. 
Discarding model... 

Training complete taking 41406.98629570007 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9209723472595215 seconds. 
Saved predicted values as M-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (43.660027998340006,), 'R2_train': 0.7884818173484388, 'MAE_train': 5.200948661477983, 'MSE_test': 67.99259265483215, 'R2_test': 0.5901533042155023, 'MAE_test': 5.5516262832925465}. 
Saved model results as M-A2-CNOT_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:45 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 11:33:20 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 11:38:32 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:05 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 11:45:07 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:46:06 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 11:51:06 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 11:56:25 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1701.218537569046 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:57:05 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 12:01:36 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 12:06:51 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 12:08:24 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 12:13:28 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:26 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 12:19:20 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 12:24:40 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1695.5455663204193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:22 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 12:35:10 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:44 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 12:41:45 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:42:51 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:29 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 12:53:53 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1753.1241557598114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:33 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:06 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 13:04:21 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:53 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 13:10:53 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:51 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 13:16:51 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 13:22:10 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1696.4589507579803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:22:52 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:26 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 13:32:42 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 13:34:21 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 13:39:28 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:40:25 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 13:45:12 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 13:50:55 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1725.9361321926117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:51:36 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:08 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 14:01:26 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 14:02:58 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 14:08:00 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:58 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:47 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 14:19:04 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1688.6094241142273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:19:44 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 14:24:15 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 14:29:27 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 14:30:59 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 14:36:00 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:36:58 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 14:41:47 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 14:47:07 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1682.814248085022 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:47:47 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 14:52:20 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 14:57:33 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 14:59:05 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 15:04:06 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:05:03 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 15:09:52 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 15:15:11 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1683.7922263145447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:15:51 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 15:20:23 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 15:25:36 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 15:27:09 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 15:32:24 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:22 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 15:38:11 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 15:43:35 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1704.2138514518738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:44:15 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 15:49:08 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 15:54:19 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:52 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 16:00:54 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:01:52 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 16:06:43 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 16:12:20 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1724.7050867080688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:13:01 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 16:17:34 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 16:22:57 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 16:24:30 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 16:29:30 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:30:29 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 16:35:22 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 16:40:41 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1701.5245614051819 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:41:21 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 16:45:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 16:51:07 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:40 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 16:57:42 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:58:40 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 17:03:29 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 17:08:59 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1697.1951372623444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:09:45 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 17:14:21 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 17:19:31 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 17:21:03 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 17:26:04 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:27:02 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:52 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 17:37:11 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1692.1501483917236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:37:51 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 17:42:22 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 17:47:42 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 17:49:14 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 17:54:38 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:55:39 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 18:00:26 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 18:05:52 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1721.8185501098633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:06:32 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 18:11:04 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 18:16:49 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 18:18:21 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 18:23:28 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:24:26 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 18:29:19 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 18:34:47 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1735.0312983989716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:35:28 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 18:40:00 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 18:45:14 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 18:46:46 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 18:51:47 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:52:45 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 18:57:35 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 19:02:57 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1689.2480881214142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:03:37 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 19:08:10 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 19:13:25 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 19:14:58 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 19:20:00 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:20:58 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 19:25:50 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 19:31:08 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1691.3492922782898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:31:50 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 19:36:21 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 19:42:19 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 19:43:52 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 19:48:53 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:49:52 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 19:54:48 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 20:00:31 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1764.4462904930115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:01:13 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 20:05:48 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 20:11:00 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 20:12:33 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 20:17:33 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:18:31 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 20:23:20 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 20:28:36 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1684.0649540424347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:29:18 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 20:33:51 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 20:39:03 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 20:40:36 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 20:45:39 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:46:37 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 20:51:28 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 20:57:10 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1713.5300590991974 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:57:51 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 21:02:25 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 21:07:35 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 21:09:07 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 21:14:09 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:15:07 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 21:19:58 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 21:25:17 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1687.2015352249146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:25:59 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 21:30:32 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 21:35:45 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 21:37:17 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 21:42:18 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:43:17 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 21:48:11 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 21:53:37 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1700.430832862854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:54:18 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 21:59:06 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 22:04:19 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 22:06:15 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 22:11:18 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:12:15 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 22:17:46 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 22:23:07 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1769.7554547786713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:23:48 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 22:28:20 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 22:33:34 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 22:35:06 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 22:40:06 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:41:04 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 22:45:54 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 22:51:14 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1686.5693204402924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:51:54 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Thu Apr  4 22:56:48 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Thu Apr  4 23:02:02 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Thu Apr  4 23:03:35 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Thu Apr  4 23:08:39 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:09:36 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:25 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Thu Apr  4 23:19:52 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1717.452160358429 seconds. 
Discarding model... 

Training complete taking 42708.18778324127 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9847264289855957 seconds. 
Saved predicted values as M-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (43.660027998340006,), 'R2_train': 0.7884818173484388, 'MAE_train': 5.200948661477983, 'MSE_test': 67.99259265483215, 'R2_test': 0.5901533042155023, 'MAE_test': 5.5516262832925465}. 
Saved model results as M-A2-CNOT_HWE-CNOT_results.json. 
