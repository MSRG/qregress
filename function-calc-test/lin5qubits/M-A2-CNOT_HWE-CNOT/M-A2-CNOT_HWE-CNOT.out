/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:29:55 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:37 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 17:39:42 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:16 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 17:46:10 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:07 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:51 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 17:57:02 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1667.7750301361084 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:42 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:08 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 18:07:21 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:54 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 18:13:49 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:45 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:50 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 18:25:02 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1679.6422955989838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:42 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 18:35:11 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:42 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 18:41:37 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:36 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:49 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 18:53:00 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1677.8622453212738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:40 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:03 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:03:06 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:37 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 19:09:30 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:26 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 19:15:07 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 19:20:22 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1640.6362526416779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:00 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:28 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:30:34 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:04 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 19:36:55 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:51 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:31 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 19:47:40 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1638.340512752533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:19 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:43 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 19:57:50 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:19 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:04:15 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:11 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:00 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 20:15:13 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1655.7791893482208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:54 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:17 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 20:25:21 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 20:26:52 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:32:00 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:57 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:39 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 20:42:50 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1654.0464322566986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:30 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 20:52:57 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:27 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 20:59:19 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:15 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 21:04:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 21:10:08 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1639.008819103241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:48 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 21:15:22 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 21:20:27 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:03 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 21:26:57 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:27:53 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:34 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 21:37:42 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1653.1751918792725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:21 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:44 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 21:47:46 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:15 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 21:54:47 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:47 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:29 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 22:05:47 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1685.1246762275696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:26 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 22:10:54 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 22:16:17 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 22:17:48 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 22:22:43 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:23:41 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:21 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 22:33:31 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1664.8610525131226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:11 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:38 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 22:43:43 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 22:45:12 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 22:50:05 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:01 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:01:06 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1654.710768699646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:46 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:09 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 23:11:11 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 23:12:42 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 23:17:32 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:29 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 23:23:12 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:28:29 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1643.1927783489227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:09 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:34 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Sun Mar 24 23:38:40 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:10 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Sun Mar 24 23:45:07 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:12 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:10 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Sun Mar 24 23:56:46 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1696.2775056362152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:57:25 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:53 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 00:06:57 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 00:09:02 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 00:13:56 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:53 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 00:19:35 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 00:24:45 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1678.849779367447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:25:24 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:29:49 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 00:34:53 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 00:36:22 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 00:41:13 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:09 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:50 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 00:51:59 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1634.1849024295807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:38 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:04 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:02:08 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:38 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 01:08:31 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:27 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 01:14:14 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 01:19:23 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1644.793791770935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:20:03 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:25 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:29:35 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:06 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 01:35:59 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:59 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:56 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 01:47:16 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1671.5284123420715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:56 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:30 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 01:57:33 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:04 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:03:58 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:04:54 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 02:09:32 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 02:14:41 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1646.7757518291473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:15:21 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 02:19:45 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 02:24:52 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:21 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:31:15 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:11 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 02:36:57 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 02:42:10 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1647.8735620975494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:50 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 02:47:13 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 02:52:15 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 02:53:45 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 02:58:38 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:59:34 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:04:36 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 03:09:45 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1665.2469515800476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:10:34 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 03:15:00 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 03:20:03 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 03:21:33 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 03:26:26 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:23 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:32:04 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 03:37:12 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1636.895102739334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:37:52 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 03:42:17 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 03:47:22 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 03:48:51 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 03:53:45 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:42 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 03:59:24 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:04:37 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1645.7897918224335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:17 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:42 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 04:14:47 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:18 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 04:21:10 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:06 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 04:26:47 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:31:59 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1641.6764261722565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:32:38 2024]  Iteration number: 0 with current cost as 0.2642859279976694 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20862386  0.58776423 -2.82460964
  3.1792738   1.9686533   1.12052736 -1.19632861  0.73773289  1.02892087
  1.27927353 -1.94433233  0.58030298]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:04 2024]  Iteration number: 0 with current cost as 0.23392956174134458 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.17325045  0.56123794 -2.82512964
  3.19637375  1.98330364  1.08270451 -1.18449389  0.68545297  1.02694119
  1.24638998 -1.97340447  0.58651934]. 
[Mon Mar 25 04:42:09 2024]  Iteration number: 50 with current cost as 0.07467460688769484 and parameters 
[-2.90318169  2.23743228 -2.12428052  1.75586821 -0.94881505 -4.05298044
  3.08502074  2.8319549   1.70237243 -2.01150843 -1.60214896 -0.28234442
  1.87213248 -2.00966525  0.42822748]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:39 2024]  Iteration number: 0 with current cost as 0.24307188392115014 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.20238456  0.58021467 -2.82925882
  3.18922694  1.97381796  1.10042734 -1.18976189  0.72355106  1.03245807
  1.27292896 -1.95063129  0.57903309]. 
[Mon Mar 25 04:48:33 2024]  Iteration number: 50 with current cost as 0.06375593642100162 and parameters 
[-2.90318321  2.23743503 -2.12427964  1.53655949 -1.25698542 -4.31647558
  3.17500028  3.01363954  1.85258217 -4.44121167  0.74469829  2.22909409
  1.89514707 -1.87429424  0.3554874 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:49:29 2024]  Iteration number: 0 with current cost as 0.24186261473692997 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.18816241  0.57316941 -2.8237212
  3.2005931   1.96495993  1.08587725 -1.18980321  0.72150305  1.0317676
  1.2605395  -1.96358718  0.57420652]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:13 2024]  Iteration number: 0 with current cost as 0.246207161539413 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.19174221  0.57487863 -2.82521684
  3.18603399  1.98219526  1.10116097 -1.1868829   0.70562637  1.03034856
  1.26407214 -1.95700963  0.58590519]. 
[Mon Mar 25 04:59:23 2024]  Iteration number: 50 with current cost as 0.07188800435019327 and parameters 
[-2.90318444  2.23743447 -2.12427986  1.64491113 -1.34234236 -4.51169297
  3.0831582   3.04996854  1.75835328 -2.08484296  1.61753432  0.81303192
  1.75662909 -1.62127022  0.41993468]. 
Training complete taking 1642.9373161792755 seconds. 
Discarding model... 

Training complete taking 41406.98629570007 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9209723472595215 seconds. 
Saved predicted values as M-A2-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (43.660027998340006,), 'R2_train': 0.7884818173484388, 'MAE_train': 5.200948661477983, 'MSE_test': 67.99259265483215, 'R2_test': 0.5901533042155023, 'MAE_test': 5.5516262832925465}. 
Saved model results as M-A2-CNOT_HWE-CNOT_results.json. 
