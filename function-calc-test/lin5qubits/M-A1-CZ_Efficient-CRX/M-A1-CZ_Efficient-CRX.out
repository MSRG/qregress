/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:24 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:59 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 17:59:48 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:32 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:15 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:59 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1128.0626969337463 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:38 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:16 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:51 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:28 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:07 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1087.0149059295654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:32:47 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 18:36:27 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:01 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:37 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:17 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1088.990172624588 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:55 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 18:54:32 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:12 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:51 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 19:05:30 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1095.2319610118866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:09 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:43 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:19 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:52 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:29 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1080.2716274261475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:10 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 19:30:59 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:51 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:37 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:23 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1134.388516664505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:46:05 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:55 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:49 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:42 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:32 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1151.1908211708069 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:05:21 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 20:09:01 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:11 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:23 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:10 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1178.455130815506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:25:12 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:17 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:16 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:07 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:32 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1235.8074698448181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:45:48 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:44 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:42 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:30 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 21:01:21 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1180.0873410701752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:05:17 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 21:09:06 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 21:12:47 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:22 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 21:19:51 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1096.4999642372131 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:23:23 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 21:26:50 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:26 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:05 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:36 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1065.9991400241852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:41:08 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:37 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 21:48:14 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:42 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 21:55:14 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1056.9121129512787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:58:43 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:18 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:56 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:38 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:51 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1134.2605395317078 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:55 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 22:21:46 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:37 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 22:29:32 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 22:33:25 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1168.2941026687622 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:18 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:10 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 22:45:01 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:53 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:51 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1161.5688083171844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:56:42 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:29 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:19 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 23:08:14 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:14 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1165.6061158180237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:02 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:55 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:45 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:34 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:25 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1150.1127436161041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:20 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:03 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:05 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:15 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:21 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1196.8191599845886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:20 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:16 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:29 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:54 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 00:12:05 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1251.7142374515533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:16:15 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Mon Mar 25 00:20:20 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 00:24:24 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 00:28:43 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 00:32:55 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1243.543704509735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:37:06 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Mon Mar 25 00:41:07 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 00:45:24 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 00:49:41 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 00:53:41 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1252.0629410743713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:57:57 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Mon Mar 25 01:02:10 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 01:06:33 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 01:10:37 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 01:14:56 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1272.052041053772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:19:10 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Mon Mar 25 01:23:17 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 01:27:35 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:42 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:43 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1248.242952823639 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:39:51 2024]  Iteration number: 0 with current cost as 0.10822095877363991 and parameters 
[-4.6631934   2.23743456 -2.12427964 -0.11653096  0.55388708 -2.77010911
  3.06858484  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432438
  1.31029899 -1.87354666]. 
Working on 0.4 fold... 
[Mon Mar 25 01:43:59 2024]  Iteration number: 0 with current cost as 0.09061534890066025 and parameters 
[-4.69009508  2.23743455 -2.12427964 -0.11653103  0.55388699 -2.77010906
  3.0685849   2.18960145  1.18552007 -1.06648317  0.60271502  1.14432437
  1.31029907 -1.87354654]. 
Working on 0.6 fold... 
[Mon Mar 25 01:48:00 2024]  Iteration number: 0 with current cost as 0.08455690745840126 and parameters 
[-4.66452268  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.7701089
  3.06858498  2.18960153  1.18552014 -1.06648301  0.60271518  1.14432438
  1.31029906 -1.87354657]. 
Working on 0.8 fold... 
[Mon Mar 25 01:52:11 2024]  Iteration number: 0 with current cost as 0.08874339162158745 and parameters 
[-4.68039068  2.23743456 -2.12427956 -0.11653095  0.55388708 -2.77010905
  3.06858491  2.18960152  1.18552013 -1.06648308  0.60271517  1.14432445
  1.31029906 -1.87354658]. 
Working on 1.0 fold... 
[Mon Mar 25 01:56:44 2024]  Iteration number: 0 with current cost as 0.09587137137514981 and parameters 
[-4.67463181  2.23743455 -2.12427964 -0.11653094  0.55388708 -2.77010906
  3.06858482  2.18960154  1.18552007 -1.06648317  0.6027151   1.14432437
  1.31029907 -1.87354655]. 
Training complete taking 1268.560842037201 seconds. 
Discarding model... 

Training complete taking 29091.75097179413 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.1317434310913086 seconds. 
Saved predicted values as M-A1-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (58.35083788344123,), 'R2_train': 0.7173097739247757, 'MAE_train': 5.013475895553207, 'MSE_test': 77.63284321281058, 'R2_test': 0.5320436678058467, 'MAE_test': 5.137892200950741}. 
Saved model results as M-A1-CZ_Efficient-CRX_results.json. 
