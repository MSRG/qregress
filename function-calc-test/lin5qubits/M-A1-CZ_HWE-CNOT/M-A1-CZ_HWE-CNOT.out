/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:05 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:00 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:06 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:05 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:04 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1198.007883310318 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:03 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:55 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:56 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1192.4105784893036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:58 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:49 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:16 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:17 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:17 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1222.8808901309967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:18 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:12 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:16 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:16 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:25 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1215.6535148620605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:35 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:35 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:45 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:05:47 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:45 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1225.0998077392578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:00 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:50 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:47 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:45 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:42 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1180.327493429184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:40 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:54 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:53 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:52 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:49:49 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1209.2960674762726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:39 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:36 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:34 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:31 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1181.6483237743378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:29 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:22 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:21 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:20 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:17 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1184.3356087207794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:15 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:08 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:06 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:45:03 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:45 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1231.8044140338898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:47 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:39 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:37 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:34 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:31 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1182.2481842041016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:28 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:46 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:44 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:43 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:43 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1218.5869238376617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:37:41 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:08 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:10 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:13 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1227.7762649059296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:16 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:06 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:02:04 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:02 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:09 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1203.5843546390533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:19 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:18:12 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:11 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:26:10 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:15 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1195.654934167862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:15 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:06 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:20 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:21 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:32 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1216.3650069236755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:30 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:58:30 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:02:38 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:09 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:11:30 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1258.47150182724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:15:28 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:28 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:29 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:28 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:27 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1201.606199502945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:30 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:20 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:20 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:19 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:26 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1202.9654495716095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:33 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:26 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:26 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:24 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:11:22 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1187.1714148521423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:31 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:24 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:23:40 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:27:43 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:31:42 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1224.8726913928986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:35:45 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:39:37 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:54 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:47:52 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:51:49 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1202.9136691093445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:40 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:44 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:51 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:47 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1195.7903645038605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:15:44 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:35 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:35 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:27:33 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:40 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1194.2916867733002 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:35:38 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:28 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:32 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:47:31 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:51:50 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1210.256340265274 seconds. 
Discarding model... 

Training complete taking 30164.021513223648 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.989011287689209 seconds. 
Saved predicted values as M-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (77.45261861492452,), 'R2_train': 0.6247680571424233, 'MAE_train': 7.4589885786647345, 'MSE_test': 84.17831231321247, 'R2_test': 0.49258879811470546, 'MAE_test': 7.787463416671031}. 
Saved model results as M-A1-CZ_HWE-CNOT_results.json. 
