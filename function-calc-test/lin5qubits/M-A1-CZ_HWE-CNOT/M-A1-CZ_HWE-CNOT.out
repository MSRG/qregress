/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:05 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:00 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:06 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:05 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:04 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1198.007883310318 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:03 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:55 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:56 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1192.4105784893036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:58 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:49 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:16 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:17 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:17 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1222.8808901309967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:18 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:12 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:16 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:16 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:25 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1215.6535148620605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:35 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:35 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:45 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:05:47 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:45 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1225.0998077392578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:00 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:50 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:47 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:45 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:42 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1180.327493429184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:40 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:54 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:53 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:52 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 19:49:49 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1209.2960674762726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:39 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:36 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:34 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:31 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1181.6483237743378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:29 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:22 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:21 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:20 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:17 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1184.3356087207794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:15 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:08 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:06 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 20:45:03 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:45 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1231.8044140338898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:47 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:39 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:37 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:34 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:31 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1182.2481842041016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:28 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:46 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:44 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:43 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:43 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1218.5869238376617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:37:41 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:08 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:10 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:13 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1227.7762649059296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:16 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:06 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:02:04 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:02 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:09 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1203.5843546390533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:19 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:18:12 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:11 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:26:10 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:15 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1195.654934167862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:15 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:06 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:20 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:21 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:32 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1216.3650069236755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:30 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 22:58:30 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:02:38 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:09 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:11:30 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1258.47150182724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:15:28 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:28 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:29 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:28 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:27 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1201.606199502945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:30 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:20 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:20 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:19 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:26 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1202.9654495716095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:33 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Sun Mar 24 23:59:26 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:26 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:24 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:11:22 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1187.1714148521423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:31 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:24 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:23:40 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:27:43 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:31:42 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1224.8726913928986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:35:45 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:39:37 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:54 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 00:47:52 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 00:51:49 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1202.9136691093445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:48 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:40 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:44 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:51 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:47 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1195.7903645038605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:15:44 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:35 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:35 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:27:33 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:40 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1194.2916867733002 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:35:38 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:28 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:32 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Mon Mar 25 01:47:31 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Mon Mar 25 01:51:50 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1210.256340265274 seconds. 
Discarding model... 

Training complete taking 30164.021513223648 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.989011287689209 seconds. 
Saved predicted values as M-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (77.45261861492452,), 'R2_train': 0.6247680571424233, 'MAE_train': 7.4589885786647345, 'MSE_test': 84.17831231321247, 'R2_test': 0.49258879811470546, 'MAE_test': 7.787463416671031}. 
Saved model results as M-A1-CZ_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:06:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:21 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:10 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 12:14:01 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 12:17:57 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:54 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1165.0123434066772 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:46 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:31 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 12:33:24 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:14 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:03 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1150.760580778122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:44:56 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:41 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:32 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 12:56:24 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 13:00:17 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1152.3933987617493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:12 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:12 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:00 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:50 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 13:20:09 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1187.3354985713959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:56 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:38 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 13:31:27 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 13:35:18 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 13:39:25 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1160.2805514335632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:17 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:58 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:50 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:46 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:37 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1152.072764158249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:02:28 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 14:06:10 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:59 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 14:13:49 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 14:17:40 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1140.570877313614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:21:29 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 14:25:18 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 14:29:07 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:57 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 14:36:49 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1149.5553812980652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:40:38 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:22 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:10 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 14:51:59 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 14:55:48 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1146.244930267334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:59:45 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 15:03:34 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 15:07:24 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 15:11:12 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 15:15:02 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1151.4485321044922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:18:57 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 15:22:45 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 15:26:36 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 15:30:25 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 15:34:25 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1155.9650490283966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:38:12 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:54 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 15:45:57 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 15:49:51 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 15:53:41 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1157.8546359539032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:57:31 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 16:01:15 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 16:05:03 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 16:08:53 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:43 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1143.4309833049774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:16:33 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 16:20:32 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 16:24:20 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 16:28:07 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 16:31:57 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1153.0833089351654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:35:46 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 16:39:30 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 16:43:19 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 16:47:08 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 16:50:55 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1139.2289803028107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:54:46 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 16:58:32 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 17:02:35 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 17:06:29 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 17:10:16 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1160.151598930359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:14:06 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 17:17:49 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 17:21:39 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 17:25:28 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 17:29:18 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1140.7310848236084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:33:06 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 17:36:56 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 17:40:45 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 17:44:35 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 17:48:23 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1145.9906866550446 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:52:13 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 17:55:55 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 17:59:44 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 18:03:33 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 18:07:22 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1138.7011551856995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:11:11 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 18:14:55 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 18:18:49 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 18:22:38 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 18:26:26 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1144.5885553359985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:30:16 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 18:34:00 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 18:37:51 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 18:41:40 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 18:45:27 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1140.4707384109497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:49:16 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 18:53:01 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 18:56:58 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 19:00:48 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 19:04:37 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1147.1253156661987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:08:23 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 19:12:07 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 19:15:58 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 19:19:48 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 19:23:39 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1143.1407964229584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:27:27 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 19:31:09 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 19:34:59 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 19:38:48 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 19:42:37 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1139.626586675644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:46:26 2024]  Iteration number: 0 with current cost as 0.34280017581763084 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.17458694  0.57700422 -2.80167121
  3.09684206  2.24843449  1.10330076 -1.06918196  0.67625016  1.16403959
  1.43384744 -1.77142893  0.68083976]. 
Working on 0.4 fold... 
[Thu Apr  4 19:50:09 2024]  Iteration number: 0 with current cost as 0.32039133247400514 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.18538016  0.58764514 -2.79746879
  3.11073902  2.26346777  1.07078164 -1.06674891  0.69732319  1.17358167
  1.40384349 -1.7945431   0.69872701]. 
Working on 0.6 fold... 
[Thu Apr  4 19:54:00 2024]  Iteration number: 0 with current cost as 0.30627609201979383 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.17516591  0.58063881 -2.79658185
  3.10234212  2.25418847  1.09054206 -1.07095879  0.68318406  1.16405065
  1.41593012 -1.7867689   0.68601903]. 
Working on 0.8 fold... 
[Thu Apr  4 19:57:51 2024]  Iteration number: 0 with current cost as 0.31225982897134785 and parameters 
[-2.90318344  2.23743464 -2.12427963 -0.17782546  0.58183063 -2.79781685
  3.10905446  2.252035    1.08019671 -1.06128184  0.6843919   1.17617501
  1.41563578 -1.78518352  0.69268186]. 
Working on 1.0 fold... 
[Thu Apr  4 20:01:41 2024]  Iteration number: 0 with current cost as 0.32411078502870383 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.18196558  0.58476465 -2.79802686
  3.10668435  2.25783613  1.08097892 -1.07173421  0.69108028  1.16557819
  1.41073695 -1.79062602  0.68962892]. 
Training complete taking 1144.8496570587158 seconds. 
Discarding model... 

Training complete taking 28750.615624666214 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0144689083099365 seconds. 
Saved predicted values as M-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (77.45261861492452,), 'R2_train': 0.6247680571424233, 'MAE_train': 7.4589885786647345, 'MSE_test': 84.17831231321247, 'R2_test': 0.49258879811470546, 'MAE_test': 7.787463416671031}. 
Saved model results as M-A1-CZ_HWE-CNOT_results.json. 
