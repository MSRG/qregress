/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:00 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:03 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:32:07 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:33:04 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:34:01 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 296.2970521450043 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:57 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:01 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:04 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:01 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:38:57 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 296.6487445831299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:54 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:57 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:00 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:57 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:43:57 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 307.24799275398254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:02 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:07 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:09 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:06 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:03 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 299.68680810928345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:00 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:02 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:05 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:02 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:02 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 297.80068278312683 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:58 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:01 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:22 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:19 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:15 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 314.12490916252136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:12 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:15 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:19 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:15 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:22 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 306.2105391025543 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:19 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:22 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:07:25 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:22 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:19 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 296.96487760543823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:15 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:21 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:25 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:22 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:19 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 299.5971760749817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:14 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:19 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:29 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:27 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:24 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 306.2727928161621 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:20:24 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:27 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:33 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:40 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:37 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 313.36023139953613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:34 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:26:37 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:46 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:43 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:40 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 305.55190443992615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:40 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:43 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:46 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:44 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:42 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 303.4061830043793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:43 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:36:47 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:52 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:47 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:44 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 298.4770476818085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:42 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:45 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:58 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:55 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:44:52 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 306.6850938796997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:49 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:03 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:06 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:12 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:09 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 317.4055869579315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:06 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:09 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:12 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:22 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:18 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 309.9263527393341 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:15 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:18 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:28 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:25 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:23 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 302.80584239959717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:18 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:23 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:03:26 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:24 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:05:20 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 298.14071249961853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:16 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:07:22 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:32 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:52 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:47 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 331.8884584903717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:48 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:53 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:57 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:54 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:15:48 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 298.09605598449707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:47 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:49 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:18:58 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:55 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:13 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 323.296630859375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:22:10 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:17 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:21 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:18 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:15 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 304.9123148918152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:15 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:17 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:20 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:18 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:18 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 304.31927394866943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:19 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:24 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:27 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:35:24 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:19 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 297.73289251327515 seconds. 
Discarding model... 

Training complete taking 7636.857675790787 total seconds. 
Now scoring model... 
Scoring complete taking 0.9880852699279785 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (135.53407644747557,), 'R2_train': 0.34338288700552877, 'MAE_train': 10.75948909842871, 'MSE_test': 137.809009963769, 'R2_test': 0.16931293281151805, 'MAE_test': 10.655559793897101}. 
Saved model results as A2-A2-CNOT_HWE-CZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:27 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:40 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:29:40 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:30:42 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:31:41 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:32:36 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 291.44770431518555 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:33:31 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:34:32 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:35:32 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:36:27 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:37:22 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 285.7798984050751 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:38:17 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:39:17 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:18 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:41:12 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:42:07 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 284.9669053554535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:02 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:44:03 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:45:04 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:46:00 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:46:54 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 286.38658714294434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:48 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:50 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:49:52 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:50:45 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:51:42 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 288.96092414855957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:52:37 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:53:37 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:54:38 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:55:33 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:56:29 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 286.7416262626648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:57:24 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:58:25 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:59:26 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:00:21 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:01:16 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 287.0891308784485 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:11 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:03:12 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:04:13 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:05:08 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:06:03 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 285.8076777458191 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:58 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:07:59 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:09:00 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:09:55 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:10:49 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 287.5127546787262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:11:44 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:12:45 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:46 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:42 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:15:37 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 288.3920171260834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:33 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:17:34 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:35 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:19:30 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:20:25 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 288.3759174346924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:21 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:22:23 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:23:24 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:24:20 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:15 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 289.54542112350464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:26:10 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:27:11 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:12 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:29:07 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:02 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 286.2356860637665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:59 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:32:00 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:33:01 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:01 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:56 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 293.38567543029785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:50 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:53 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:54 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:38:48 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:39:42 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 287.6567120552063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:40:38 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:41:39 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:42:46 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:41 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:44:36 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 293.35243558883667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:31 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:32 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:47:56 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:49:04 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:59 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 322.48280572891235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:55 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:56 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:57 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:51 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:47 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 288.54481959342957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:55:42 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:44 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:57:47 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:44 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:39 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 292.60767793655396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:35 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:36 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:02:38 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:32 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:04:27 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 287.69299149513245 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:05:22 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:06:24 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:07:25 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:19 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:14 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 287.01155972480774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:09 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:11:10 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:13 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:13:08 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:14:03 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 286.71687412261963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:14:58 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:16:00 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:17:01 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:56 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:50 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 289.0638704299927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:45 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:45 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:47 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:42 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:38 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 288.64002895355225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:34 2024]  Iteration number: 0 with current cost as 0.247090918263202 and parameters 
[-3.01512808  2.13238182 -1.86073841 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:25:35 2024]  Iteration number: 0 with current cost as 0.2250103144286847 and parameters 
[-3.02131248  2.13594182 -1.85353274 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:35 2024]  Iteration number: 0 with current cost as 0.22223841808564548 and parameters 
[-3.02390183  2.1357917  -1.84922732 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:27:30 2024]  Iteration number: 0 with current cost as 0.22424481755829995 and parameters 
[-3.01220175  2.13973075 -1.87124237 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:28:36 2024]  Iteration number: 0 with current cost as 0.2268624586649289 and parameters 
[-3.02506662  2.1316456  -1.84408749 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 297.2323269844055 seconds. 
Discarding model... 

Training complete taking 7251.631255149841 total seconds. 
Now scoring model... 
Scoring complete taking 0.9700098037719727 seconds. 
Saved predicted values as A2-A2-CNOT_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (135.53407644747557,), 'R2_train': 0.34338288700552877, 'MAE_train': 10.75948909842871, 'MSE_test': 137.809009963769, 'R2_test': 0.16931293281151805, 'MAE_test': 10.655559793897101}. 
Saved model results as A2-A2-CNOT_HWE-CZ_results.json. 
