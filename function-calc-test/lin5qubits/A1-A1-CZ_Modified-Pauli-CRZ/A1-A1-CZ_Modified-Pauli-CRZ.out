/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:13 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:52 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:33:10 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:34:21 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:35:32 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 352.72865104675293 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:36:45 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:10 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:06 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:41:28 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.7460153102875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:41 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:00 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:56 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:09 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:20 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.9039659500122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:32 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:52 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:48 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:52:02 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:15 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.7590534687042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:26 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:46 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:43 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:56 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:09 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.3056342601776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:20 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:41 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:38 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:49 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.91362380981445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:16 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:36 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:31 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:45 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:57 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.85923767089844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:09 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:30 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:26 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:39 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:51 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.5564181804657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:03 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:28 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:23 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:39 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:52 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.90182614326477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:05 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:25 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:22 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:34 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:46 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.03939867019653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:58 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:19 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:15 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:26 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:41 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.5845470428467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:13 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:11 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:34 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 352.7283058166504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:47 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:07 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:04 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:19 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:32 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.0167324542999 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:45 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:04 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:02 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:15 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.8863408565521 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:41 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:07 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:03 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:17 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:29 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.19529914855957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:43 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:02 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:59 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:13 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:24 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.6200556755066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:36 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:59 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:05 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:36 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 373.7437307834625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:50 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:12 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:19 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:30 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:48 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.84809160232544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:01 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:32 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:33 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:48 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:00 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.81181263923645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:26 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:46 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:42 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:27:53 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:03 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.56711316108704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:18 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:37 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:33 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:46 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.04457330703735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:13 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:32 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:40 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:52 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:03 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3794801235199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:15 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:37 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:35 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:50 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 357.51040744781494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:13 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:31 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:36 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:49 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:01 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 360.63054275512695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:54:14 2024]  Iteration number: 0 with current cost as 0.3700387255470995 and parameters 
[-1.82176683  2.23743458 -2.12427953 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029904 -1.87354675  0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.3122912106762491 and parameters 
[-1.54439865  2.23743459 -2.12427959 -0.11653103  0.55388713 -2.77010902
  3.06858493  2.18960145  1.18552003 -1.06648313  0.60271515  1.1443244
  1.31029903 -1.87354675  0.72965075  2.88578424 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:56:53 2024]  Iteration number: 0 with current cost as 0.3315072595772406 and parameters 
[-1.43602935  2.23743458 -2.12427952 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.87354674  0.72965069  2.88578419 -0.54534341 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:05 2024]  Iteration number: 0 with current cost as 0.33672967186451364 and parameters 
[-1.41170488  2.23743464 -2.12427952 -0.11653097  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271516  1.14432445
  1.31029904 -1.87354674  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:17 2024]  Iteration number: 0 with current cost as 0.33971857161021435 and parameters 
[-1.40907062  2.23743464 -2.12427958 -0.11653103  0.55388702 -2.77010909
  3.06858498  2.18960145  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674  0.72965074  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 376.08386969566345 seconds. 
Discarding model... 

Training complete taking 8976.365704536438 total seconds. 
Now scoring model... 
Scoring complete taking 0.9617588520050049 seconds. 
Saved predicted values as A1-A1-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274676516,), 'R2_train': -0.0025489989050868633, 'MAE_train': 12.57727151849314, 'MSE_test': 191.37228842545193, 'R2_test': -0.15355654216717363, 'MAE_test': 12.01322042722634}. 
Saved model results as A1-A1-CZ_Modified-Pauli-CRZ_results.json. 
