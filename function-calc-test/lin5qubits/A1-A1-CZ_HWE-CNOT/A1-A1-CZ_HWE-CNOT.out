/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:52:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 17:57:24 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:27 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:40 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1130.9416408538818 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:03 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:16:08 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:23 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:45 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:15 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:36 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1133.622608423233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:58 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:35:06 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:17 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:06 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:21 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1122.4118280410767 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:53:43 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:55 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:14 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:57 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:17 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1138.6179132461548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:38 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:12:39 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:51 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:12 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:37 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:04 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1139.0177710056305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:48 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:31:53 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:04 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:24 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:53 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:25 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1162.7524764537811 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:46:01 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:51:04 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:15 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:38 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:10 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:32 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1138.6961674690247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:59 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:10:34 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:45 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:36 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:50 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1154.153415441513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:29:27 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:39 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:00 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:30 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:08 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1154.7573540210724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:29 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:48:32 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:43 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:04 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:30 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:44 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1117.761477470398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:02:06 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:07:08 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:19 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:39 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:14:08 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:27 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1125.9417488574982 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:20:51 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:26:48 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:00 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:19 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:58 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:14 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1188.3268644809723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:40 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:45:42 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:45:53 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:41 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:23 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:51 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1171.4669888019562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:05:13 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:24 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:43 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:12:08 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:15:22 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1110.486501455307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:18:42 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:23:44 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:23:55 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:33:55 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1114.1079804897308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:15 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:42:16 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:45:47 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:49:13 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:28 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1113.2823169231415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:48 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:00:51 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:02 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:23 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:49 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:11:03 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1115.7810082435608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:24 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:19:28 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:41 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:00 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:26:25 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:29:40 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1118.5329885482788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:03 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:38:04 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:17 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:34 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:45:00 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:48:14 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1110.4852755069733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:33 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:56:36 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:56:48 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:00:09 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:35 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:06:58 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1123.8364670276642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:10:18 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:15:25 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:15:36 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:19:16 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:22:42 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:26:20 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1161.9018802642822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:29:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:34:48 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:34:59 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:38:19 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:41:51 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:45:05 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1134.2586116790771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:48:33 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:53:37 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:53:50 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:12 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:00:40 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:03:56 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1126.4500615596771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:21 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 01:12:36 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:49 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 01:16:07 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:39 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:22:59 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1139.1768617630005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:26:20 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 01:31:23 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 01:31:34 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 01:34:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:26 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:39 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1122.4403219223022 seconds. 
Discarding model... 

Training complete taking 28369.209784030914 total seconds. 
Now scoring model... 
Scoring complete taking 0.9097073078155518 seconds. 
Saved predicted values as A1-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (92.77359297339554,), 'R2_train': 0.5505430783385121, 'MAE_train': 7.815642370651912, 'MSE_test': 108.54235460391446, 'R2_test': 0.34572688509000316, 'MAE_test': 8.255832775750633}. 
Saved model results as A1-A1-CZ_HWE-CNOT_results.json. 
