/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:54 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:05:05 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:41 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:21:52 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:06 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:43 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2849.75998878479 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:19 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:10 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:52:35 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:08 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:09:02 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:17 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:52 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2837.4697551727295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:36 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:23 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:40:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:50 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:56:55 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 20:00:13 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:46 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2868.87558221817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:24 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:27:32 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:03 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 20:44:02 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:12 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:49 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2840.2313554286957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:45 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:27 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:14:59 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:35 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:31:33 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:47 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:40:20 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2850.648699760437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:15 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:47 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:03:02 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:36 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 22:19:25 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:35 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2876.327248096466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:11 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:04 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:50:04 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:42 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:06:27 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:35 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:15:10 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2792.9623503684998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:44 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:26:30 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:36:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:01 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:52:50 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 23:56:06 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:01:42 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2805.03334236145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:07:30 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:13:18 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:23:54 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 00:27:25 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:40:10 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 00:43:20 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:48:53 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2815.410813808441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:00:14 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:10:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 01:13:47 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 01:26:51 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:01 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:36 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2817.540298461914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:21 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:47:05 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:57:39 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:16 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 02:14:49 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 02:18:08 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:23:46 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2878.1638526916504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:29:20 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:04 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:45:02 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 02:48:32 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:01:17 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 03:04:26 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:10:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2824.8528673648834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:16:24 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:22:23 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:32:20 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 03:35:51 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:48:44 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 03:51:52 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:57:34 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2803.3295996189117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:03:09 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:41 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:19:51 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 04:23:22 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 04:36:56 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 04:40:09 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:45:49 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2894.0984556674957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:51:23 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:57:09 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:07:12 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 05:10:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:23:47 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 05:26:55 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:29 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2797.5197825431824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:00 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:43:46 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:53:48 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 05:57:19 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:10:21 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 06:13:49 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:19:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2857.570837020874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:25:37 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:31:30 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:41:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 06:45:02 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:57:59 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 07:01:08 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:06:42 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2799.5016820430756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:12:18 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:18:20 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:28:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 07:32:02 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 07:45:29 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 07:48:38 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:54:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2870.075623035431 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:00:08 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:05:56 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:16:29 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 08:20:24 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 08:33:53 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 08:37:03 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:42:36 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2882.578288078308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:48:11 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:53:58 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:04:34 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 09:08:05 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 09:21:28 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 09:24:39 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:30:14 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2859.4702768325806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 09:35:49 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:41:38 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:51:46 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 09:55:21 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 10:08:16 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 10:11:28 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:17:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2836.255747795105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 10:23:06 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:28:50 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:38:56 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 10:42:30 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 10:55:34 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 10:58:44 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:04:19 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2809.804321050644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 11:09:55 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 11:16:08 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:26:06 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 11:29:37 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 11:42:26 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 11:45:37 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:51:10 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2808.678015232086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 11:56:45 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:02:29 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:12:26 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 12:15:57 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 12:28:50 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 12:31:58 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:37:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2782.621551513672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 12:43:07 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:48:52 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:58:51 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 13:02:22 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 13:15:06 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 13:18:15 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 13:23:50 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2776.148781299591 seconds. 
Discarding model... 

Training complete taking 70834.93084573746 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0444462299346924 seconds. 
Saved predicted values as M-A1-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (56.69639960769639,), 'R2_train': 0.725324972114938, 'MAE_train': 4.6821574885332335, 'MSE_test': 68.21126780162588, 'R2_test': 0.5888351711238791, 'MAE_test': 4.998572948563718}. 
Saved model results as M-A1-CZ_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:26:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:26:32 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:32:17 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:42:27 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 12:46:05 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 12:59:09 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:19 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:53 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2818.5491445064545 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:13:29 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:19:10 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:29:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:49 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 13:45:49 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:59 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:54:40 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2803.3705022335052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:12 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:59 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:16:15 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:50 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 14:32:54 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 14:36:07 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:41:45 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2825.4688601493835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:47:40 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:38 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:03:40 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 15:07:11 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 15:20:34 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 15:23:53 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:29:47 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2893.283492088318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:35:29 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:12 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:51:13 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:07:30 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:40 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:16:13 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2774.585533618927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:21:45 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:27:28 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:37:25 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 16:40:55 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 16:53:58 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 16:57:05 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:02:41 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2789.1236011981964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:08:15 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:13:58 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:23:57 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 17:27:36 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 17:40:43 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 17:43:51 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:49:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2814.888613462448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:55:08 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:53 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:10:57 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 18:14:28 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 18:27:43 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 18:30:53 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:27 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2815.45360660553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:42:05 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:47:53 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:57:52 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 19:01:24 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 19:14:10 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 19:17:20 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:22:53 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2779.5140686035156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:28:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:34:10 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:44:12 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 19:47:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:00:30 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 20:03:54 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:09:28 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2796.240369796753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:15:00 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:20:42 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:30:41 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:11 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 20:46:58 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 20:50:07 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:55:41 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2773.615735054016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:01:15 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:06:59 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:16:57 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 21:20:28 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 21:33:20 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 21:36:30 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:42:03 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2781.3529255390167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:47:36 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:53:20 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:03:26 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 22:07:10 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 22:20:03 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 22:23:15 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:28:50 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2811.8692715168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:34:26 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:40:12 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:50:12 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 22:53:57 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:06:44 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 23:09:52 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:15:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2790.7181310653687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:20:57 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:26:45 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:36:47 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:18 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Thu Apr  4 23:53:06 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Thu Apr  4 23:56:14 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:01:47 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2781.7566137313843 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:20 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:13:06 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:23:10 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 00:26:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 00:39:33 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 00:42:49 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:48:23 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2802.7056424617767 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:54:01 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:59:45 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:09:47 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 01:13:21 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 01:26:10 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 01:29:18 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:34:52 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2787.475606918335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:40:30 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:46:14 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:56:15 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:46 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 02:12:47 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 02:15:56 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:21:30 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2794.3956809043884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:27:05 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:32:51 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:42:51 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 02:46:23 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 02:59:06 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 03:02:17 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:07:51 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2779.7377712726593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:13:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:19:13 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:29:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 03:32:50 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 03:45:37 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 03:48:45 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:54:17 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2787.3200976848602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:59:52 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:05:37 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:15:38 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 04:19:09 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 04:31:58 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 04:35:07 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:40:40 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2780.4841520786285 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:46:12 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:52:18 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:02:21 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 05:05:57 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 05:18:50 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 05:22:02 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:27:39 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2823.0411088466644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:33:15 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:39:05 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:49:09 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 05:52:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 06:05:37 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 06:08:46 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:14:21 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2800.725243806839 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:19:55 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:25:41 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:35:40 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 06:39:12 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 06:52:14 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 06:55:24 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:01:07 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2811.3350064754486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:06:47 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:12:32 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:22:31 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Fri Apr  5 07:26:16 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Fri Apr  5 07:39:15 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Fri Apr  5 07:42:23 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:47:58 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2829.935022830963 seconds. 
Discarding model... 

Training complete taking 70046.94777202606 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0696368217468262 seconds. 
Saved predicted values as M-A1-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (56.69639960769639,), 'R2_train': 0.725324972114938, 'MAE_train': 4.6821574885332335, 'MSE_test': 68.21126780162588, 'R2_test': 0.5888351711238791, 'MAE_test': 4.998572948563718}. 
Saved model results as M-A1-CZ_Full-Pauli-CRZ_results.json. 
