/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:54 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:05:05 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:41 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 18:21:52 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:06 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:43 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2849.75998878479 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:19 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:10 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:52:35 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:08 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:09:02 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:17 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:52 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2837.4697551727295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:36 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:23 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:40:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:50 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 19:56:55 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 20:00:13 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:46 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2868.87558221817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:24 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:27:32 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:03 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 20:44:02 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:12 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:49 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2840.2313554286957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:45 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:27 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:14:59 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:35 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 21:31:33 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:47 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:40:20 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2850.648699760437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:15 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:47 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:03:02 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:36 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 22:19:25 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:35 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2876.327248096466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:11 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:04 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:50:04 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:42 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:06:27 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:35 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:15:10 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2792.9623503684998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:44 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:26:30 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:36:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:01 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Sun Mar 24 23:52:50 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Sun Mar 24 23:56:06 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:01:42 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2805.03334236145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:07:30 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:13:18 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:23:54 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 00:27:25 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 00:40:10 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 00:43:20 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:48:53 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2815.410813808441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:25 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:00:14 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:10:16 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 01:13:47 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 01:26:51 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:01 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:36 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2817.540298461914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:21 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:47:05 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:57:39 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:16 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 02:14:49 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 02:18:08 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:23:46 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2878.1638526916504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:29:20 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:04 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:45:02 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 02:48:32 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:01:17 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 03:04:26 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:10:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2824.8528673648834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:16:24 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:22:23 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:32:20 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 03:35:51 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 03:48:44 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 03:51:52 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:57:34 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2803.3295996189117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:03:09 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:41 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:19:51 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 04:23:22 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 04:36:56 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 04:40:09 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:45:49 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2894.0984556674957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:51:23 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:57:09 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:07:12 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 05:10:43 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 05:23:47 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 05:26:55 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:29 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2797.5197825431824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:00 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:43:46 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:53:48 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 05:57:19 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:10:21 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 06:13:49 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:19:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2857.570837020874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:25:37 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:31:30 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:41:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 06:45:02 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 06:57:59 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 07:01:08 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:06:42 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2799.5016820430756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:12:18 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:18:20 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:28:30 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 07:32:02 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 07:45:29 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 07:48:38 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:54:24 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2870.075623035431 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:00:08 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:05:56 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:16:29 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 08:20:24 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 08:33:53 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 08:37:03 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:42:36 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2882.578288078308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:48:11 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:53:58 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:04:34 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 09:08:05 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 09:21:28 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 09:24:39 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:30:14 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2859.4702768325806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 09:35:49 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:41:38 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:51:46 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 09:55:21 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 10:08:16 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 10:11:28 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:17:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2836.255747795105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 10:23:06 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:28:50 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:38:56 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 10:42:30 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 10:55:34 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 10:58:44 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:04:19 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2809.804321050644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 11:09:55 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 11:16:08 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:26:06 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 11:29:37 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 11:42:26 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 11:45:37 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:51:10 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2808.678015232086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 11:56:45 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:02:29 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:12:26 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 12:15:57 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 12:28:50 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 12:31:58 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:37:32 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2782.621551513672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 12:43:07 2024]  Iteration number: 0 with current cost as 0.15971984834360353 and parameters 
[-3.2412104   2.27433505 -2.12006721 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61835042  1.14432445
  1.67264903 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:48:52 2024]  Iteration number: 0 with current cost as 0.18116508247068502 and parameters 
[-3.31794361  2.24321454 -2.11025913 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.61377133  1.14432445
  1.76708869 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:58:51 2024]  Iteration number: 50 with current cost as 0.0837929712678295 and parameters 
[-4.48606421  2.04432468 -1.28238794 -0.11651517  0.55389003 -2.77009176
  3.06859777  2.18960345  1.18552134 -1.06650606  1.0503489   1.14432131
  0.29225641 -1.87356558  0.72961266  2.88577072 -0.54534128 -0.47524249
 -2.02652091  0.72897017  1.60509393  2.830767   -1.26456437 -0.25136777]. 
Working on 0.6 fold... 
[Mon Mar 25 13:02:22 2024]  Iteration number: 0 with current cost as 0.16318204984479523 and parameters 
[-3.2865079   2.25757231 -2.11446335 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.61227506  1.14432445
  1.7244615  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
[Mon Mar 25 13:15:06 2024]  Iteration number: 50 with current cost as 0.06672901137968079 and parameters 
[-4.517009    6.62101443 -1.87068902 -0.11653049  0.55388758 -2.77010784
  3.0685839   2.18960287  1.18552164 -1.06648449 -3.90347702  1.14432398
  0.52353581 -1.87354569  0.72965086  2.88578409 -0.54534291 -0.47522382
 -2.02654152  0.72897426  1.60512676  2.8307719  -1.26456679 -0.25135944]. 
Working on 0.8 fold... 
[Mon Mar 25 13:18:15 2024]  Iteration number: 0 with current cost as 0.1694194619858414 and parameters 
[-3.29285167  2.25729121 -2.11541135 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60874907  1.14432445
  1.7333964  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 13:23:50 2024]  Iteration number: 0 with current cost as 0.17440684061681233 and parameters 
[-3.2889231   2.25216996 -2.11450676 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61150411  1.14432445
  1.7317654  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 2776.148781299591 seconds. 
Discarding model... 

Training complete taking 70834.93084573746 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0444462299346924 seconds. 
Saved predicted values as M-A1-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (56.69639960769639,), 'R2_train': 0.725324972114938, 'MAE_train': 4.6821574885332335, 'MSE_test': 68.21126780162588, 'R2_test': 0.5888351711238791, 'MAE_test': 4.998572948563718}. 
Saved model results as M-A1-CZ_Full-Pauli-CRZ_results.json. 
