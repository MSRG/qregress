/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:33:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:26 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:23 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:33 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:36:02 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:36:52 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.00725030899048 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:35 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:24 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:17 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:46 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:34 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 222.06989169120789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:41:27 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:42:17 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:06 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:37 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:28 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 233.9874963760376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:12 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:02 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:52 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:24 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:48:19 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 231.04279446601868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:02 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:55 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:45 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:15 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:03 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 223.97121405601501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:48 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:53:36 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:54:26 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:55 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:47 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 224.1025869846344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:31 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:20 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:10 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:39 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:29 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 222.19595980644226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:14 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:02 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:01:52 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:23 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:12 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 221.91069555282593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:03:56 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:46 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:35 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:04 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:01 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 229.89320015907288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:46 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:34 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:24 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:53 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:43 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 220.517498254776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:27 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:12:16 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:05 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:35 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:25 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 223.8888761997223 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:09 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:58 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:05 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:34 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.27369022369385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:18 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:20:06 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:56 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:26 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:15 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 221.301504611969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:23:01 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:59 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:49 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:18 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:07 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 232.95353507995605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:55 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:44 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:28:36 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:07 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:57 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 230.68753266334534 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:41 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:31 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:22 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:32:52 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:51 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 233.55354762077332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:34:34 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:23 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:11 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:40 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:29 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 217.45077562332153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:12 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:01 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:05 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:40:34 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:41:23 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 234.53910064697266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:06 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:58 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:49 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:18 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:05 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 222.05606722831726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:50 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:37 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:47:27 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:56 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:43 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 221.22726774215698 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:34 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:28 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:20 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:50 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:37 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 230.89900851249695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:22 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:54:10 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:59 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:29 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:21 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 225.95906257629395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:57:16 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:07 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:56 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:25 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:13 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 229.6491503715515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:58 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:47 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:35 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:04 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:03:53 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 219.7442111968994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:36 2024]  Iteration number: 0 with current cost as 0.38011340407366556 and parameters 
[-1.86063587  2.23743464 -2.12427958 -0.11653103  0.55388705 -2.77010897
  3.06858495  2.18960148  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965075  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:33 2024]  Iteration number: 0 with current cost as 0.3402131780397023 and parameters 
[-1.73750486  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:22 2024]  Iteration number: 0 with current cost as 0.3214981746198782 and parameters 
[-1.59867868  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960148  1.18552002 -1.06648312  0.60271513  1.14432448
  1.31029899 -1.8735468   0.72965074  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:13 2024]  Iteration number: 0 with current cost as 0.3568027324609079 and parameters 
[-1.80309345  2.23743461 -2.12427961 -0.116531    0.55388705 -2.770109
  3.06858496  2.18960148  1.18552004 -1.06648311  0.60271513  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578419 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:02 2024]  Iteration number: 0 with current cost as 0.35714830453772717 and parameters 
[-1.79851555  2.23743466 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858496  2.1896015   1.18552006 -1.06648306  0.60271515  1.1443245
  1.31029901 -1.87354675  0.72965078  2.88578424 -0.54534338 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 249.80589818954468 seconds. 
Discarding model... 

Training complete taking 5725.689086198807 total seconds. 
Now scoring model... 
Scoring complete taking 0.758350133895874 seconds. 
Saved predicted values as A1_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.69854451800143,), 'R2_train': -0.001385076867856716, 'MAE_train': 12.570106825680046, 'MSE_test': 193.76226346793214, 'R2_test': -0.16796286697288498, 'MAE_test': 12.06528046815796}. 
Saved model results as A1_Modified-Pauli-CRZ_results.json. 
