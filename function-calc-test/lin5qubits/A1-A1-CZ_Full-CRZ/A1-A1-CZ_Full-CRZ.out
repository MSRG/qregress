test.sh: line 9: python: command not found
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_train.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_train.bin into X and y data. 
Loading dataset from /home/grierjones/qregress/function-calc-test/linear/linear_test.bin... 
Successfully loaded /home/grierjones/qregress/function-calc-test/linear/linear_test.bin into X and y data. 
Training model with dataset /home/grierjones/qregress/function-calc-test/linear/linear_train.bin 
 at time Fri Mar 15 16:21:16 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 16:22:33 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 16:25:17 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 16:27:44 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 16:30:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 16:33:26 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 806.3232514858246 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 16:36:17 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 16:39:27 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 16:41:45 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 16:45:17 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 16:48:42 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 916.2719948291779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 16:51:33 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 16:54:43 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 16:56:56 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 16:59:51 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 17:02:46 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 845.3754255771637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 17:05:39 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 17:08:55 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 17:11:10 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 17:14:04 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 17:16:56 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 847.9811015129089 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 17:19:48 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 17:22:59 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 17:25:16 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 17:28:09 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 17:30:45 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 821.9308803081512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 17:33:18 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 17:36:08 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 17:38:06 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 17:40:39 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 17:43:11 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 742.9513945579529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 17:45:32 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 15 17:48:02 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 17:49:49 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 17:52:07 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 17:54:25 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 669.5752251148224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 17:56:42 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 17:59:15 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 18:01:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 18:03:19 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 18:05:36 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.6206834316254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 18:07:54 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 18:10:26 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 18:12:13 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 18:14:30 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 18:16:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.0920221805573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 18:19:05 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 18:21:39 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.6 fold... 
[Fri Mar 15 18:23:26 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 18:25:43 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 18:28:00 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.5964171886444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 18:30:17 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 18:32:50 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 18:34:37 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 18:36:54 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 18:39:11 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.43346118927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 18:41:29 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 18:44:02 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 18:45:49 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 18:48:07 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 18:50:25 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 673.0049965381622 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 18:52:42 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 18:55:15 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 18:57:01 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.8 fold... 
[Fri Mar 15 18:59:18 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 19:01:36 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.7857716083527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 19:03:54 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 19:06:27 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 19:08:14 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 19:10:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 19:12:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.5116565227509 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 19:15:05 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 19:17:38 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 19:19:24 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 19:21:40 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 19:23:56 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 667.100968837738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 19:26:12 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 19:28:42 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 19:30:28 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 19:32:44 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 1.0 fold... 
[Fri Mar 15 19:35:00 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 664.1376006603241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 19:37:16 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 19:39:49 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 19:41:36 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 19:43:53 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 19:46:11 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.4967498779297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 19:48:28 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 19:51:01 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 19:52:48 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 19:55:05 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 19:57:23 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.5273413658142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 19:59:41 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 20:02:13 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 20:04:00 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 20:06:17 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 20:08:34 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Training complete taking 670.7832894325256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 20:10:51 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 20:13:24 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 20:15:10 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 20:17:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 20:19:45 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.6505196094513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Mar 15 20:22:03 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 20:24:36 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 20:26:22 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 20:28:40 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 20:30:58 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.1228973865509 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Mar 15 20:33:15 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 20:35:48 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 20:37:34 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 20:39:51 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 20:42:09 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.4674694538116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Mar 15 20:44:27 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. /home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/grierjones/qregress/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,

Working on 0.4 fold... 
[Fri Mar 15 20:47:00 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 20:48:47 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 20:51:04 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 20:53:21 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 671.4099702835083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Mar 15 20:55:38 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 20:58:12 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 21:00:00 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 21:02:18 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 21:04:35 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 673.6410624980927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Mar 15 21:06:51 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Mar 15 21:09:23 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Mar 15 21:11:10 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Mar 15 21:13:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Mar 15 21:15:47 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 672.940450668335 seconds. 
Discarding model... 

Training complete taking 17732.732917785645 total seconds. 
Now scoring model... 
Scoring complete taking 0.8659460544586182 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274915722,), 'R2_train': -0.0025489989166755933, 'MAE_train': 12.57727151830348, 'MSE_test': 191.37228833007708, 'R2_test': -0.15355654159227172, 'MAE_test': 12.013220427830294}. 
Saved model results as A1-A1-CZ_Full-CRZ_results.json. 
