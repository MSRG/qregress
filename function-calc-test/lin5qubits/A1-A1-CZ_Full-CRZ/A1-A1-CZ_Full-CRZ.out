/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:14 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:17 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:42:30 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:08 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:27 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:51 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2152.1517724990845 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:17 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:09 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:39 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 18:30:48 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 18:38:04 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2115.9714336395264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:14 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:29 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 19:06:29 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:57 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2150.602094888687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:18 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:06 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:52 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 19:42:06 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 19:49:36 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2143.9340953826904 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:13 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:26 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:00 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:57 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2184.67285823822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:29 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:41:46 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:38 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:45 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:18 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2161.6180996894836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:13 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:03 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:33 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:41 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2073.3578145504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:43:46 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:51:28 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:44 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 22:03:39 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:20 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2020.3753900527954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:42 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:26:07 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:31:39 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 22:38:41 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:20 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2081.9956929683685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:52:10 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:56 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:05:14 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 23:12:00 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2008.843254327774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:25:29 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:06 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:38:10 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Sun Mar 24 23:44:53 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:40 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1976.8562550544739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:58:36 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:06:34 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:42 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:48 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 00:28:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2245.7853367328644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:36:28 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:44:39 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:50:30 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:42 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:52 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2161.553959131241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:12:08 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:32 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:36 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:14 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 01:37:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1954.8906247615814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:44:27 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:51:46 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:56:55 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 02:04:16 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 02:11:56 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2052.9615473747253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:18:39 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:26:04 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:09 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 02:37:48 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 02:44:27 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1941.5734405517578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:50:59 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:58:09 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:03:13 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 03:10:00 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 03:16:35 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1950.1943657398224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:23:25 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:30:40 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:35:47 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 03:42:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 03:49:01 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1923.6281504631042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:55:32 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:03:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:08:07 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 04:14:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 04:21:07 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1930.1158928871155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:27:41 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:35:01 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:40:09 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 04:46:46 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 04:53:25 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1934.68541264534 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:00:01 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:07:28 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:12:41 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 05:19:22 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 05:26:00 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1957.0963768959045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:32:39 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:39:58 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:45:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 05:51:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 05:58:12 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1934.3466613292694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:04:49 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:12:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:17:05 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 06:23:47 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 06:30:31 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1938.901067018509 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:37:09 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:44:40 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:50:41 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 06:57:41 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 07:04:46 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2083.3802177906036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:12:33 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:20:22 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:25:56 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Mon Mar 25 07:33:18 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Mon Mar 25 07:40:23 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2105.477215528488 seconds. 
Discarding model... 

Training complete taking 51184.969759464264 total seconds. 
Now scoring model... 
Scoring complete taking 2.4102494716644287 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274915722,), 'R2_train': -0.0025489989166755933, 'MAE_train': 12.57727151830348, 'MSE_test': 191.37228833007708, 'R2_test': -0.15355654159227172, 'MAE_test': 12.013220427830294}. 
Saved model results as A1-A1-CZ_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:50 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:03 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:54:57 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:00:30 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 12:07:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 12:14:49 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2126.9128704071045 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:37 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:59 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:58 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 12:44:29 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 12:51:42 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2201.744238138199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:01 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:06:48 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:15 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:05 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:50 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2034.0456504821777 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:32:48 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:40:22 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:45:48 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 13:52:34 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:03 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1986.1670079231262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:39 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:59 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:03 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:43 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:22 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1936.9824621677399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:38:01 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:45:21 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:29 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:13 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:58 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1963.702885389328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:10:38 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:18:01 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:23:20 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 15:29:54 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 15:36:32 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1946.125562429428 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:43:04 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:50:23 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:55:29 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 16:02:07 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 16:08:41 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1929.2125210762024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:15:17 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:22:38 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:42 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:18 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 16:40:58 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1943.9738655090332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:47:42 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:55:19 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:00:23 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 17:06:59 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:34 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1951.559517621994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:20:13 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:27:40 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:32:49 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 17:39:27 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 17:46:11 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1954.1633212566376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:52:51 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:12 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:05:23 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 18:12:10 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 18:18:45 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1957.5868039131165 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:25:24 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:32:47 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:38:02 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 18:44:31 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 18:51:12 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1944.9912264347076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:57:46 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:05:20 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:10:26 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 19:17:03 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 19:23:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1954.9800176620483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:30:32 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:38:18 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:43:41 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 19:50:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 19:57:22 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2035.4876148700714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:04:28 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:12:28 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:18:36 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 20:26:23 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 20:33:39 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2174.420047044754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:41:10 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:49:41 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:55:51 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 21:03:35 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 21:11:47 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2299.938984155655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:19:27 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:27:19 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:32:44 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 21:39:38 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 21:47:01 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2109.2345798015594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:54:30 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:03:05 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:09:20 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 22:17:28 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 22:25:17 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2281.4497265815735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:32:04 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:39:41 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:45:14 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 22:52:03 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 22:58:48 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2008.6294813156128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:05:41 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:13:18 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:18:34 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:20 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Thu Apr  4 23:32:22 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2018.565004825592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:39:24 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:47:02 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:52:26 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:22 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Apr  5 00:06:13 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2018.2004759311676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:12:51 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:20:16 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:25:34 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Apr  5 00:32:33 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Apr  5 00:39:31 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 2010.032880306244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:46:29 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:54:12 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:59:27 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Apr  5 01:06:12 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Apr  5 01:12:46 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1982.5454227924347 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:19:16 2024]  Iteration number: 0 with current cost as 0.37003872473955834 and parameters 
[-1.82176682  2.23743458 -2.12427964 -0.11653113  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551993 -1.06648319  0.6027151   1.1443244
  1.31029888 -1.8735468   0.72965075  2.88578409 -0.54534346 -0.47522485
 -2.02654251  0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:26:36 2024]  Iteration number: 0 with current cost as 0.31229121067364896 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653108  0.55388708 -2.77010897
  3.06858488  2.18960145  1.18551998 -1.06648308  0.60271515  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.5453433  -0.4752248
 -2.0265424   0.72897374  1.60512664  2.83077102 -1.2645671  -0.251361
 -2.39279213 -2.27309774  3.1333716   2.54856958 -0.67550782 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:31:47 2024]  Iteration number: 0 with current cost as 0.3315072310266021 and parameters 
[-1.43602958  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551993 -1.0664832   0.60271504  1.14432439
  1.31029887 -1.8735468   0.72965075  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856953 -0.67550787 -2.69002207]. 
Working on 0.8 fold... 
[Fri Apr  5 01:38:27 2024]  Iteration number: 0 with current cost as 0.33672964341152595 and parameters 
[-1.41170511  2.23743458 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858486  2.18960139  1.18551998 -1.06648308  0.6027151   1.14432439
  1.31029893 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654252  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337149  2.54856958 -0.67550787 -2.69002208]. 
Working on 1.0 fold... 
[Fri Apr  5 01:45:05 2024]  Iteration number: 0 with current cost as 0.3397185712456695 and parameters 
[-1.40907062  2.23743452 -2.12427964 -0.11653114  0.55388696 -2.77010909
  3.06858487  2.18960134  1.18551987 -1.0664832   0.60271499  1.14432433
  1.31029887 -1.8735468   0.72965069  2.88578408 -0.54534335 -0.47522485
 -2.02654252  0.72897364  1.60512658  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856952 -0.67550787 -2.69002208]. 
Training complete taking 1940.6951496601105 seconds. 
Discarding model... 

Training complete taking 50711.34808564186 total seconds. 
Now scoring model... 
Scoring complete taking 2.824315309524536 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.93879274915722,), 'R2_train': -0.0025489989166755933, 'MAE_train': 12.57727151830348, 'MSE_test': 191.37228833007708, 'R2_test': -0.15355654159227172, 'MAE_test': 12.013220427830294}. 
Saved model results as A1-A1-CZ_Full-CRZ_results.json. 
