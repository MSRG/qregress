/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:46 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:54 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:35 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:41 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1860.1815629005432 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:03:53 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:04 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:59 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:11 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:07 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1901.9917254447937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:26 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:15 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:03 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:11 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:02 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1833.0479667186737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:54 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:29 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:01 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:23:30 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:51 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1864.964107990265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:37:17 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:44:39 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:42 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:55 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:26 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1973.0241944789886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:06 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:17 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:39 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:42 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:18 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1876.4370913505554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:06 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:47:28 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:56 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:36 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:53 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1874.7348518371582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:29 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:47 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:26:11 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:04 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:26 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1793.5620374679565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:42:14 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:35 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:57 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:49 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:05:15 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1728.1576194763184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:11:09 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:35 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:54 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:43 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:11 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1736.3832230567932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:39:57 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:46:20 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:52:45 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:36 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:03:07 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1735.6810581684113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:08:53 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:21 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:21:45 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:31 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:32:04 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1736.5145173072815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:37:51 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:44:47 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:51:39 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:55:29 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:01:47 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1780.6144580841064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:07:44 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:14:56 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:22:12 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:26:28 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:33:47 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1934.4800708293915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:39:47 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:46:13 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:34 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:56:21 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:02:46 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1724.3081352710724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:08:26 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:14:47 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:21:05 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:24:53 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:31:14 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1707.6125011444092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:36:55 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:43:17 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:49:40 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:53:29 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:59:47 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1716.4728138446808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:30 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:12:01 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:18:21 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:22:06 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:28:35 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1764.8597960472107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:35:19 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:42:30 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:49:47 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:54:06 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:00:51 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1896.7255866527557 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:06:34 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:13:00 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:19:42 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:24:04 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:30:55 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1803.7308695316315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:36:38 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:43:05 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:49:24 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:53:18 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:59:43 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1735.854841709137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:42 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:12:15 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:18:51 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:56 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:29:43 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1807.731241941452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:35:43 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:42:20 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:49:06 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:53:11 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:01:06 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1906.7342247962952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:08:04 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:15:00 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:22:09 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:26:05 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:37 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1856.2870872020721 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:38:26 2024]  Iteration number: 0 with current cost as 0.3801133962442851 and parameters 
[-1.86063584  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648308  0.60271513  1.14432448
  1.31029899 -1.87354675  0.72965083  2.88578422 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:45:30 2024]  Iteration number: 0 with current cost as 0.34021318343595736 and parameters 
[-1.73750488  2.23743464 -2.12427959 -0.11653093  0.55388708 -2.77010897
  3.06858498  2.18960155  1.18552009 -1.06648308  0.6027151   1.1443245
  1.31029899 -1.87354675  0.72965085  2.88578424 -0.54534335 -0.4752248
 -2.0265424   0.7289737   1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:53:06 2024]  Iteration number: 0 with current cost as 0.3214981746074217 and parameters 
[-1.59867869  2.23743467 -2.12427954 -0.11653094  0.55388714 -2.77010894
  3.06858501  2.18960151  1.18552011 -1.06648308  0.60271516  1.14432448
  1.31029902 -1.87354674  0.7296508   2.88578419 -0.54534329 -0.47522479
 -2.02654244  0.7289737   1.6051267   2.83077104 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:57:00 2024]  Iteration number: 0 with current cost as 0.3568027274758451 and parameters 
[-1.80309343  2.23743458 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858493  2.1896015   1.18552004 -1.06648314  0.60271513  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.02654246  0.72897367  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856956 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:03:54 2024]  Iteration number: 0 with current cost as 0.35714829943321796 and parameters 
[-1.79851553  2.23743458 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.1896015   1.18552004 -1.06648311  0.6027151   1.14432448
  1.31029901 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.4752248
 -2.02654246  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1878.8970172405243 seconds. 
Discarding model... 

Training complete taking 45428.99184179306 total seconds. 
Now scoring model... 
Scoring complete taking 2.2319483757019043 seconds. 
Saved predicted values as A1_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.69854454792807,), 'R2_train': -0.001385077012841185, 'MAE_train': 12.570106826782041, 'MSE_test': 193.76226291247175, 'R2_test': -0.16796286362467305, 'MAE_test': 12.0652804593115}. 
Saved model results as A1_Full-CRZ_results.json. 
