/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:16 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:18 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:19 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:20 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:22 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 307.6757526397705 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:23 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:23 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:26 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:28 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 311.3866913318634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:34 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:35 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:34 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:36 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:40 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 306.3723373413086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:40 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:40 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:43 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:42 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:42 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 303.0949184894562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:43 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:46 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:46 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:47 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:47 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 303.96282029151917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:48 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:51 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:53 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:54 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:55 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 308.1325144767761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:56 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:55 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:07:56 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:56 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:57 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 301.5490860939026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:58 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:59 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:03 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:04 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:15:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 309.70178627967834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:06 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:07 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:06 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:06 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:20:05 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 299.4780464172363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:06 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:06 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:06 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 299.4145815372467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:05 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:28:05 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:08 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 304.99219489097595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:11 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:14 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:33:14 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:14 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:35:16 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 306.35345244407654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:17 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:21 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:22 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:24 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 308.82243752479553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:27 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:29 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:35 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:36 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:39 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 314.86921095848083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:40 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:42 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:45 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:49 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:52 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 312.4578149318695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:54 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:56 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:58 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:01 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:04 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 313.37516927719116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:57:07 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:10 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:12 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:16 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:19 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 314.42074251174927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:21 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:22 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:24 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:05:28 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:32 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 313.50697588920593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:35 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:08:37 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:39 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:44 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:49 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 316.4757068157196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:52 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:54 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:56 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:58 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:01 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 316.60505056381226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:08 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:12 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:17 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:22 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:27 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 324.33166694641113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:34 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:39 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:25:43 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:26:48 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:55 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 326.80300211906433 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:00 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:30:03 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:08 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:14 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:19 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 325.74585247039795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:25 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:28 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:35 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:47 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:59 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 336.8454303741455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:03 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:06 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:10 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:13 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:17 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 321.82405042648315 seconds. 
Discarding model... 

Training complete taking 7808.199333429337 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1671569347381592 seconds. 
Saved predicted values as M-M-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (79.39266763449696,), 'R2_train': 0.6153691707539284, 'MAE_train': 5.876818643951188, 'MSE_test': 92.18149258458158, 'R2_test': 0.4443471167503872, 'MAE_test': 6.031775482207715}. 
Saved model results as M-M-CZ_HWE-CZ_results.json. 
