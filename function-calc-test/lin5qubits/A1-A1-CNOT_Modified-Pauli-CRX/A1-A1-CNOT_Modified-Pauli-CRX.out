/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:24 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:50 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:32 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:25 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1200.5942075252533 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:26 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:53 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:39 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:54 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1227.6156222820282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:24 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:52 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:38 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:41 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1268.480786561966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:59 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:58 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:26 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:16 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:08 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1220.2962243556976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:20 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:19 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:00:46 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:27 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:17 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1196.550491809845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:17 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:17 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:46 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:30 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:21 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1203.1506881713867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:20 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:22 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:40:49 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:33 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:48:20 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1206.1328194141388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:52:26 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:10 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:50 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:44 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1215.2689325809479 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:44 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:16:42 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:07 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:24:51 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:42 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1205.1569929122925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:46 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:00 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:24 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:45:02 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:48:52 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1205.2738060951233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:52 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:54 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:25 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:04 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:54 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1199.9225554466248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:52 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:50 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:19 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:02 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:28:50 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1194.945612668991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:47 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:36:44 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:33 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:14 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:12 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1226.1376259326935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:11 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:57:22 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:01:53 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:50 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:09:41 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1226.4749460220337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:13:39 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:36 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:58 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:58 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1216.367778301239 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:57 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:01 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:29 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:29 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:27 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1261.0238728523254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:03 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:06 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:03:31 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:10 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:10:59 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1199.9334962368011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:56 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:18:54 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:27 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:06 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:57 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1201.0770363807678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:57 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:55 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:18 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:58 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:03 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1204.036770582199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:02 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:58:58 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:29 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:11 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:11:01 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1198.1943175792694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:00 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:18:57 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:23:19 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:26:59 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:30:55 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1193.1411957740784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:34:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:49 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:28 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:47:10 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:50:59 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1209.078406572342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:02 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:01 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:24 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:05 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:10:53 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1188.9433917999268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:18:49 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:14 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:26:54 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:30:53 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1198.1616559028625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:34:49 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:46 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:14 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:46:54 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:50:46 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1195.0177586078644 seconds. 
Discarding model... 

Training complete taking 30260.97910952568 total seconds. 
Now scoring model... 
Scoring complete taking 1.0172748565673828 seconds. 
Saved predicted values as A1-A1-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (141.15084360785573,), 'R2_train': 0.316171535189956, 'MAE_train': 10.842655335701696, 'MSE_test': 135.63034493209386, 'R2_test': 0.18244552019476767, 'MAE_test': 10.275606121983028}. 
Saved model results as A1-A1-CNOT_Modified-Pauli-CRX_results.json. 
