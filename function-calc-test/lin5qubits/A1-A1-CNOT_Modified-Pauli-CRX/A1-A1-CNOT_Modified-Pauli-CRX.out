/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:24 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:50 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:32 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:25 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1200.5942075252533 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:26 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:53 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:39 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:54 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1227.6156222820282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:24 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:52 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:38 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:41 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1268.480786561966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:59 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:58 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:26 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:16 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:08 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1220.2962243556976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:20 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:19 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:00:46 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:27 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:17 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1196.550491809845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:17 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:17 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:46 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:30 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:21 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1203.1506881713867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:20 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:22 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:40:49 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:33 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:48:20 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1206.1328194141388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:52:26 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:10 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:50 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:44 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1215.2689325809479 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:44 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:16:42 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:07 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:24:51 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:42 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1205.1569929122925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:46 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:00 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:24 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:45:02 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:48:52 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1205.2738060951233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:52 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:54 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:01:25 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:04 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:54 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1199.9225554466248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:52 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:50 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:19 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:25:02 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:28:50 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1194.945612668991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:47 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:36:44 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:33 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:14 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:12 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1226.1376259326935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:11 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:57:22 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:01:53 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:50 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:09:41 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1226.4749460220337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:13:39 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:36 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:58 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:58 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1216.367778301239 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:57 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:38:01 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:29 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:29 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:27 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1261.0238728523254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:03 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:06 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:03:31 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:10 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:10:59 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1199.9334962368011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:56 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:18:54 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:27 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:06 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:57 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1201.0770363807678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:57 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:55 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:18 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:58 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:03 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1204.036770582199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:02 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:58:58 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:29 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:11 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:11:01 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1198.1943175792694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:00 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:18:57 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:23:19 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:26:59 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:30:55 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1193.1411957740784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:34:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:49 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:43:28 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:47:10 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:50:59 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1209.078406572342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:02 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:59:01 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:24 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:05 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:10:53 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1188.9433917999268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:51 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:18:49 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:14 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:26:54 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:30:53 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1198.1616559028625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:34:49 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:46 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:43:14 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:46:54 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:50:46 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1195.0177586078644 seconds. 
Discarding model... 

Training complete taking 30260.97910952568 total seconds. 
Now scoring model... 
Scoring complete taking 1.0172748565673828 seconds. 
Saved predicted values as A1-A1-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (141.15084360785573,), 'R2_train': 0.316171535189956, 'MAE_train': 10.842655335701696, 'MSE_test': 135.63034493209386, 'R2_test': 0.18244552019476767, 'MAE_test': 10.275606121983028}. 
Saved model results as A1-A1-CNOT_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:43 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:10 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:33:09 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:37:33 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:41:13 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:45:03 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1201.3135991096497 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:49:11 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:53:16 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:57:43 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:01:23 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:16 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1203.2960495948792 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:09:14 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:14 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:17:42 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:21:22 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:22 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1206.7108700275421 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:21 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:33:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:52 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:50 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:46:08 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1247.580137014389 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:08 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:54:31 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:08 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:49 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:06:40 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1233.2346949577332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:42 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:42 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:19:08 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:49 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:26:39 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1205.5254333019257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:47 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:15 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:46 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:43:48 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:38 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1248.3472969532013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:51:35 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:55:40 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:01 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:03:44 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:07:32 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1198.378398656845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:11:32 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:30 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:57 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:23:53 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:27:43 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1209.253791809082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:31:43 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:35:43 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:40:07 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:43:46 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:47:34 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1193.7146015167236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:51:36 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:55:34 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:00:00 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:03:42 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:07:30 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1192.5614578723907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:11:29 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:15:28 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:20:05 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:23:47 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:27:53 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1220.243320465088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:31:49 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:35:48 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:40:13 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:43:54 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:47:44 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1192.9016931056976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:51:43 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:55:42 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:00:07 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:03:49 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:07:46 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1213.2627849578857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:11:56 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:15:54 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:20:18 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:23:58 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:27:49 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1191.9611797332764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:31:48 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:35:47 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:40:11 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:43:52 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:47:40 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1192.1467213630676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:51:39 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:55:38 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:00:02 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:03:42 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:07:32 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1189.4753963947296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:11:29 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:15:28 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:19:57 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:23:38 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:27:28 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1197.8166670799255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:31:27 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:27 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:39:52 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:43:33 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:47:23 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1194.2096099853516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:51:21 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:55:19 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:59:50 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:03:30 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:07:22 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1200.4457581043243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:11:22 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:15:25 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:20:02 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:23:46 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:27:37 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1227.6288599967957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:31:48 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:35:57 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:40:47 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:44:43 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:48:35 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1275.3076593875885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:53:05 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:57:11 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:01:36 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:05:17 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:09:10 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1203.922336101532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:13:09 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:17:07 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:32 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:25:13 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:29:02 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1190.3939490318298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:32:59 2024]  Iteration number: 0 with current cost as 0.42077059383619764 and parameters 
[-3.13068052  1.76497685 -2.33071265 -0.11653103  0.55388704 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648308  1.77426429  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:36:56 2024]  Iteration number: 0 with current cost as 0.32273442527540636 and parameters 
[-3.17181257  1.62372212 -2.35276816 -0.11653106  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551995 -1.06648316  2.22034504  1.14432445
  1.31029899 -1.8735468   0.72965073  2.88578416 -0.54534331 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:41:19 2024]  Iteration number: 0 with current cost as 0.34567405544082425 and parameters 
[-3.15941599  1.69751475 -2.34528949 -0.11653103  0.55388704 -2.77010905
  3.06858491  2.18960141  1.18551998 -1.06648312  2.01990166  1.14432445
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:45:05 2024]  Iteration number: 0 with current cost as 0.34791311164269956 and parameters 
[-3.15624354  1.7006583  -2.3438564  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  2.00393328  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578416 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:49:00 2024]  Iteration number: 0 with current cost as 0.34607399009197204 and parameters 
[-3.16151192  1.67429289 -2.34662262 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  2.08248564  1.14432445
  1.31029899 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1202.0941610336304 seconds. 
Discarding model... 

Training complete taking 30231.72816491127 total seconds. 
Now scoring model... 
Scoring complete taking 1.0230557918548584 seconds. 
Saved predicted values as A1-A1-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (141.15084360785573,), 'R2_train': 0.316171535189956, 'MAE_train': 10.842655335701696, 'MSE_test': 135.63034493209386, 'R2_test': 0.18244552019476767, 'MAE_test': 10.275606121983028}. 
Saved model results as A1-A1-CNOT_Modified-Pauli-CRX_results.json. 
