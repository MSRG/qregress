/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:31 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:59 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:04 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:47 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:27 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2006.8957755565643 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:14 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:33 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:42 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:24 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:18 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2019.892541885376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:37 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:00 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:08 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:36 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:18 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1985.6543140411377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:44 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:35 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:36 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:04 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:37:39 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1992.06973195076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:56 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:50:32 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:46 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:37 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2037.5621500015259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:54 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:24:13 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:26 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:56 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:36 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1980.5425162315369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:54 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:09 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:39 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:19 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1962.686357975006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:23:37 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:57 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:36:54 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:22 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:01 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1965.3077201843262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:22 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:09:41 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:17 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:22:55 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1971.0373289585114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:29:13 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:43:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:49:52 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:41 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2027.5529327392578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:01 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:39 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:17:49 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:24:43 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:30 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2084.568196296692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:37:45 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:44:11 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:51:18 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:57:54 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:46 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2021.7202644348145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:11:26 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:17:54 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:24:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:24 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:38:04 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1977.741039276123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:24 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:50:49 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:58:13 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:04:43 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:24 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1998.5053911209106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:17:44 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:03 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:01 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:37:30 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:44:12 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1967.1842639446259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:50:29 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:56:59 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:04:09 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:10:38 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:17:15 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1987.2583663463593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:23:38 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:00 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:36:59 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:44:00 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:36 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1995.2621531486511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:56:53 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:03:12 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:10:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:16:38 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:23 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1969.3500843048096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:29:42 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:36:05 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:43:18 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:49:54 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:56:39 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1997.5361399650574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:03:00 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:49 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:55 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:23:30 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:30:29 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2041.1563289165497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:37:01 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:43:54 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:50:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:57:20 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:59 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.8928644657135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:10:18 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:16:36 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:23:36 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:30:02 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:36:40 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1960.1310865879059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:42:59 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:49:17 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:56:17 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:03:08 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:09:44 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1983.7178013324738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:16:02 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:22:20 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:29:32 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:36:03 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:43:00 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.3503921031952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:49:17 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:55:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:02:44 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:09:22 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:16:04 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.1799125671387 seconds. 
Discarding model... 

Training complete taking 49922.75708150864 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9876048564910889 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (90.5804432966594,), 'R2_train': 0.5611681524663557, 'MAE_train': 8.413097733019578, 'MSE_test': 139.60058302946018, 'R2_test': 0.1585136637652913, 'MAE_test': 9.467664189873734}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRZ_results.json. 
