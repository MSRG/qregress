/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:31 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:59 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:04 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:47 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:27 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2006.8957755565643 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:14 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:33 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:42 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:24 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:18 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2019.892541885376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:37 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:00 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:08 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:36 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:18 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1985.6543140411377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:44 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:35 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:36 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:04 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:37:39 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1992.06973195076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:56 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:50:32 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:46 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:37 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2037.5621500015259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:54 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:24:13 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:26 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:56 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:36 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1980.5425162315369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:54 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:09 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:39 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:19 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1962.686357975006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:23:37 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:57 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:36:54 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:22 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:01 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1965.3077201843262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:22 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:09:41 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:17 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:22:55 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1971.0373289585114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:29:13 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:35:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:43:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:49:52 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:41 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2027.5529327392578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:01 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:39 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:17:49 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:24:43 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:30 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2084.568196296692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:37:45 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:44:11 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:51:18 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:57:54 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:46 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2021.7202644348145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:11:26 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:17:54 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:24:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:24 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:38:04 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1977.741039276123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:24 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:50:49 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:58:13 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:04:43 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:24 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1998.5053911209106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:17:44 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:03 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:01 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:37:30 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:44:12 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1967.1842639446259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:50:29 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:56:59 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:04:09 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:10:38 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:17:15 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1987.2583663463593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:23:38 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:00 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:36:59 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:44:00 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:36 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1995.2621531486511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:56:53 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:03:12 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:10:10 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:16:38 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:23 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1969.3500843048096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:29:42 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:36:05 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:43:18 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:49:54 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:56:39 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1997.5361399650574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:03:00 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:49 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:55 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:23:30 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:30:29 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2041.1563289165497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:37:01 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:43:54 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:50:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:57:20 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:59 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.8928644657135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:10:18 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:16:36 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:23:36 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:30:02 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:36:40 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1960.1310865879059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:42:59 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:49:17 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:56:17 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:03:08 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:09:44 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1983.7178013324738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:16:02 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:22:20 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:29:32 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:36:03 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:43:00 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.3503921031952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:49:17 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:55:42 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:02:44 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:09:22 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:16:04 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.1799125671387 seconds. 
Discarding model... 

Training complete taking 49922.75708150864 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9876048564910889 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (90.5804432966594,), 'R2_train': 0.5611681524663557, 'MAE_train': 8.413097733019578, 'MSE_test': 139.60058302946018, 'R2_test': 0.1585136637652913, 'MAE_test': 9.467664189873734}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:37:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:32 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 11:43:41 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:50:41 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:57:07 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:03:35 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1931.4118468761444 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:09:42 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:15:52 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:22:42 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:29:01 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:35:32 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1918.4259464740753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:41:41 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:08 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:55:09 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:28 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:08:07 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1957.0325376987457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:14:18 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:21 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:27:05 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:31 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:40:01 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1907.6831560134888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:46:07 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:52:12 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:58:58 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:05:18 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:11:41 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1904.0660741329193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:49 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:24:07 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:30:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:18 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:43:44 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1929.1198945045471 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:50:00 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:56:04 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:53 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:10 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:15:35 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1902.026798248291 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:21:41 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:27:44 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:34:44 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:41:10 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:47:34 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1919.3704917430878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:53:40 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:59:59 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:06:43 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:13:00 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:19:29 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1932.70925116539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:25:53 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:32:07 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:38:51 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:45:07 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:51:29 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1903.4612121582031 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:57:36 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:04:12 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:10:58 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:17:20 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:24:05 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1956.083301782608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:30:15 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:36:21 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:43:06 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:49:24 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:55:49 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1904.4769685268402 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:01:57 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:08:02 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:15:15 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:21:43 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:28:07 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1941.495709657669 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:34:19 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:40:31 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:47:14 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:53:30 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:00:00 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1907.0694134235382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:06:05 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:12:15 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:18:57 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:25:29 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:32:02 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1922.4925768375397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:38:08 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:44:13 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:50:58 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:57:11 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:03:35 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1891.852288722992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:09:40 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:15:46 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:22:31 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:28:47 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:35:12 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1894.9688820838928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:41:15 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:47:25 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:54:12 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:00:47 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:07:12 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1921.6377272605896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:13:16 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:19:19 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:26:05 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:32:22 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:38:47 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1896.95334482193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:44:53 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:56 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:57:39 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:03:56 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:10:28 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1897.6970810890198 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:16:32 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:22:37 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:29:19 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:35:48 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:42:12 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1914.0808155536652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:48:26 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:54:35 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:01:23 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:07:59 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:23 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1924.258800983429 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:20:33 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:26:53 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:34:58 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:41:15 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:47:43 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1996.696784734726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:53:47 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:00:04 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:06:51 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:13:07 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:19:34 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1958.1975648403168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:26:24 2024]  Iteration number: 0 with current cost as 0.3738035414459726 and parameters 
[-3.27143789  2.41516913 -2.10474687 -0.11653103  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.62801992  1.14432446
  1.72535265 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:32:30 2024]  Iteration number: 0 with current cost as 0.3355063714265608 and parameters 
[-3.32853687  2.33422177 -2.11558138 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648309  0.61983996  1.14432446
  1.80239515 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:39:27 2024]  Iteration number: 0 with current cost as 0.33363454059488185 and parameters 
[-3.3175665   2.38873094 -2.10920057 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6260053   1.14432445
  1.78186148 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:45:44 2024]  Iteration number: 0 with current cost as 0.33973943621583635 and parameters 
[-3.32576269  2.37640361 -2.10832613 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62028117  1.14432445
  1.79199476 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:52:08 2024]  Iteration number: 0 with current cost as 0.35238699151073094 and parameters 
[-3.31407359  2.3621216  -2.10974495 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.6211415   1.14432444
  1.78183587 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 1911.9123990535736 seconds. 
Discarding model... 

Training complete taking 48045.182035684586 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0666344165802002 seconds. 
Saved predicted values as M-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (90.5804432966594,), 'R2_train': 0.5611681524663557, 'MAE_train': 8.413097733019578, 'MSE_test': 139.60058302946018, 'R2_test': 0.1585136637652913, 'MAE_test': 9.467664189873734}. 
Saved model results as M-A2-CNOT_Full-Pauli-CRZ_results.json. 
