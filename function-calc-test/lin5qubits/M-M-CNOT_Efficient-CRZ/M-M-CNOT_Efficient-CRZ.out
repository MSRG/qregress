/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:45 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:27 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:50 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:31 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:06 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1232.5687141418457 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:17 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:58:59 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:16 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:59 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:36 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1223.0309088230133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:41 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:19 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:37 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:12 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:47 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1210.6437723636627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:53 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:34 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:51 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:26 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:51:04 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1219.0467188358307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:08 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:39 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:04 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:40 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:24 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1223.013469696045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:36 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:18 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:36 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:28:15 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:54 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1226.3566589355469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:53 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:21 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:31 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:02 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:30 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1171.1483659744263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:24 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:04:02 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:34 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:06 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1174.5153057575226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:57 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:28 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:23:41 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:27:18 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:47 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1181.891782283783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:42 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:15 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:43:30 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:58 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:28 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1176.7147843837738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:55:17 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:58:39 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:02:59 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:31 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:59 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1170.4637038707733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:47 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:17 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:40 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:11 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:40 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1184.0426025390625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:34:29 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:01 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:16 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:44 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:11 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1171.7111775875092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:54:10 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:57:40 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:01:54 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:05:19 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:40 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1163.5020639896393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:13:25 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:16:56 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:14 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:45 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:21 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1183.4437568187714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:08 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:44 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:40:54 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:44:25 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:48:08 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1198.0758304595947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:33 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:40 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:04:05 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:07:35 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1166.7255260944366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:12:36 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:16:16 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:20:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:23:58 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:27:21 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1174.7936627864838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:32:19 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:35:43 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:03 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:43:41 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:11 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1197.5688247680664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:17 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:54 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:00:08 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:37 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:07:19 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1205.4370753765106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:12:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:15:52 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:20:05 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:23:45 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:27:11 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1191.916701555252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:32:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:35:37 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:40:01 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:43:35 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:07 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1197.0593032836914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:56:01 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:00:20 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:03:53 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:07:29 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1222.4729173183441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:12:28 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:16:02 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:20:21 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:23:54 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:27:23 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1205.748962879181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:32 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743337 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:36:08 2024]  Iteration number: 0 with current cost as 0.44562349757193553 and parameters 
[-0.15387672  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010915
  3.06858498  2.18960163  1.18552016 -1.06648326  0.6027151   1.14432445
  1.31029916 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77011024
  3.06858372  2.18960272  1.18552125 -1.06648435  0.60271637  1.14432572
  1.31030025 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:44:11 2024]  Iteration number: 0 with current cost as 0.21347934944694782 and parameters 
[ 1.1030676   2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.60271542  1.14432476
  1.3102993  -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:47:39 2024]  Iteration number: 0 with current cost as 0.44383582544209577 and parameters 
[ 0.08121583  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960187  1.1855202  -1.06648308  0.60271531  1.14432466
  1.3102992  -1.8735468 ]. 
Training complete taking 1218.5489683151245 seconds. 
Discarding model... 

Training complete taking 29890.44270825386 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.25319242477417 seconds. 
Saved predicted values as M-M-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.55034073327465,), 'R2_train': 0.3287700988471408, 'MAE_train': 10.909980984564196, 'MSE_test': 127.74699652595675, 'R2_test': 0.2299648773749745, 'MAE_test': 9.746227585200575}. 
Saved model results as M-M-CNOT_Efficient-CRZ_results.json. 
