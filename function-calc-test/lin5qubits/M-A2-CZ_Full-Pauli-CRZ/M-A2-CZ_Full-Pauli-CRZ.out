/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:15 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:40:54 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:17 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:41 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:03:39 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:27 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:15:17 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3268.668098926544 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:44 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:35:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:33 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:00 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:57:50 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:38 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:22 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3242.52933549881 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:46 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:29:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:31 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:52 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:51:38 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:26 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:27 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3252.4577491283417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:03 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:23:20 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:44 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:06 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:45:55 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:42 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:28 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3249.397280216217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:07:08 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:17:45 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:09 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:01 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:40:45 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 21:42:31 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:52:15 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3273.943199634552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:42 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:12:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:12:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:49 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:34:30 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 22:36:18 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:46:12 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3242.5904490947723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:45 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:06:15 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:17:03 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:29:16 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:02 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:51 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3312.6344442367554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:50:57 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:01:16 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:00 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:24:09 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:56 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:35:36 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3238.9008984565735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:55:16 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:55:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:05:57 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:17:55 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:41 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:29:33 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3239.402999639511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:38:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:49:17 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:49:41 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:56 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:11:37 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 02:13:25 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:23:20 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3222.901112318039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:32:38 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:42:55 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:43:18 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:53:37 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:05:27 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 03:07:14 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:16:59 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3215.943410873413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:26:15 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:36:33 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:36:57 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:47:16 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:58:57 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 04:00:44 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:10:25 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3211.7839798927307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:19:46 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:30:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:30:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:40:50 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:52:29 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 04:54:16 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:58 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3212.3589498996735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:13:19 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:23:36 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:23:59 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:34:15 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:45:56 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 05:47:44 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:57:38 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3217.420867919922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:06:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:17:10 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:17:33 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:28:13 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:39:57 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 06:41:45 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:51:30 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3233.7737901210785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:00:50 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:11:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:11:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:21:50 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:33:30 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 07:35:17 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:05 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3213.75900888443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:54:24 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:04:43 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:05:06 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:15:24 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:27:02 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 08:28:50 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 08:38:31 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3209.1380891799927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:47:53 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:58:15 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:58:38 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:08:59 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:20:40 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 09:22:28 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:32:13 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3218.265037059784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:41:31 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:51:50 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:52:13 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:02:35 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:14:21 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 10:16:08 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 10:25:50 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3217.9193024635315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 10:35:32 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:46:13 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 10:46:37 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:57:14 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:09:10 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 11:10:58 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 11:20:38 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3288.5348670482635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 11:29:57 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:40:33 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 11:40:58 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:51:26 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:03:08 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 12:04:55 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 12:14:35 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3234.5587754249573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 12:23:51 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:34:08 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 12:34:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 12:44:46 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:56:24 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 12:58:11 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 13:07:50 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3198.7574763298035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 13:17:18 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 13:27:52 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 13:28:16 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 13:38:33 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 13:50:16 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 13:52:03 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 14:01:59 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3253.024684906006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 14:11:24 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 14:22:03 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 14:22:26 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 14:32:42 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 14:44:22 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 14:46:13 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 14:56:01 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3235.1554243564606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 15:05:19 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 15:15:32 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 15:15:56 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 15:26:12 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 15:37:56 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 15:40:00 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 15:49:55 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3237.474187850952 seconds. 
Discarding model... 

Training complete taking 80941.29541993141 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0806646347045898 seconds. 
Saved predicted values as M-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (67.20753708568752,), 'R2_train': 0.6744020387391296, 'MAE_train': 5.416843276611692, 'MSE_test': 50.742992612201945, 'R2_test': 0.6941306832950995, 'MAE_test': 5.393613284147864}. 
Saved model results as M-A2-CZ_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 11:39:28 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:39:52 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 11:50:25 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:02:34 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 12:04:22 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:14:26 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3340.3847539424896 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:24:37 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:35:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:30 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:46:05 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:58:32 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 13:00:21 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 13:10:10 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3297.1844289302826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:33 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:30:00 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:25 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:51 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:52:45 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:58 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:53 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3280.908222913742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:14:15 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:24:52 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:25:16 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:52 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:48:01 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 14:49:49 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:06 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3320.212862253189 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:35 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:20:05 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:20:31 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:26 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:43:30 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 15:45:19 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:55:08 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3310.35254240036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:04:44 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:15:11 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:15:35 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:26:04 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:38:01 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 16:39:50 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 16:49:43 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3264.519719362259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:59:10 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:09:56 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:10:21 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:20:53 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:32:47 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 17:34:37 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:44:29 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3292.154265642166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:54:03 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:04:30 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:04:55 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:15:41 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:27:41 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 18:29:30 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 18:39:35 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3323.4930403232574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:49:25 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:59:58 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:00:21 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:10:51 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:22:52 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 19:24:41 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:34:35 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3276.2698583602905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:44:02 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:54:28 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:54:52 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:05:17 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:17:24 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 20:19:14 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:29:11 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3286.8641595840454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:38:49 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:49:16 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:49:40 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:00:15 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:12:16 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 21:14:07 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 21:23:57 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3282.4337496757507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:33:30 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:44:15 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:44:40 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:55:30 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:07:36 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 22:09:25 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:19:14 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3308.9186680316925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:28:40 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:39:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:39:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:50:20 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:02:23 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 23:04:14 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:06 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3311.9796566963196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:23:53 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:34:19 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:34:42 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:45:11 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:57:03 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Thu Apr  4 23:58:52 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:08:43 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3262.1007585525513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:18:13 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:29:05 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:29:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:40:25 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:52:21 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 00:54:10 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:04:18 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3329.463688850403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:13:43 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:24:22 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:24:45 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:35:22 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:47:17 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 01:49:06 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:58:55 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3280.4942762851715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:08:24 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:18:54 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:19:17 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:29:46 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:41:36 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 02:43:26 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:53:17 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3293.715131044388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:03:18 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:13:46 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:14:10 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:24:41 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:36:44 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 03:38:37 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 03:48:33 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3295.5033650398254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:58:13 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:08:52 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:09:17 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:19:43 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:32:29 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 04:34:18 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:44:07 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3319.7721843719482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:53:34 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:04:10 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:04:34 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:15:10 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:27:09 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 05:28:56 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 05:38:50 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3283.3566336631775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:48:17 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:58:45 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:59:10 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:09:37 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:21:24 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 06:23:15 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 06:33:02 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3250.669848680496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:42:27 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 06:52:52 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 06:53:17 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:03:45 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:15:44 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 07:17:31 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 07:27:23 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3264.9172348976135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:37:01 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 07:47:31 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 07:47:56 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:58:23 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 08:10:12 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 08:12:02 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 08:21:53 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3266.4312267303467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:31:19 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 08:41:45 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 08:42:08 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:52:33 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 09:04:25 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 09:06:22 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 09:16:14 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3271.2180745601654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:25:50 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 09:36:22 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Fri Apr  5 09:36:45 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:47:21 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 09:59:11 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Fri Apr  5 10:01:01 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 10:11:37 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3309.6336574554443 seconds. 
Discarding model... 

Training complete taking 82322.95453238487 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.220874547958374 seconds. 
Saved predicted values as M-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (67.20753708568752,), 'R2_train': 0.6744020387391296, 'MAE_train': 5.416843276611692, 'MSE_test': 50.742992612201945, 'R2_test': 0.6941306832950995, 'MAE_test': 5.393613284147864}. 
Saved model results as M-A2-CZ_Full-Pauli-CRZ_results.json. 
