/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:15 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:40:54 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:17 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:41 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:03:39 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:27 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:15:17 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3268.668098926544 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:44 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:35:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:33 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:00 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:57:50 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:38 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:22 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3242.52933549881 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:46 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:29:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:31 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:52 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:51:38 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:26 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:27 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3252.4577491283417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:03 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:23:20 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:44 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:06 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:45:55 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:42 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:28 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3249.397280216217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:07:08 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:17:45 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:09 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:01 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:40:45 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 21:42:31 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:52:15 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3273.943199634552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:42 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:12:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:12:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:22:49 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:34:30 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 22:36:18 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:46:12 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3242.5904490947723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:45 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:06:15 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:17:03 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:29:16 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:02 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:51 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3312.6344442367554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:50:57 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:01:16 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:12:00 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:24:09 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:56 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:35:36 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3238.9008984565735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:55:16 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:55:39 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:05:57 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:17:55 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:41 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:29:33 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3239.402999639511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:38:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:49:17 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:49:41 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:56 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:11:37 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 02:13:25 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:23:20 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3222.901112318039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:32:38 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:42:55 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:43:18 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:53:37 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:05:27 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 03:07:14 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:16:59 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3215.943410873413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:26:15 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:36:33 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:36:57 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:47:16 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:58:57 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 04:00:44 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:10:25 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3211.7839798927307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:19:46 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:30:07 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:30:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:40:50 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:52:29 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 04:54:16 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:58 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3212.3589498996735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:13:19 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:23:36 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:23:59 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:34:15 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:45:56 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 05:47:44 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:57:38 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3217.420867919922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:06:56 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:17:10 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:17:33 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:28:13 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:39:57 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 06:41:45 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:51:30 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3233.7737901210785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:00:50 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:11:09 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 07:11:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:21:50 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:33:30 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 07:35:17 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:05 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3213.75900888443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:54:24 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:04:43 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:05:06 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:15:24 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:27:02 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 08:28:50 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 08:38:31 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3209.1380891799927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:47:53 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:58:15 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 08:58:38 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:08:59 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:20:40 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 09:22:28 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:32:13 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3218.265037059784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:41:31 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:51:50 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 09:52:13 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:02:35 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:14:21 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 10:16:08 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 10:25:50 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3217.9193024635315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 10:35:32 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:46:13 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 10:46:37 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:57:14 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:09:10 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 11:10:58 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 11:20:38 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3288.5348670482635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 11:29:57 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:40:33 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 11:40:58 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:51:26 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:03:08 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 12:04:55 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 12:14:35 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3234.5587754249573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 12:23:51 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:34:08 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 12:34:32 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 12:44:46 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:56:24 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 12:58:11 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 13:07:50 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3198.7574763298035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 13:17:18 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 13:27:52 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 13:28:16 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 13:38:33 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 13:50:16 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 13:52:03 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 14:01:59 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3253.024684906006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 14:11:24 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 14:22:03 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 14:22:26 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 14:32:42 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 14:44:22 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 14:46:13 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 14:56:01 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3235.1554243564606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 15:05:19 2024]  Iteration number: 0 with current cost as 0.1974594559216218 and parameters 
[-3.26359206  2.31654335 -2.12898401 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.68016566  1.14432446
  1.69711001 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 15:15:32 2024]  Iteration number: 50 with current cost as 0.12532015956038917 and parameters 
[-1.49225178  2.31685845 -1.41132822 -0.11652985  0.55388602 -2.77010846
  3.06858423  2.18960328  1.18552009 -1.06648328  3.3861081   1.14432424
  2.45544637 -1.87354657  0.72964976  2.88578365 -0.54534257 -0.4752291
 -2.02654485  0.72897471  1.60512763  2.83076993 -1.26456786 -0.2513607 ]. 
Working on 0.4 fold... 
[Mon Mar 25 15:15:56 2024]  Iteration number: 0 with current cost as 0.2336574077910072 and parameters 
[-3.35430471  2.32407218 -2.12987112 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.69367294  1.14432445
  1.79758368 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 15:26:12 2024]  Iteration number: 0 with current cost as 0.19873095076938577 and parameters 
[-3.3162389   2.31049051 -2.12541839 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.67515027  1.14432445
  1.75546213 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 15:37:56 2024]  Iteration number: 50 with current cost as 0.0936709505971591 and parameters 
[-1.40378142  2.41094242 -1.55179803 -0.1165006   0.5539162  -2.77011619
  3.06860715  2.18962015  1.1855363  -1.06645818  3.54903119  1.14432482
  2.51659002 -1.8735541   0.72966128  2.88581255 -0.54531964 -0.4752101
 -2.02652105  0.72897674  1.60514376  2.83081652 -1.26454141 -0.25134827]. 
Working on 0.8 fold... 
[Mon Mar 25 15:40:00 2024]  Iteration number: 0 with current cost as 0.2120721602517624 and parameters 
[-3.32303628  2.31472764 -2.12827089 -0.11653102  0.55388709 -2.77010897
  3.06858499  2.18960146  1.18551999 -1.06648308  0.67869362  1.14432446
  1.76277571 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 15:49:55 2024]  Iteration number: 0 with current cost as 0.2207740696324174 and parameters 
[-3.31737849  2.31867884 -2.1290784  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.68581851  1.14432445
  1.75786798 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 3237.474187850952 seconds. 
Discarding model... 

Training complete taking 80941.29541993141 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0806646347045898 seconds. 
Saved predicted values as M-A2-CZ_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (67.20753708568752,), 'R2_train': 0.6744020387391296, 'MAE_train': 5.416843276611692, 'MSE_test': 50.742992612201945, 'R2_test': 0.6941306832950995, 'MAE_test': 5.393613284147864}. 
Saved model results as M-A2-CZ_Full-Pauli-CRZ_results.json. 
