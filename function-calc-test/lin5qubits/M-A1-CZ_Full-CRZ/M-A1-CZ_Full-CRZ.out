/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:31:29 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:11 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:12 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:13:41 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2718.4814069271088 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:43 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:10 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:18 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:50:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:19 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2657.1890535354614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:32 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:16 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:08 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:56 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:46 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2612.272598028183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:50:14 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:47 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:07:19 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:33 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:42 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2521.256451845169 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:16 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:42 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:49:02 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:45 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:07 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2540.6143679618835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:26 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:22:52 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:22 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:39:51 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:21 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2541.2616465091705 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:57:06 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:29 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:02 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:44 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:29 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2586.5036008358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:14 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:48:31 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:04 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:05:36 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:03 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2552.4261388778687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:22:29 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:31:09 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:10 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:47 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:17 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2589.4923717975616 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:00 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:14:33 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:23:07 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:54 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:40:34 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2600.473629951477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:49:11 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:45 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:06:09 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:15:00 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:23:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2581.190945625305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:01 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:40:19 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:48:42 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:57:06 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:45 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2527.2630524635315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:06 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:22:38 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:09 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:39:31 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:48:00 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2538.060873270035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:56:18 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:05:03 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:13:28 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:22:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:30:39 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2558.9289910793304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:39:06 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:47:34 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:56:04 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:04:41 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:13:01 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2536.4681375026703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:21:33 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:29:54 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:38:58 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:47:42 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:56:25 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2611.7729127407074 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:04:59 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:13:40 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:22:05 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:30:37 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:39:30 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2582.0134353637695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:48:03 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:57:00 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:05:39 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:14:11 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:22:51 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2602.4234652519226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:31:47 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:40:35 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:48:55 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:57:18 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:05:50 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2574.3919422626495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:14:23 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:56 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:31:29 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:40:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:48:48 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2574.5319695472717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:57:08 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:05:22 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:13:48 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:22:24 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:30:49 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2521.743567943573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 08:39:13 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:47:37 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:27 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:05:07 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:13:57 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2587.820453643799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 09:22:13 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:30:49 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:39:11 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:47:36 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:56:03 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2528.548993587494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:04:39 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:13:00 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:21:20 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:29:49 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:38:09 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2522.5333008766174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 10:46:59 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:55:43 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:04:03 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 11:12:34 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:20:50 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 2556.681887626648 seconds. 
Discarding model... 

Training complete taking 64324.3459250927 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.8050851821899414 seconds. 
Saved predicted values as M-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (58.35083788403513,), 'R2_train': 0.7173097739218985, 'MAE_train': 5.013475892245843, 'MSE_test': 77.63284334844319, 'R2_test': 0.5320436669882785, 'MAE_test': 5.13789221227575}. 
Saved model results as M-A1-CZ_Full-CRZ_results.json. 
