/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:33:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:24 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:21 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:51 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:50 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:00 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1658.4359438419342 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:58 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:55 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:22 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:16 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:28 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1650.719008922577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:29 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:23 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:59 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:51 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:11 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1662.3272998332977 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:10 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:03 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:29 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:24 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:31 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1635.0466966629028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:49 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:44 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:09 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:00 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:08 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1666.0982656478882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:11 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:04 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:39 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:33 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:46 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1650.4105336666107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:41 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:36 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:00 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:48 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:15 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1665.9506244659424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:29 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:24 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:56:45 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:34 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:42 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1631.3961460590363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:09 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:24:48 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:35 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:43 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1685.5508568286896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:41:44 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:46:38 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:13 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:06 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:11 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1641.2495062351227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:09:06 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:08 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:36 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:25:24 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1662.344422340393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:47 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:40 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:12 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:59 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:58:07 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1633.2610068321228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:02 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:08:52 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:16 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:20:04 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:25:12 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1629.527010679245 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:11 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:19 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:15 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:08 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:53:22 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1686.3462750911713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:59:18 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:04:20 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:09:45 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:15:34 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:41 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1638.0484702587128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:26:35 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:27 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:36:51 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:38 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:51 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1638.3007247447968 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:53:53 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:58:46 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:04:15 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:10:01 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:10 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.1137804985046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:03 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:26:12 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:40 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:37:34 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:42:58 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1670.1397867202759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:48:55 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:53:51 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:14 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:05:00 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:08 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1632.6631853580475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:06 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:21:00 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:22 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:11 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:37:27 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1654.2174067497253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:43:40 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:48:36 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:54:01 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:59:53 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:05:01 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1635.8004038333893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:10:57 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:15:52 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:21:17 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:04 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:32:11 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.9433014392853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:38:07 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:43:01 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:48:26 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:13 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:59:23 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.7469770908356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:18 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:11 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:15:35 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:21:23 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:26:31 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1630.9864919185638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:32:30 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:23 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:02 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:48:49 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:53:59 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1647.9920432567596 seconds. 
Discarding model... 

Training complete taking 41198.61805009842 total seconds. 
Now scoring model... 
Scoring complete taking 0.954909086227417 seconds. 
Saved predicted values as A2_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (33.80481932693365,), 'R2_train': 0.8362269957964928, 'MAE_train': 4.281238878246925, 'MSE_test': 39.46734444064715, 'R2_test': 0.7620981921883174, 'MAE_test': 4.145503937538271}. 
Saved model results as A2_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:01:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:10 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:06:57 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:15 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:17:56 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:22:58 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1594.6370604038239 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:28:44 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:33:30 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:46 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:44:27 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:28 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1592.206958770752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:55:16 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:00:02 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:18 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:10:59 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:16:05 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1595.995493888855 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:53 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:03 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:20 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:38:01 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:01 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1619.0807621479034 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:51 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:53:38 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:58:56 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:51 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:51 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1607.2114939689636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:15:38 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:20:25 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:47 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:31:43 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:36:54 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1622.5377788543701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:41 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:47:26 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:52:40 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:25 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:25 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1586.2564752101898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:08 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:13:51 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:06 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:24:43 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:29:53 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1591.8460686206818 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:35:39 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:40:25 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:45:38 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:51:19 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:56:33 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1618.5627813339233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:02:40 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:07:33 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:12:47 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:18:25 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:29 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1594.4809954166412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:29:13 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:34:01 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:16 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:44:55 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:50:29 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1623.3841152191162 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:56:17 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:00:59 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:06:14 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:12:02 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:17:03 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1594.0635929107666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:22:49 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:27:35 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:32:51 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:38:29 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:43:28 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1586.6564729213715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:49:17 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:54:00 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:59:16 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:04:52 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:09:51 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1604.0211145877838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:16:00 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:20:43 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:25:56 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:31:32 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:31 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1575.3675396442413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:42:16 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:47:00 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:52:18 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:58:11 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:03:22 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1614.190634727478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:09:09 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:13:57 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:19:33 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:25:11 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:30:24 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1663.7609765529633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:36:56 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:41:50 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:47:04 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:52:53 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:57:54 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1605.5094335079193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:03:39 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:08:24 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:13:39 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:19:15 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:24:14 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1578.7430355548859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:29:57 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:34:42 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:39:56 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:45:36 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:50:36 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1582.4976139068604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:56:21 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:01:05 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:06:19 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:11:57 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:17:19 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1603.9248950481415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:23:04 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:27:47 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:33:01 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:38:39 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:43:42 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1583.157354593277 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:49:28 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:18 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:59:37 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:06:01 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:11:34 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1671.6553316116333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:17:19 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:22:21 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:27:36 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:34:01 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:38:59 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1657.3185744285583 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:44:56 2024]  Iteration number: 0 with current cost as 0.15816724283231598 and parameters 
[-3.25612077  2.17381937 -2.19973102 -0.11653104  0.55388706 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   1.30515913  1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:50:03 2024]  Iteration number: 0 with current cost as 0.11125372440921122 and parameters 
[-3.30297968  2.14712575 -2.2214215  -0.11653103  0.55388706 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.43920123  1.14432447
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534332 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:56:04 2024]  Iteration number: 0 with current cost as 0.1254906583125634 and parameters 
[-3.2769673   2.158708   -2.21272476 -0.11653103  0.55388707 -2.77010899
  3.06858497  2.18960145  1.18551998 -1.0664831   1.3761795   1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:01:41 2024]  Iteration number: 0 with current cost as 0.1380929605663198 and parameters 
[-3.2612221   2.14815726 -2.21631126 -0.11653103  0.55388707 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.0664831   1.3693885   1.14432445
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:06:43 2024]  Iteration number: 0 with current cost as 0.12840383959314663 and parameters 
[-3.28100026  2.15605401 -2.21298184 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.0664831   1.38698029  1.14432446
  1.31029899 -1.87354679  0.72965079  2.88578419 -0.54534333 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1653.1768906116486 seconds. 
Discarding model... 

Training complete taking 40220.24496507645 total seconds. 
Now scoring model... 
Scoring complete taking 0.9537374973297119 seconds. 
Saved predicted values as A2_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (33.80481932693365,), 'R2_train': 0.8362269957964928, 'MAE_train': 4.281238878246925, 'MSE_test': 39.46734444064715, 'R2_test': 0.7620981921883174, 'MAE_test': 4.145503937538271}. 
Saved model results as A2_Modified-Pauli-CRX_results.json. 
