/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:50 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:48:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:13 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:39 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:11 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2935.4414138793945 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:45 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:08 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:29 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2987.740962743759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:39 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:48 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:24 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3018.34211063385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:18:15 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:53 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:25 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2998.625822544098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:08:40 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:22 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:27 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3067.5241210460663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:03 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:07 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:21 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:24:31 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3156.626631975174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:53 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:53:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:29 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:40 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3189.6532368659973 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:46:22 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:53:22 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:24 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:10:39 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3188.51899433136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:15 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:39:28 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:46:40 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:55:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:04 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3205.2617931365967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:16:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:32:53 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:12 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:49:06 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:57:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3151.747194290161 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:09:01 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:24:43 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:56 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:40:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:48:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3106.524968147278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:00:55 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:17:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:24:34 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 03:33:27 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:41:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3162.6679854393005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:53:29 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:07 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:24:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:33:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3099.5309953689575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:45:12 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:01:29 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:08:27 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:17:22 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:25:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3105.6547622680664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:04 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:52:56 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:00:02 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:08:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:17:10 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3135.407251596451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:44:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:51:59 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 07:00:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:08:43 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3096.9109485149384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:20:49 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:37:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:43:50 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 07:52:58 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:01:09 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3140.718993663788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:13:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:29:13 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:36:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 08:45:04 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:53:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3127.2529733181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:05:21 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:21:10 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:20 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 09:36:44 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:44:33 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3072.9749853610992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:56:33 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:12:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:18:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 10:27:48 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:35:38 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3056.5856721401215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 10:47:10 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:03:02 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:09:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 11:18:36 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:26:26 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3044.757303714752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 11:37:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:52:53 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:59:32 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 12:08:18 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:16:10 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3011.6979610919952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 12:28:27 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 12:44:39 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 12:51:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 13:00:19 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:08:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3082.347034931183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 13:19:47 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 13:35:38 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 13:42:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 13:51:13 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:58:59 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3071.034432411194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 14:11:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 14:26:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 14:32:54 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 14:41:14 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 14:48:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2939.1540019512177 seconds. 
Discarding model... 

Training complete taking 77152.70319032669 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.6754987239837646 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRZ_results.json. 
