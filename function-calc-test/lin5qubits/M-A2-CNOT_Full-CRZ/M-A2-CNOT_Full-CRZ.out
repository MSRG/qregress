/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:50 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:48:32 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:13 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:39 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:11 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2935.4414138793945 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:45 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:08 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:29 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2987.740962743759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:39 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:48 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:24 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3018.34211063385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:18:15 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:53 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:25 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2998.625822544098 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:08:40 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:22 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:27 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3067.5241210460663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:03 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:07 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:21 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:24:31 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3156.626631975174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:53 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:53:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:29 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:40 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3189.6532368659973 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:46:22 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:53:22 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:24 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:10:39 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3188.51899433136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:15 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:39:28 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:46:40 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 00:55:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:04 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3205.2617931365967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:16:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:32:53 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:40:12 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 01:49:06 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:57:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3151.747194290161 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:09:01 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:24:43 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:56 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 02:40:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:48:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3106.524968147278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:00:55 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:17:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:24:34 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 03:33:27 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:41:46 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3162.6679854393005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:53:29 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:09:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:16:07 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 04:24:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:33:03 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3099.5309953689575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:45:12 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:01:29 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:08:27 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 05:17:22 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:25:15 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3105.6547622680664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:04 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:52:56 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:00:02 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 06:08:53 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:17:10 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3135.407251596451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:44:50 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:51:59 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 07:00:52 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:08:43 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3096.9109485149384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:20:49 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:37:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:43:50 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 07:52:58 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:01:09 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3140.718993663788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:13:09 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:29:13 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:36:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 08:45:04 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:53:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3127.2529733181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:05:21 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:21:10 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:20 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 09:36:44 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:44:33 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3072.9749853610992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:56:33 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:12:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:18:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 10:27:48 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:35:38 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3056.5856721401215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 10:47:10 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:03:02 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:09:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 11:18:36 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:26:26 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3044.757303714752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 11:37:59 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:52:53 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:59:32 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 12:08:18 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:16:10 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3011.6979610919952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 12:28:27 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 12:44:39 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 12:51:16 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 13:00:19 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:08:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3082.347034931183 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 13:19:47 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 13:35:38 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 13:42:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 13:51:13 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:58:59 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3071.034432411194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 14:11:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 14:26:26 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 14:32:54 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Mon Mar 25 14:41:14 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 14:48:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2939.1540019512177 seconds. 
Discarding model... 

Training complete taking 77152.70319032669 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.6754987239837646 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:29:20 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:32:54 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:47:27 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:54:01 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 12:02:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:09:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2856.178034067154 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:20:39 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:19 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:42 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 12:50:08 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:06 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2950.600737810135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:20 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:33:24 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 13:42:33 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:50:51 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3151.8399834632874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:02:37 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:15 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:10 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 14:34:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:42:12 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3103.7407624721527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:22 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:09:57 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:16:38 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 15:25:17 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:33:05 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3036.7919986248016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:45:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:00:37 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:07:28 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 16:16:20 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:24:12 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3059.9080080986023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:36:04 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:52:19 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:59:17 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 17:08:12 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:16:13 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3113.7520866394043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:27:38 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:42:52 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:50:02 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 17:59:02 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:06:30 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3020.8489320278168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:18:14 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:32:48 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:39:21 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 18:47:47 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:55:17 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2918.581084728241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:06:47 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:21:44 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:28:23 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 19:37:02 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:44:39 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2961.7716958522797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:56:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:11:43 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:18:13 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 20:26:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:34:38 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3010.9424941539764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:46:05 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:00:51 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:07:44 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 21:16:32 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:23:57 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2958.945657491684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:35:29 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:30 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:57:10 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 22:05:43 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:13:20 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2937.3859899044037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:24:31 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:39:33 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:45:54 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 22:54:19 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:01:48 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2906.4374866485596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:12:53 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:28:18 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:34:54 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Thu Apr  4 23:43:21 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:51:02 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2964.1243000030518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:02:21 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:17:36 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:24:34 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 00:33:06 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:40:47 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2996.025216817856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:52:23 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:07:37 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:14:05 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 01:22:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:30:32 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2980.7243452072144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:41:53 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:56:57 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:03:43 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 02:12:04 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:19:40 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2933.393828392029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:30:47 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:46:01 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:52:15 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 03:00:21 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:07:56 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2908.429340362549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:19:17 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:34:38 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:41:20 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 03:49:54 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:57:26 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2971.9871213436127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:08:36 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:23:10 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:29:52 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 04:38:50 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:46:29 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2931.2155940532684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:57:55 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:13:04 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:19:41 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 05:28:14 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:36:07 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 2994.463670015335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:47:36 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:03:21 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:10:04 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 06:19:21 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:27:36 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3116.7119596004486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:39:52 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:55:45 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:02:46 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 07:11:43 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:19:50 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3129.580006837845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:32:00 2024]  Iteration number: 0 with current cost as 0.44731676974652385 and parameters 
[-1.26342713  2.23743432 -2.12427979 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551967 -1.0664834   0.6027151   1.14432414
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.7289737   1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279218 -2.2730979   3.13337139  2.54856927 -0.67550803 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:48:12 2024]  Iteration number: 0 with current cost as 0.23225510178558495 and parameters 
[ 1.31234093  2.23743464 -2.12427943 -0.11653123  0.55388688 -2.77010917
  3.06858478  2.18960165  1.18551978 -1.06648329  0.6027153   1.14432465
  1.31029878 -1.8735466   0.729651    2.88578439 -0.54534355 -0.47522485
 -2.02654261  0.7289739   1.60512684  2.83077087 -1.2645673  -0.25136125
 -2.39279238 -2.27309774  3.13337135  2.54856938 -0.67550807 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:55:09 2024]  Iteration number: 0 with current cost as 0.4080225459531984 and parameters 
[-1.00216457  2.23743451 -2.12427964 -0.11653103  0.55388708 -2.77010922
  3.06858473  2.18960133  1.18551998 -1.06648346  0.6027151   1.14432433
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856946 -0.67550787 -2.69002214]. 
Working on 0.8 fold... 
[Fri Apr  5 08:04:16 2024]  Iteration number: 0 with current cost as 0.46742629522734175 and parameters 
[-0.30763464  2.23743464 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.87354664  0.7296508   2.88578451 -0.54534335 -0.47522454
 -2.0265424   0.72897385  1.60512664  2.83077107 -1.26456678 -0.25136089
 -2.39279187 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:12:28 2024]  Iteration number: 0 with current cost as 0.48975815155432034 and parameters 
[ 0.0757259   2.23743432 -2.12427964 -0.11653134  0.55388692 -2.77010929
  3.06858451  2.18960114  1.18551983 -1.0664834   0.6027151   1.14432445
  1.31029883 -1.8735468   0.7296508   2.88578419 -0.54534351 -0.47522485
 -2.02654256  0.7289737   1.60512632  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856927 -0.67550787 -2.69002202]. 
Training complete taking 3136.9590108394623 seconds. 
Discarding model... 

Training complete taking 75051.34005904198 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.8363656997680664 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184558395,), 'R2_train': 0.2834338374676022, 'MAE_train': 10.976254797333095, 'MSE_test': 167.36739707502417, 'R2_test': -0.008859524176060285, 'MAE_test': 10.950798118296555}. 
Saved model results as M-A2-CNOT_Full-CRZ_results.json. 
