/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:54:40 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:23 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:03:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:06 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:08 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:09 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2308.408954143524 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:46 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:45 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:42 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:48 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2321.6726744174957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:32 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:25 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:48:35 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2327.5355932712555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:08 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:45 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:19:53 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:52 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2360.0897595882416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:47 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:06 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:14 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:24 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2369.323682308197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:04 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:44 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:21 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:39 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2352.3682346343994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:23 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:06 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:09:30 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:17:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:32 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2334.648503303528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:48 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:20 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:20 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:04:20 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2323.7364032268524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:01 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:48 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:03 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:35:09 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:43:01 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2321.731760263443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:49:38 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:14 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:36 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:13:32 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:32 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2313.2152581214905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:12 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:33:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:29 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:52:19 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:22 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2329.1567857265472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:08 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:11:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:12 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:07 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:12 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2327.0151467323303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:45:46 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:50:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:52 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:09:48 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:17:45 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2316.190498113632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:24:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:29:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:40:30 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:48:34 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:30 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2328.869698047638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:08:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:19:31 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:34 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:35:26 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2329.354246377945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:42:00 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:46:36 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:57:42 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:05:35 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:13:23 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2276.2691457271576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:19:51 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:24:28 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:34 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:43:28 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:51:25 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2282.740889787674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:58:00 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:02:35 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:13:55 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:21:44 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:29:36 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2292.894383430481 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:36:07 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:40:40 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:51:46 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:59:45 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:07:37 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2282.3043146133423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:14:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:18:41 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:29:48 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:37:40 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:45:37 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2276.905162334442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:52:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:56:44 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:07:52 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:15:44 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:23:42 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2289.280356168747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:30:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:34:59 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:46:07 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:54:01 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:01:55 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2289.703328371048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:08:28 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:13:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:24:09 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:31:59 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:39:46 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2270.3820288181305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:46:21 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:50:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:02:03 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:09:53 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:17:41 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2278.1044290065765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:24:21 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:28:51 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:39:56 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:47:41 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:55:27 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2262.2505650520325 seconds. 
Discarding model... 

Training complete taking 57764.152477025986 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.0784101486206055 seconds. 
Saved predicted values as M-M-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150772525196,), 'R2_train': -0.5635258410974684, 'MAE_train': 16.553766459728568, 'MSE_test': 301.15155920189375, 'R2_test': -0.8152855575874847, 'MAE_test': 16.304477346428968}. 
Saved model results as M-M-CZ_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:07 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:37 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:54 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:12:43 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:06 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3533.8019444942474 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:29 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:49 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:25 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:12:49 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:28 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3636.0770308971405 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:53 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:07 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:00:29 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:12:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:38 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3541.1298830509186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:34:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:56:51 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:07:54 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:19:25 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3275.6766843795776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:28:45 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:35:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:51:24 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:02:30 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:43 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3268.498240709305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:23:08 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:29:50 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:45:29 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:56:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:07:48 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3234.1229190826416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:17:06 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:23:35 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:39:28 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:50:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:01:32 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3225.4189851284027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:11:11 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:17:33 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:33:19 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:44:22 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:55:28 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3235.5753705501556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:05:07 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:11:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:28:04 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:39:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:50:48 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3320.8820729255676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:00:29 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:07:13 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:23:22 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:34:50 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:46:12 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3335.229306459427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:55:49 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:02:22 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:18:23 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:29:40 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:41:00 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3278.3461809158325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:50:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:57:13 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:13:26 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:11 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:36:31 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3336.3670358657837 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:46:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:52:45 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:09:07 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:20:42 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:32:04 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3325.125993013382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:41:43 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:48:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:04:23 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:15:36 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:26:56 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3292.162015914917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:36:22 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:43:00 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:58:55 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:10:00 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:21:20 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3262.5800321102142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:30:37 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:37:11 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:53:15 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:04:28 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:16:07 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3295.327115535736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:25:40 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:32:14 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:48:11 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:59:39 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:11:00 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3287.481105566025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:20:43 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:27:34 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:44:09 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:55:47 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:07:38 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3399.9069974422455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:17:22 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:24:06 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:40:45 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:52:21 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:03:48 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3367.3816866874695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:13:15 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:19:51 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:36:01 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:47:07 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:58:36 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3290.20512509346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:08:07 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:14:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:31:34 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:43:20 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:54:56 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3379.0267050266266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:04:40 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:11:26 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:28:22 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:39:36 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:50:53 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3357.281012535095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:00:07 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:06:45 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:22:55 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:34:32 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:46:23 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3338.280034303665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:55:54 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:02:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:18:21 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:29:34 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:41:16 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3278.0298914909363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:50:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:56:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:12:36 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:23:42 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:34:37 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3216.3000938892365 seconds. 
Discarding model... 

Training complete taking 83310.21427941322 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.851402521133423 seconds. 
Saved predicted values as M-M-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150772525196,), 'R2_train': -0.5635258410974684, 'MAE_train': 16.553766459728568, 'MSE_test': 301.15155920189375, 'R2_test': -0.8152855575874847, 'MAE_test': 16.304477346428968}. 
Saved model results as M-M-CZ_Full-CRZ_results.json. 
