/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:54:40 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:23 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:03:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:06 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:08 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:09 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2308.408954143524 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:46 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:45 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:42 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:48 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2321.6726744174957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:32 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:25 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:48:35 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2327.5355932712555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:08 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:45 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:19:53 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:52 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2360.0897595882416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:47 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:06 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:14 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:24 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2369.323682308197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:04 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:44 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:21 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:39 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2352.3682346343994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:23 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:06 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:09:30 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:17:29 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:32 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2334.648503303528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:48 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:20 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:20 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:04:20 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2323.7364032268524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:01 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:15:48 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:03 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:35:09 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:43:01 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2321.731760263443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:49:38 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:14 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:36 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:13:32 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:32 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2313.2152581214905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:12 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:33:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:29 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:52:19 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:22 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2329.1567857265472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:08 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:11:54 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:12 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:31:07 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:12 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2327.0151467323303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:45:46 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:50:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:52 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:09:48 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:17:45 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2316.190498113632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:24:27 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:29:15 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:40:30 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:48:34 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:30 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2328.869698047638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:08:03 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:19:31 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:34 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:35:26 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2329.354246377945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:42:00 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:46:36 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:57:42 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:05:35 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:13:23 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2276.2691457271576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:19:51 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:24:28 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:34 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:43:28 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:51:25 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2282.740889787674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:58:00 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:02:35 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:13:55 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:21:44 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:29:36 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2292.894383430481 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:36:07 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:40:40 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:51:46 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:59:45 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:07:37 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2282.3043146133423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:14:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:18:41 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:29:48 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:37:40 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:45:37 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2276.905162334442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:52:09 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:56:44 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:07:52 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:15:44 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:23:42 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2289.280356168747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:30:20 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:34:59 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:46:07 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:54:01 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:01:55 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2289.703328371048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:08:28 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:13:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:24:09 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:31:59 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:39:46 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2270.3820288181305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:46:21 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:50:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:02:03 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:09:53 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:17:41 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2278.1044290065765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:24:21 2024]  Iteration number: 0 with current cost as 0.5502979015750127 and parameters 
[-1.74536136  2.23743486 -2.12427941 -0.1165308   0.55388708 -2.7701092
  3.06858476  2.18960145  1.18552021 -1.06648308  0.60271533  1.14432445
  1.31029921 -1.87354657  0.7296508   2.88578419 -0.54534335 -0.47522463
 -2.02654263  0.72897347  1.60512664  2.83077107 -1.2645671  -0.25136082
 -2.39279195 -2.27309752  3.13337177  2.54856981 -0.67550765 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:28:51 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578404 -0.54534366 -0.47522501
 -2.02654256  0.72897354  1.60512648  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:39:56 2024]  Iteration number: 0 with current cost as 0.19763134127909535 and parameters 
[64.41734328  2.23743464 -2.12426945 -0.11652084  0.55388708 -2.77013954
  3.06858498  2.18959126  1.18553018 -1.06649328  0.60272529  1.14432445
  1.31029899 -1.87353661  0.7296508   2.88578419 -0.54534335 -0.47521466
 -2.0265526   0.7289737   1.60512664  2.83076088 -1.2645671  -0.25135086
 -2.39278199 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:47:41 2024]  Iteration number: 0 with current cost as 0.2013448034758812 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11653103  0.55388633 -2.77011047
  3.06858423  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432295
  1.31029899 -1.8735468   0.72965005  2.88578419 -0.54534335 -0.4752241
 -2.02654315  0.7289737   1.60512664  2.83077032 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:55:27 2024]  Iteration number: 0 with current cost as 0.5086763801419845 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010931
  3.06858481  2.18960145  1.18551998 -1.06648325  0.6027151   1.14432428
  1.31029899 -1.8735468   0.72965064  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077073 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2262.2505650520325 seconds. 
Discarding model... 

Training complete taking 57764.152477025986 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.0784101486206055 seconds. 
Saved predicted values as M-M-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150772525196,), 'R2_train': -0.5635258410974684, 'MAE_train': 16.553766459728568, 'MSE_test': 301.15155920189375, 'R2_test': -0.8152855575874847, 'MAE_test': 16.304477346428968}. 
Saved model results as M-M-CZ_Full-CRZ_results.json. 
