/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:48 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:22 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:38 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:33 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3181.3003833293915 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:19 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:42 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:24 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:59 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3212.5983912944794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:34 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:25 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:23 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:13 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:04 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3333.956030845642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:05 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:26:48 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:51 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:39 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3320.2925419807434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:27 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:21:59 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:33:23 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:19 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:52:57 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3246.643916606903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:28 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:16:04 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:28:09 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:38:02 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:48:01 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3310.060129404068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:46 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:11:56 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:24:07 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:34:19 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:44:24 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3413.654853820801 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:58:51 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:08:45 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:20:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:30:12 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:39:18 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3240.7252118587494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:32 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:02:37 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:15:14 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:25:37 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:36:05 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3408.348646402359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:49:04 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:58:16 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:09:23 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:18:39 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:28:01 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3109.275077819824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:40:57 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:50:12 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:01:18 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:10:39 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:19:56 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3111.8894367218018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:32:50 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:42:08 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:12 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:02:25 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:11:38 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3106.750250339508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:24:33 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:33:49 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:45:02 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:54:19 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:03:30 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3104.6632187366486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:16:15 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:25:28 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:36:43 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:46:06 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:55:21 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3170.2942452430725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:09:46 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:19:59 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:32:06 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:42:01 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:52:13 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3386.838737010956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:06:08 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:16:01 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:28:06 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:37:49 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:48:00 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3360.1081857681274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 08:01:50 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:11:34 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:23:48 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:33:59 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:43:47 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3364.7358360290527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:58:15 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:08:27 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:19:13 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:28:06 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:37:05 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3154.085372209549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:50:34 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:00:06 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:10:46 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:19:37 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:28:35 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3058.5498633384705 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 10:41:05 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 10:50:04 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:01:02 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 11:10:51 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:20:24 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3146.5586988925934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 11:33:49 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 11:43:22 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:54:59 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 12:04:53 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:15:25 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3333.925966024399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 12:29:56 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 12:39:53 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 12:51:54 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 13:01:49 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 13:11:37 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3345.5772671699524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 13:25:34 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 13:35:01 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 13:46:40 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 13:56:14 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 14:05:50 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3264.7167432308197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 14:19:57 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 14:29:55 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 14:41:50 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 14:51:48 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 15:01:33 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3330.8649373054504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 15:15:11 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 15:25:01 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 15:36:37 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 15:46:32 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 15:56:32 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3297.4699976444244 seconds. 
Discarding model... 

Training complete taking 81313.88463807106 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.5414795875549316 seconds. 
Saved predicted values as M-M-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.55034071437586,), 'R2_train': 0.32877009893869913, 'MAE_train': 10.90998099702426, 'MSE_test': 127.74699642063811, 'R2_test': 0.2299648780098158, 'MAE_test': 9.746227604530995}. 
Saved model results as M-M-CNOT_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:46 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:41:15 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:10 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:01:00 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:09:57 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:18:52 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3005.180967569351 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:31:17 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:11 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:55 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:59:51 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:01 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3018.252015352249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:35 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:38 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:20 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:50:40 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:59 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3067.2949888706207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:48 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:22:01 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:33:11 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:42:32 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:51:54 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3118.986980199814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:04:46 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:13:51 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:24:41 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:51 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:42:57 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3064.2629437446594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:56:01 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:05:03 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:15:55 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:25:13 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:34:32 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3088.2764616012573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:47:30 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:56:47 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:58 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:16:56 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:25:49 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3069.929508447647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:38:29 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:47:28 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:58:08 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:07:09 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:16:09 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3017.656974554062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:28:50 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:37:56 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:48:48 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:57:45 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:06:43 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3051.380666255951 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:19:39 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:28:49 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:39:45 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:48:40 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:57:31 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3027.7254531383514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:10:06 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:19:18 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:30:43 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:40:31 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:50:07 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3201.552553653717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:03:57 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:13:52 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:25:36 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:35:45 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:45:45 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3366.629269838333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:00:15 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:10:07 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:21:55 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:32:02 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:41:59 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3314.115026950836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:55:01 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:04:17 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:15:15 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:24:49 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:34:30 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3196.7009172439575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:48:27 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:58:57 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:11:03 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:21:19 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:30:51 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3339.169177055359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:43:54 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:53:44 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:05:24 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:15:33 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:25:30 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3356.342805147171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:40:16 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:50:32 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:02:45 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:13:11 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:23:58 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3510.7874295711517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:39:03 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:49:33 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:02:02 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:12:30 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:22:59 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3445.597541809082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:35:40 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:45:02 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:56:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:58 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:17:24 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3344.475965499878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:31:52 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:42:15 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:53:34 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:02:51 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:12:23 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3277.0019764900208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:26:22 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:36:13 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:48:03 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:58:11 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:08:20 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3328.428042411804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:22:04 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:31:46 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:43:39 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:53:42 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:03:42 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3364.299937725067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:18:06 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:28:35 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:40:47 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:51:28 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:02:03 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3493.0629811286926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:16:22 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:26:51 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:38:50 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:48:45 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:58:45 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3400.9220366477966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:12:58 2024]  Iteration number: 0 with current cost as 0.3845025772034433 and parameters 
[11.52437891  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77011024
  3.06858498  2.18960272  1.18551998 -1.06648435  0.6027151   1.14432572
  1.31029899 -1.8735468   0.72964954  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.72897243  1.60512537  2.8307698  -1.2645671  -0.25136105
 -2.39279345 -2.27309774  3.13337155  2.54856832 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:23:08 2024]  Iteration number: 0 with current cost as 0.4456235043985707 and parameters 
[-0.15387649  2.23743464 -2.12427946 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648326  0.6027151   1.14432463
  1.31029899 -1.87354662  0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.02654258  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:35:05 2024]  Iteration number: 0 with current cost as 0.4279630028595717 and parameters 
[12.75061238  2.2374359  -2.12427837 -0.11652849  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:45:16 2024]  Iteration number: 0 with current cost as 0.2134793718296356 and parameters 
[ 1.10306729  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960177  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:55:09 2024]  Iteration number: 0 with current cost as 0.44383582482302214 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653082  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18551998 -1.06648308  0.60271531  1.14432466
  1.3102992  -1.87354659  0.7296508   2.88578419 -0.54534314 -0.47522464
 -2.02654262  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550766 -2.69002223]. 
Training complete taking 3385.037638902664 seconds. 
Discarding model... 

Training complete taking 80853.0709309578 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.937239170074463 seconds. 
Saved predicted values as M-M-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.55034071437586,), 'R2_train': 0.32877009893869913, 'MAE_train': 10.90998099702426, 'MSE_test': 127.74699642063811, 'R2_test': 0.2299648780098158, 'MAE_test': 9.746227604530995}. 
Saved model results as M-M-CNOT_Full-CRZ_results.json. 
