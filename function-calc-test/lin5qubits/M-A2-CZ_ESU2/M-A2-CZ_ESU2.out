/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:19 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:55 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:45 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:09 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:37 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:46 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 677.5821402072906 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:10 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:02 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:29 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:56 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:03 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 673.5230240821838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:32 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:26 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:55 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:25 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:33 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 688.2460346221924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:51 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:41 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:09 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:37 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:48 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 678.9038443565369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:16 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:09 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:38 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:02 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:18 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 687.0246636867523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:45 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:38 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:33:04 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:29 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:47 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 689.0912976264954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:08 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:01 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:42 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:08 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:18 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 692.264119386673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:43 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:40 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:08 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:38 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:50 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 695.1971645355225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:18 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:22 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:50 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:28 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:12:44 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 715.205729007721 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:16 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:10 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:31 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:52 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:11 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 683.4609580039978 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:41 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:35 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:05 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:38 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:54 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 701.6537563800812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:21 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:40:10 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:42 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:16 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:30 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 694.3011753559113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:50:06 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:08 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:39 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:10 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:10 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 706.2400455474854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:39 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:36 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:55 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:14 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:26 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 674.2877480983734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:57 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:48 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:13 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:19:34 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:40 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 671.8363270759583 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:04 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:26:03 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:28:26 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:55 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:09 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 690.8247153759003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:44 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:38 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:05 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:42:30 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:38 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 684.645437002182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:56 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:49 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:10 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:53:29 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:42 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 674.5377044677734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:16 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:06 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:02:37 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:15 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 707.7028126716614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:56 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:11:55 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:14:27 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:50 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 21:18:59 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 682.3384857177734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:21:28 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:23:27 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:25:55 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:28:32 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 21:30:45 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 705.687228679657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:33:15 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:07 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:41 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:24 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 21:42:31 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 704.8637323379517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:54 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:46:52 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:22 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:44 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:55 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 682.4166736602783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:21 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:58:16 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:00:43 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:03:13 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 22:05:27 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 701.4355487823486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:00 2024]  Iteration number: 0 with current cost as 0.3229016170994012 and parameters 
[-1.45034102  2.23743464 -2.12427937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552025 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:49 2024]  Iteration number: 0 with current cost as 0.3065379173479664 and parameters 
[-1.40256759  2.23743482 -2.12427926 -0.11653084  0.55388727 -2.77010879
  3.06858498  2.18960164  1.18552008 -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:18 2024]  Iteration number: 0 with current cost as 0.2902141455082642 and parameters 
[-1.46071846  2.23743464 -2.12427948 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960161  1.18552014 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:40 2024]  Iteration number: 0 with current cost as 0.29853384426024115 and parameters 
[-1.42614214  2.23743464 -2.12427947 -0.11653103  0.55388716 -2.77010897
  3.0685849   2.18960145  1.18552007 -1.06648308]. 
Working on 1.0 fold... 
[Sun Mar 24 22:16:55 2024]  Iteration number: 0 with current cost as 0.31432267411186554 and parameters 
[-1.44104315  2.23743464 -2.12427945 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308]. 
Training complete taking 677.8113808631897 seconds. 
Discarding model... 

Training complete taking 17241.082372188568 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.5464670658111572 seconds. 
Saved predicted values as M-A2-CZ_ESU2_predicted_values.csv
Model scores: {'MSE_train': (191.95743083459215,), 'R2_train': 0.07003067157585341, 'MAE_train': 11.197196492244611, 'MSE_test': 197.88038239785186, 'R2_test': -0.19278612154185493, 'MAE_test': 10.220631560325149}. 
Saved model results as M-A2-CZ_ESU2_results.json. 
