/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:59 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:02 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:33:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:35:02 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:36:22 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.40927505493164 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:38:15 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:38 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:17 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:38 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:43:59 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.8879029750824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:50 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:28 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:04 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:24 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:45 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 467.82701587677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:02 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:00 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:20 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 454.378276348114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:13 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:43 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:20 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:46 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:08 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 467.04246520996094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:01 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:21 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:00 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:21 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 458.75891971588135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:01 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:38 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:59 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:21 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.2917973995209 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:14 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:36 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:13 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:34 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:54 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.20643067359924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:49 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:11 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:47 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:09 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:30 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.07775807380676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:22 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:45 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:24 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:52 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:14 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 464.26400923728943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:08 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:27 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:05 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:26 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.0811002254486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:45 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:10 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:47 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:08 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:28 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 473.8498229980469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:59 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:56 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:23 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 480.93740725517273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:58 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:04 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:28 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 465.3117187023163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:22 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:43 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:20 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:22:42 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:03 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.45857191085815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:55 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:17 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:53 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:15 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:37 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.00602197647095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:30 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:52 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:29 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:50 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:39:12 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.587965965271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:05 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:26 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:02 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:23 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.92636728286743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:36 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:56 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:35 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:54 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:16 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.8777713775635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:10 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:30 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:08 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:00:27 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.4387741088867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:03:41 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:01 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:59 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:20 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.0695822238922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:13 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:12:33 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:30 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:51 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 450.8596022129059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:44 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:03 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:41 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:01 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:22 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 450.9149830341339 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:15 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:36 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:12 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:32 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:31:53 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.97632026672363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:47 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:08 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:36:44 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:03 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:25 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.46741008758545 seconds. 
Discarding model... 

Training complete taking 11438.908375024796 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9492709636688232 seconds. 
Saved predicted values as M-M-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.5503407173194,), 'R2_train': 0.32877009892443865, 'MAE_train': 10.909980995083536, 'MSE_test': 127.746996437042, 'R2_test': 0.22996487791093612, 'MAE_test': 9.746227601520179}. 
Saved model results as M-M-CNOT_Modified-Pauli-CRZ_results.json. 
