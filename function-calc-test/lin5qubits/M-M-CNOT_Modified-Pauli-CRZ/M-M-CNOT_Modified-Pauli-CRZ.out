/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:59 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:02 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:33:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:35:02 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:36:22 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.40927505493164 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:38:15 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:38 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:17 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:38 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:43:59 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.8879029750824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:50 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:28 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:04 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:24 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:45 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 467.82701587677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:02 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:00 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:20 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 454.378276348114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:13 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:43 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:20 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:46 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:08 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 467.04246520996094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:01 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:21 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:00 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:21 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 458.75891971588135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:01 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:38 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:59 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:21 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.2917973995209 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:24:14 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:36 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:13 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:34 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:54 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.20643067359924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:49 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:11 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:47 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:09 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:30 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.07775807380676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:22 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:45 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:24 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:52 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:14 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 464.26400923728943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:08 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:27 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:05 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:26 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.0811002254486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:45 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:10 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:47 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:08 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:28 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 473.8498229980469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:59 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:56 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:23 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 480.93740725517273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:39 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:58 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:04 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:28 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 465.3117187023163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:22 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:43 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:20 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:22:42 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:03 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.45857191085815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:55 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:17 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:53 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:15 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:37 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.00602197647095 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:30 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:52 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:29 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:50 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:39:12 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.587965965271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:05 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:26 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:02 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:23 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:44 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.92636728286743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:36 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:56 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:35 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:54 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:16 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.8777713775635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:10 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:30 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:08 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:00:27 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.4387741088867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:03:41 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:01 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:39 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:59 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:20 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.0695822238922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:13 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:12:33 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:30 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:51 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 450.8596022129059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:44 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:20:03 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:41 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:01 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:22 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 450.9149830341339 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:15 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:36 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:12 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:32 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:31:53 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.97632026672363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:33:47 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:08 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:36:44 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:03 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:25 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.46741008758545 seconds. 
Discarding model... 

Training complete taking 11438.908375024796 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9492709636688232 seconds. 
Saved predicted values as M-M-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.5503407173194,), 'R2_train': 0.32877009892443865, 'MAE_train': 10.909980995083536, 'MSE_test': 127.746996437042, 'R2_test': 0.22996487791093612, 'MAE_test': 9.746227601520179}. 
Saved model results as M-M-CNOT_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:31:59 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:32:42 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:34:04 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:35:43 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:37:04 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:38:23 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 456.9954216480255 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:40:16 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:41:37 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:43:14 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:44:36 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:45:57 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.8336646556854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:49 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:49:10 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:50:46 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:52:08 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:53:29 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.6422529220581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:55:22 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:56:44 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:58:20 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:59:43 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:01:04 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 459.25152015686035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:03:02 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:04:22 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:06:00 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:07:21 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:08:42 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.6881582736969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:10:35 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:11:54 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:32 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:14:52 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:16:14 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.25739336013794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:18:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:19:30 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:21:07 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:22:27 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:23:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.11639380455017 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:41 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:27:05 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:42 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:04 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:31:24 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 457.51305389404297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:19 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:40 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:16 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:38 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:58 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.2629804611206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:40:52 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:17 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:43:56 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:45:26 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:46:47 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 467.6644113063812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:50:01 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:38 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:05 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:25 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 464.12565994262695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:56:24 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:57:46 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:23 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:00:45 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:02:08 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 456.91976261138916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:07 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:27 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:07:05 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:26 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:47 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 458.3059937953949 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:11:40 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:12:59 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:38 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:57 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:17:18 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 451.29882192611694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:13 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:34 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:18 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:39 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:59 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 461.26315927505493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:51 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:28:12 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:29:53 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:15 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:32:36 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 456.772408246994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:28 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:49 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:37:25 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:38:46 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:40:07 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 450.8428690433502 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:41:59 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:21 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:59 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:20 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:41 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 453.7824900150299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:32 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:50:57 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:36 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:57 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:18 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 457.0730082988739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:57:11 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:38 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:15 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:36 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:55 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 458.2307560443878 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:48 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:06:09 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:07:48 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:09 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:10:42 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 466.03885531425476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:37 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:13:57 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:15:33 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:16:54 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:18:15 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 452.46648359298706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:20:10 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:21:30 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:30 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:06 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:31 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 500.2782974243164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:28:27 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:29:49 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:31:28 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:49 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:34:11 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 455.52752709388733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:36:09 2024]  Iteration number: 0 with current cost as 0.38450260508484435 and parameters 
[11.52438017  2.23743464 -2.12427837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354553  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:37:28 2024]  Iteration number: 0 with current cost as 0.44562350405131135 and parameters 
[-0.1538765   2.23743446 -2.12427946 -0.11653085  0.55388708 -2.77010897
  3.0685848   2.18960145  1.18551998 -1.06648326  0.6027151   1.14432445
  1.31029899 -1.87354662  0.72965062  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:39:04 2024]  Iteration number: 0 with current cost as 0.42796320875556654 and parameters 
[12.75061111  2.2374321  -2.12427964 -0.11653103  0.55388455 -2.77011151
  3.06858372  2.18960145  1.18551998 -1.06648562  0.60271257  1.14432192
  1.31029772 -1.8735468   0.72964827  2.88578293 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:40:27 2024]  Iteration number: 0 with current cost as 0.21347939421238987 and parameters 
[ 1.10306698  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.77010929
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271479  1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:41:48 2024]  Iteration number: 0 with current cost as 0.44383582482302186 and parameters 
[ 0.08121584  2.23743464 -2.12427942 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354659  0.72965059  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 461.95541071891785 seconds. 
Discarding model... 

Training complete taking 11465.10798072815 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0416033267974854 seconds. 
Saved predicted values as M-M-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (138.5503407173194,), 'R2_train': 0.32877009892443865, 'MAE_train': 10.909980995083536, 'MSE_test': 127.746996437042, 'R2_test': 0.22996487791093612, 'MAE_test': 9.746227601520179}. 
Saved model results as M-M-CNOT_Modified-Pauli-CRZ_results.json. 
