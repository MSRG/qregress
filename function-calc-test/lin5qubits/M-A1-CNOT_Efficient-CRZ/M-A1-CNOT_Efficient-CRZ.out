/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:47 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:40 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:52 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:44:15 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:24 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1060.8489820957184 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:22 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:53:14 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:35 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:54 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:04 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1060.0428159236908 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:06 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:01 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:13 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:36 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:52 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.150361776352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:50 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:44 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:59 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:24 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:47 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1078.0200021266937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:57 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:58 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:17 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:00 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:20 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1112.9409425258636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:35 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:34 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:03 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:55 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:11 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1127.4247562885284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:18 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:27 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:57 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:37 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:54 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1126.6348054409027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:09 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:09 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:39 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:23 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:36 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1118.939965248108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:43 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:05 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:10:43 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1095.4832117557526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:53 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:50 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:23:09 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:34 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:48 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1080.3089308738708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:58 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:54 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:19 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:50 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:01 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1087.8755254745483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:05 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:00 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:59:16 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:04:47 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:55 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1076.2286517620087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:57 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:47 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:05 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:39 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.8416130542755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:49 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:46 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:34:58 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:24 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:42:34 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1064.2783784866333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:24 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:41 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:08 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:22 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.717934370041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:25 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:07:18 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:10:39 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:08 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:18:18 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1075.5236585140228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:22:20 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:25:15 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:28:35 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:05 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:36:17 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1080.1803090572357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:22 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:43:17 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:46:33 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:03 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:54:13 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1072.9189298152924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:11 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:06 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:25 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:51 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:02 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1071.250012397766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:08 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:08 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:24 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:28:07 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:20 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1099.3170125484467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:23 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:37:17 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:43 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:13 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:48:23 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1081.9439837932587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:23 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:20 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:36 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:04:03 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:06:15 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.8988695144653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:10:19 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:13:19 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:16:37 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:22:13 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:24:28 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1092.139128446579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:32 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:27 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:48 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:40:18 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:31 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1081.743232011795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:46:27 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:49:22 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:38 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:58:10 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:23 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1072.6292099952698 seconds. 
Discarding model... 

Training complete taking 27099.2820353508 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.607412338256836 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.3537191069769,), 'R2_train': -0.24194773268441727, 'MAE_train': 14.396656404405359, 'MSE_test': 224.07254137994852, 'R2_test': -0.35066758178812285, 'MAE_test': 12.795027044288524}. 
Saved model results as M-A1-CNOT_Efficient-CRZ_results.json. 
