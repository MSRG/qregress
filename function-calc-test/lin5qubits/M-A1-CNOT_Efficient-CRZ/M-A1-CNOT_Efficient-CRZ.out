/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:47 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:40 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:52 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:44:15 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:24 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1060.8489820957184 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:22 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:53:14 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:35 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:54 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:04 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1060.0428159236908 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:06 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:01 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:13 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:36 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:52 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.150361776352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:50 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:44 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:59 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:37:24 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:47 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1078.0200021266937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:57 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:58 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:17 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:00 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:20 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1112.9409425258636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:35 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:34 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:03 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:55 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:11 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1127.4247562885284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:18 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:27 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:57 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:37 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:54 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1126.6348054409027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:09 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:09 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:39 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:23 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:36 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1118.939965248108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:43 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:05 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:10:43 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1095.4832117557526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:53 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:50 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:23:09 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:34 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:48 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1080.3089308738708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:58 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:54 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:19 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:50 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:49:01 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1087.8755254745483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:05 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:00 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:59:16 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:04:47 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:55 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1076.2286517620087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:10:57 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:47 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:05 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:39 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.8416130542755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:49 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:46 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:34:58 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:24 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:42:34 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1064.2783784866333 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:24 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:41 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:08 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:22 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.717934370041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:25 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:07:18 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:10:39 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:08 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:18:18 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1075.5236585140228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:22:20 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:25:15 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:28:35 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:05 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:36:17 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1080.1803090572357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:22 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:43:17 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:46:33 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:03 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:54:13 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1072.9189298152924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:11 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:06 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:25 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:51 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:02 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1071.250012397766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:08 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:08 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:24 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:28:07 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:20 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1099.3170125484467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:34:23 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:37:17 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:40:43 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:13 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:48:23 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1081.9439837932587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:23 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:20 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:36 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:04:03 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:06:15 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1070.8988695144653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:10:19 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:13:19 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:16:37 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:22:13 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:24:28 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1092.139128446579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:28:32 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:27 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:48 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:40:18 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:31 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1081.743232011795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:46:27 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:49:22 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:38 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:58:10 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:00:23 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1072.6292099952698 seconds. 
Discarding model... 

Training complete taking 27099.2820353508 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.607412338256836 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.3537191069769,), 'R2_train': -0.24194773268441727, 'MAE_train': 14.396656404405359, 'MSE_test': 224.07254137994852, 'R2_test': -0.35066758178812285, 'MAE_test': 12.795027044288524}. 
Saved model results as M-A1-CNOT_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:37:44 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:40:04 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:42:49 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:45:51 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:50:52 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:52:52 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 989.0145783424377 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:56:36 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:59:28 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:02:30 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:07:40 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:09:43 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1015.0581781864166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:13:34 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:16:16 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:19:22 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:24:32 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:26:33 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1007.0031778812408 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:18 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:33:16 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:22 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:37 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:45 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1032.2193648815155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:31 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:50:20 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:53:22 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:37 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:00:43 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1021.1193327903748 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:40 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:07:26 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:28 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:52 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:03 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1034.5468211174011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:54 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:24:39 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:27:43 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:01 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:06 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1025.8201575279236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:38:53 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:41:40 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:45 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:49:46 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:51:46 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 999.8847901821136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:55:39 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:26 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:01:26 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:06:43 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:08:49 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1025.7971184253693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:40 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:29 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:35 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:23:46 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:53 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1023.0317325592041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:41 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:32:29 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:39 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:40:50 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:42:55 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1018.8182148933411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:46:46 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:49:25 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:52:29 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:55 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:02 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1034.7404084205627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:58 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:06:43 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:09:53 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:15:03 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:17:11 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1019.1665279865265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:20:52 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:23:46 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:26:51 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:31:54 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:33:55 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1008.8946225643158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:37:41 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:40:23 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:43:27 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:48:38 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:50:39 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1000.6818652153015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:54:18 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:56:55 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:59:58 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:04:55 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:06:59 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 983.3193664550781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:10:42 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:13:29 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:16:28 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:21:35 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:35 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 991.6989288330078 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:27:13 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:29:55 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:32:56 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:37:53 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:39:56 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 981.4476895332336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:43:36 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:46:15 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:49:12 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:54:19 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:56:18 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 980.8084645271301 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:59:56 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:02:35 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:05:34 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:33 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:12:35 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 977.3700542449951 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:16:14 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:18:53 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:21:53 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:26:51 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:51 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 974.7837274074554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:32:29 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:14 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:38:18 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:43:28 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:45:28 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 998.0577065944672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:49:04 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:51:52 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:54:50 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:59:55 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:01:56 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 988.3054077625275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:05:34 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:08:12 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:11:11 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:16:04 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:18:01 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 962.3860113620758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:21:37 2024]  Iteration number: 0 with current cost as 0.46074512914140836 and parameters 
[-1.08295665  2.23743414 -2.12427914 -0.11653153  0.55388708 -2.77010947
  3.06858448  2.18960095  1.18551998 -1.06648358  0.6027146   1.14432395
  1.31029849 -1.8735473 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:24:19 2024]  Iteration number: 0 with current cost as 0.3762389871801105 and parameters 
[-1.15776276  2.23743464 -2.12427939 -0.11653103  0.55388708 -2.7701091
  3.06858486  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:27:16 2024]  Iteration number: 0 with current cost as 0.5001342552128883 and parameters 
[-0.07997905  2.23743435 -2.12427935 -0.11653103  0.55388708 -2.77010926
  3.06858441  2.18960145  1.18551998 -1.06648337  0.6027151   1.14432445
  1.3102987  -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:32:15 2024]  Iteration number: 0 with current cost as 0.39858866235595647 and parameters 
[-1.12518703  2.23743464 -2.12427964 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.18960128  1.18551998 -1.06648344  0.60271493  1.14432427
  1.31029863 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:34:13 2024]  Iteration number: 0 with current cost as 0.40254991577958243 and parameters 
[-1.11690592  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858467  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 971.9247860908508 seconds. 
Discarding model... 

Training complete taking 25065.899988651276 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.5701706409454346 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.3537191069769,), 'R2_train': -0.24194773268441727, 'MAE_train': 14.396656404405359, 'MSE_test': 224.07254137994852, 'R2_test': -0.35066758178812285, 'MAE_test': 12.795027044288524}. 
Saved model results as M-A1-CNOT_Efficient-CRZ_results.json. 
