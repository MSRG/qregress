/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:03 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:05 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:19 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:58 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:39 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 17:43:38 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 826.132161617279 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:49 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:47 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:58 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:36 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:19 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 17:57:11 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 812.3436269760132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:22 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:18 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:29 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:17 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:18 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 18:11:09 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 838.3173987865448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:20 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:17 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 18:16:27 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:05 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:02 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 18:24:58 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 827.9926526546478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:08 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:28 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:46 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:24 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:01 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 18:38:53 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 834.0913691520691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:02 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:19 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:30 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:07 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 18:52:40 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 827.9011080265045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:52 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:49 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:04 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:42 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 19:03:40 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 19:06:33 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 832.5980401039124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:43 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 19:09:39 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:52 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:30 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:37 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 19:20:30 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 836.5982348918915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:39 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:39 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:26 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:09 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:49 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 19:34:39 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 849.7427289485931 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:51 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:44 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:58 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:00 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 19:48:45 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 847.100875377655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:56 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:51 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:06 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:52 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:30 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 20:02:23 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 816.9924862384796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:33 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:31 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 20:07:42 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 20:10:22 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 20:13:01 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 20:15:57 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 813.379869222641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:06 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:26 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:37 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 20:24:15 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:04 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 20:30:01 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 844.3193876743317 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:11 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 20:33:13 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:25 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:09 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 20:43:41 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 819.4936547279358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:50 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:46 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 20:48:58 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:04 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:43 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 20:57:35 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 833.8725814819336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:57:44 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:40 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 21:02:51 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:40 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:18 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 21:11:11 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 816.1098031997681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:20 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:17 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:29 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 21:19:08 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:45 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 21:24:38 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 807.0427327156067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:24:47 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:42 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:53 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:36 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:36 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 21:38:28 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 830.5909950733185 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:38 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:36 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:48 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:32 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:22 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 21:52:15 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 826.4391877651215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:24 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 21:55:22 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 21:57:34 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 22:00:11 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 22:02:49 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 22:05:54 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 820.6169192790985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:05 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:00 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:11 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 22:13:50 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 22:16:30 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 22:19:21 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 805.7035562992096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:19:32 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 22:22:26 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:39 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:44 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:22 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 22:33:30 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 848.8072361946106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:39 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:34 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:46 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:27 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:10 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 22:47:02 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 812.4332523345947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:12 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 22:50:07 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 22:52:17 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 22:54:56 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 23:00:45 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 823.0707099437714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:56 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:37 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:47 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:24 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:06 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Sun Mar 24 23:14:57 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 852.1659038066864 seconds. 
Discarding model... 

Training complete taking 20703.858573675156 total seconds. 
Now scoring model... 
Scoring complete taking 1.1639342308044434 seconds. 
Saved predicted values as A1_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (88.55801841982029,), 'R2_train': 0.5709661222366585, 'MAE_train': 6.823009599037837, 'MSE_test': 102.84891277089528, 'R2_test': 0.38004589296708124, 'MAE_test': 6.734594454442711}. 
Saved model results as A1_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:49 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:56 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 11:31:47 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 11:33:54 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 11:36:28 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 11:38:59 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 11:41:45 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 778.7278451919556 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:41:55 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 11:44:47 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 11:46:54 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 11:49:29 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 11:52:02 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 11:54:47 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 782.8092086315155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:54:57 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 11:57:45 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 11:59:51 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 12:02:31 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:03 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 12:07:47 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 779.9915583133698 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:07:57 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:46 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:52 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:25 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 12:17:58 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 12:20:44 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 776.3375072479248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:20:54 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:43 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:50 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:22 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:54 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 12:33:41 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 777.6031723022461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:51 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:39 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:45 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:18 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:56 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 12:46:51 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 790.0113668441772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:01 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:52 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:57 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:31 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:01 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 12:59:46 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 774.1301763057709 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:55 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:43 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:48 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 13:07:19 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 13:09:49 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 13:12:45 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 779.7869493961334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:55 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 13:15:43 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 13:17:48 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 13:20:19 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 13:22:55 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 13:25:39 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 773.1769185066223 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:25:48 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 13:28:35 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:44 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:16 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 13:38:31 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 771.9904017448425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:38:40 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 13:41:28 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 13:43:33 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:15 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 13:48:45 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 13:51:30 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 779.2096018791199 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:51:39 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 13:54:29 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 13:56:35 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 13:59:11 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 14:01:42 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 14:04:26 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 776.2152616977692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:36 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:31 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:38 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 14:12:16 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 14:14:47 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 14:17:31 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 785.2885150909424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:41 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 14:20:28 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 14:22:35 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:06 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 14:27:37 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 14:30:21 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 770.1599230766296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:30:31 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:20 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:26 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:56 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:27 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 14:43:13 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 771.5485906600952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:43:22 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 14:46:44 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:50 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 14:51:22 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:00 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 14:57:38 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 865.4330546855927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:57:48 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 15:00:35 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:41 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 15:05:13 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 15:07:45 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 15:10:29 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 770.4227492809296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:10:38 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 15:13:27 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 15:15:32 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 15:18:11 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 15:20:46 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 15:23:30 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 780.9927680492401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:23:39 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 15:26:27 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 15:28:33 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 15:31:08 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 15:33:40 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 15:36:24 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 774.5021269321442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:36:34 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 15:39:22 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:27 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 15:44:03 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:35 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 15:49:29 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 784.8083589076996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:49:39 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 15:52:39 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:45 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 15:57:15 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 15:59:46 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 16:02:31 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 781.9591462612152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:02:41 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 16:05:28 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 16:07:34 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:04 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:35 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 16:15:19 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 767.8346996307373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:15:29 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 16:18:16 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 16:20:22 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 16:23:18 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:49 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 16:28:32 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 793.4408965110779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:28:42 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 16:31:30 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 16:33:36 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 16:36:09 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 16:38:41 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 16:41:25 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 773.229189157486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:41:35 2024]  Iteration number: 0 with current cost as 0.35500805116785045 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10068166  0.57804376 -2.71312321
  2.9984599   2.2115748   1.29389852 -1.066444    0.77897797  1.19948176
  1.41257462 -1.78090429  0.71826863]. 
Working on 0.4 fold... 
[Thu Apr  4 16:44:25 2024]  Iteration number: 0 with current cost as 0.3045958845874813 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10480918  0.5722906  -2.72710982
  3.00908302  2.20312239  1.28039842 -1.06645481  0.72924678  1.18391982
  1.40264647 -1.78657914  0.73125248]. 
Working on 0.6 fold... 
[Thu Apr  4 16:46:32 2024]  Iteration number: 0 with current cost as 0.31661990013555186 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10317616  0.57535608 -2.72032308
  3.00851612  2.20730569  1.27899333 -1.06645036  0.75076925  1.19065437
  1.40137846 -1.78988776  0.72366073]. 
Working on 0.8 fold... 
[Thu Apr  4 16:49:03 2024]  Iteration number: 0 with current cost as 0.32415566064644286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10332026  0.57412068 -2.72245379
  3.01184837  2.20843275  1.27260684 -1.06645032  0.75099784  1.1907259
  1.39827721 -1.79331089  0.72180784]. 
Working on 1.0 fold... 
[Thu Apr  4 16:51:36 2024]  Iteration number: 0 with current cost as 0.3189447872393021 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.10339221  0.5745812  -2.72180728
  3.00616163  2.20612194  1.2837265  -1.06645111  0.74652064  1.18932511
  1.40448185 -1.78629743  0.72610498]. 
[Thu Apr  4 16:54:19 2024]  Iteration number: 50 with current cost as 0.14249316430985187 and parameters 
[-2.90318346  2.23743504 -2.12427974  0.41657565  0.22318555 -3.56757173
  3.03034209  3.83265364  2.9976124  -1.04568099  1.58180363  1.5771792
  2.47939329  0.1482469   0.66755939]. 
Training complete taking 774.0140335559845 seconds. 
Discarding model... 

Training complete taking 19533.625890493393 total seconds. 
Now scoring model... 
Scoring complete taking 0.7185735702514648 seconds. 
Saved predicted values as A1_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (88.55801841982029,), 'R2_train': 0.5709661222366585, 'MAE_train': 6.823009599037837, 'MSE_test': 102.84891277089528, 'R2_test': 0.38004589296708124, 'MAE_test': 6.734594454442711}. 
Saved model results as A1_HWE-CNOT_results.json. 
