/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:58 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:15 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:41 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:09 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:34 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:22 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1032.094634771347 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:34 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:10 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:59:41 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:23 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:30 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1088.5369153022766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:43 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:16 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:41 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:12 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:06 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1052.6169972419739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:08 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:40 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:06 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:37 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:29 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1045.2136690616608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:39 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:00 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:36 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:59 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:36 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1024.7333710193634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:33 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:56 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:19 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:46 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:34 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1016.5551347732544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:19:34 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:52 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:13 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:37 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:20 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1006.0769248008728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:21 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:06 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:46:37 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:50:23 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1029.1669700145721 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:34 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:02 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:00:28 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:55 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:48 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1040.8553552627563 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:50 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:32 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:50 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:41 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1012.2590107917786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:37 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 20:31:05 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:35 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:58 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:40 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1019.406227350235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:43 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:04 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:26 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:48 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:33 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1011.9312870502472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:01:36 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:54 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:08:15 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:34 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:15:19 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1006.800286769867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:25 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 21:21:43 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:25:00 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:28:18 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:02 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1008.605562210083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:35:14 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:29 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:51 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:14 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:02 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1015.3641605377197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:02 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 21:55:23 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:58:46 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:02:07 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:05:51 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1008.4059791564941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:54 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 22:12:31 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:15:58 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:19:28 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:23:17 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1045.7180061340332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:26:14 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:37 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:32:58 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:36:17 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:57 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 998.8989729881287 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:42:51 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 22:46:07 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:49:26 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:50 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:56:27 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 990.6530101299286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:59:25 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 23:02:45 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:09 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:36 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:13:23 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1015.151697397232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:22 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:43 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:08 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:26:24 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:07 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1004.8522953987122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:06 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:20 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:39:35 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:42:53 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:46:33 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 982.8697330951691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:49:29 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:48 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:56:10 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:59:31 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:03:16 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1007.3732056617737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:06:15 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Mon Mar 25 00:09:39 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:13:03 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:16:33 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:16 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1020.3988525867462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:26 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:45 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:30:05 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:33:28 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:37:07 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1007.6590194702148 seconds. 
Discarding model... 

Training complete taking 25492.19799208641 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.8189549446105957 seconds. 
Saved predicted values as M-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (85.93496617285085,), 'R2_train': 0.5836739300351366, 'MAE_train': 6.503502401976829, 'MSE_test': 79.03249565699748, 'R2_test': 0.523606823333555, 'MAE_test': 5.847272921136886}. 
Saved model results as M-A2-CZ_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:07:34 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:09:07 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 12:11:27 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:53 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:16:17 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:18:58 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 714.8317816257477 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:07 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:32 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:54 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:20 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:31:01 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 723.0675759315491 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:07 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:31 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:55 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:21 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:42:58 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 720.7951166629791 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:14 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:36 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:02 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:22 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:04 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 723.1075112819672 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:11 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:34 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:58 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:04:21 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:03 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 719.4385709762573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:09:12 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 13:11:34 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:54 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:16:15 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:53 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 709.8172318935394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:03 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:31 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:25:52 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:15 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:30:54 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 720.1629991531372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:32:59 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 13:35:21 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:37:45 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:40:06 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:43 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 710.1405136585236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:44:51 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 13:47:08 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:49:33 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:52:02 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:54:42 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 718.4198896884918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:51 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 13:59:14 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:01:36 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:00 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:06:41 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 719.9694509506226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:08:55 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 14:11:19 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:13:51 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:16:15 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:19:01 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 741.6493892669678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:21:11 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 14:23:38 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:57 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:28:22 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:02 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 718.9580645561218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:10 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 14:35:37 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:38:01 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:40:25 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:43:05 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 723.0850002765656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:11 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 14:47:35 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:01 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:29 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:55:10 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 725.0872585773468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:57:22 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 14:59:47 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:08 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:04:35 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:07:12 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 721.7008135318756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:20 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 15:11:49 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:14:16 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:42 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:19:18 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 727.6766862869263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:21:28 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 15:23:51 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:26:16 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:28:40 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:31:18 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 716.8201005458832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:33:25 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 15:35:49 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:38:12 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:33 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:13 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 715.6108665466309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:45:25 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 15:47:50 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:50:12 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:52:31 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:55:08 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 716.878520488739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:57:21 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 15:59:42 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:02:08 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:04:34 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:07:19 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 728.1541821956635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:09:24 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 16:11:47 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:14:13 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:16:43 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:19:25 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 726.6775019168854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:21:34 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 16:24:02 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:26:25 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:28:51 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:31:32 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 728.0157005786896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:33:38 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 16:36:00 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:38:21 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:40:44 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:43:19 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 706.5833687782288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:45:27 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 16:47:50 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:50:14 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:52:41 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:55:19 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 719.8207643032074 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:57:25 2024]  Iteration number: 0 with current cost as 0.1588426573092961 and parameters 
[-4.66791849  2.23743464 -2.12427941 -0.1165308   0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552021 -1.06648308  0.60271522  1.14432456
  1.3102991  -1.87354669]. 
Working on 0.4 fold... 
[Thu Apr  4 16:59:49 2024]  Iteration number: 0 with current cost as 0.13028048452542584 and parameters 
[-4.68790621  2.2374345  -2.12427964 -0.11653103  0.55388708 -2.77010937
  3.06858485  2.18960145  1.18551998 -1.06648308  0.60271497  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:02:13 2024]  Iteration number: 0 with current cost as 0.1234611361062283 and parameters 
[-4.71702488  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.7701092
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:04:33 2024]  Iteration number: 0 with current cost as 0.1376979059294356 and parameters 
[-4.73696963  2.23743475 -2.12427953 -0.11653092  0.55388708 -2.77010919
  3.06858498  2.18960145  1.1855201  -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:07:11 2024]  Iteration number: 0 with current cost as 0.14026363697123015 and parameters 
[-4.66549958  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010925
  3.06858485  2.18960145  1.18551998 -1.06648322  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 711.5373866558075 seconds. 
Discarding model... 

Training complete taking 18008.006851434708 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.7622787952423096 seconds. 
Saved predicted values as M-A2-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (85.93496617285085,), 'R2_train': 0.5836739300351366, 'MAE_train': 6.503502401976829, 'MSE_test': 79.03249565699748, 'R2_test': 0.523606823333555, 'MAE_test': 5.847272921136886}. 
Saved model results as M-A2-CZ_Efficient-CRZ_results.json. 
