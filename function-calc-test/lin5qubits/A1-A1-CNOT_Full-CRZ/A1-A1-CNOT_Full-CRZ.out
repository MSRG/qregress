/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:05 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:11 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:17 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:30 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:13 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2095.789637327194 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:00 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:00 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:39 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:25 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2049.3246569633484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:10 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:05 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:34 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2030.3058414459229 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:55 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:51 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:25 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:15 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:49 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.7208619117737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:17 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:54:13 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:09:44 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:22 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2015.9335567951202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:22:57 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:50 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:26 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:17 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:58 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2015.5108852386475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:36 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:32 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:17 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:03 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:46 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2023.6622037887573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:18 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:07 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 21:50:32 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2016.2897357940674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:51 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:31 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:24 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 22:32:03 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2030.1615538597107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:42 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 22:42:33 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:08 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:03 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 23:06:05 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2038.7768800258636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:50 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:16:56 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:50 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 23:33:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 23:41:33 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2140.1343870162964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:47:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:52 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:45 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 00:09:19 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2151.9861719608307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:24 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:39 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:25 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 00:44:41 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:52:44 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2117.6192033290863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:58:39 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 01:03:44 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:09:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:08 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:10 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2121.137840986252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:34:04 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:10 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:44:52 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:54:47 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:02:28 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2051.3236525058746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:12:53 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:18:28 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 02:28:14 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:59 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2005.244235754013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:41:27 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:46:18 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:51:51 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:01:37 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 03:09:15 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1999.2731523513794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:14:50 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:19:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:18 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:35:15 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 03:43:41 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2080.2251813411713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:49:30 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:55:15 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:01:42 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:33 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:21:57 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2305.678491830826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:27:51 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 04:32:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:38:12 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 04:47:53 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:55:28 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1987.8228361606598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:01:01 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:05:54 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:11:22 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:21:07 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:55 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.962679386139 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:34:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:39:27 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:45:02 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:54:50 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:02:35 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2018.4383161067963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:08:12 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 06:13:04 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:18:35 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 06:28:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:36:02 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.6800153255463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:41:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 06:46:37 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:52:59 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:04:06 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:47 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2228.0942261219025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:19:06 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 07:24:08 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:29:46 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:40:40 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 07:49:31 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2198.608932495117 seconds. 
Discarding model... 

Training complete taking 51744.705891132355 total seconds. 
Now scoring model... 
Scoring complete taking 2.2882168292999268 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510237542225,), 'R2_train': 0.0637923351235985, 'MAE_train': 12.576626094729802, 'MSE_test': 169.18972262632695, 'R2_test': -0.019844163482825028, 'MAE_test': 11.370996008300859}. 
Saved model results as A1-A1-CNOT_Full-CRZ_results.json. 
