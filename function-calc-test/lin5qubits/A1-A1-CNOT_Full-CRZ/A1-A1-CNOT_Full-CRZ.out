/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:05 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:11 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:17 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:30 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:13 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2095.789637327194 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:00 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:00 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:39 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:25 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2049.3246569633484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:10 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:05 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:34 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2030.3058414459229 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:55 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:51 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:25 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 19:36:15 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:49 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.7208619117737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:17 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 19:54:13 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:59:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:09:44 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:22 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2015.9335567951202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:22:57 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:50 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:26 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:17 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:58 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2015.5108852386475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:56:36 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:32 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:17 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:03 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:46 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2023.6622037887573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:18 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:07 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:43 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 21:50:32 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:18 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2016.2897357940674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:51 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:14:31 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:24 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 22:32:03 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2030.1615538597107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:42 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 22:42:33 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:08 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:03 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 23:06:05 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2038.7768800258636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:11:50 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:16:56 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:50 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Sun Mar 24 23:33:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Sun Mar 24 23:41:33 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2140.1343870162964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:47:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Sun Mar 24 23:52:52 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:45 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 00:09:19 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2151.9861719608307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:24 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 00:28:39 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:34:25 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 00:44:41 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 00:52:44 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2117.6192033290863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:58:39 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 01:03:44 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:09:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:08 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:10 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2121.137840986252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:34:04 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:10 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:44:52 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 01:54:47 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:02:28 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2051.3236525058746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:08:02 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:12:53 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:18:28 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 02:28:14 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:59 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2005.244235754013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:41:27 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 02:46:18 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:51:51 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:01:37 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 03:09:15 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1999.2731523513794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:14:50 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:19:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:18 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 03:35:15 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 03:43:41 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2080.2251813411713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:49:30 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 03:55:15 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:01:42 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:33 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:21:57 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2305.678491830826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:27:51 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 04:32:42 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:38:12 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 04:47:53 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 04:55:28 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1987.8228361606598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:01:01 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:05:54 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:11:22 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:21:07 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:55 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.962679386139 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:34:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 05:39:27 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:45:02 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 05:54:50 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:02:35 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2018.4383161067963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:08:12 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 06:13:04 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:18:35 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 06:28:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 06:36:02 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2007.6800153255463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:41:33 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 06:46:37 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:52:59 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:04:06 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:47 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2228.0942261219025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:19:06 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Mon Mar 25 07:24:08 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:29:46 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Mon Mar 25 07:40:40 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Mon Mar 25 07:49:31 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2198.608932495117 seconds. 
Discarding model... 

Training complete taking 51744.705891132355 total seconds. 
Now scoring model... 
Scoring complete taking 2.2882168292999268 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510237542225,), 'R2_train': 0.0637923351235985, 'MAE_train': 12.576626094729802, 'MSE_test': 169.18972262632695, 'R2_test': -0.019844163482825028, 'MAE_test': 11.370996008300859}. 
Saved model results as A1-A1-CNOT_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:40:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:43 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:26 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:58 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 12:03:42 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 12:11:16 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1983.312359571457 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:45 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 12:21:50 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:27:26 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:25 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2055.8292922973633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:51:11 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:18 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:02:09 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 13:12:10 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 13:20:07 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2088.6877360343933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:25:55 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:48 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:36:27 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:38 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 13:54:47 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2067.7098042964935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:23 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:24 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:04 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 14:21:07 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 14:29:17 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2073.050089120865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:34:50 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 14:39:52 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:45:44 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 14:55:45 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:38 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2057.6388947963715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:12 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 15:14:02 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:35 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 15:29:35 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 15:37:10 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2012.5779576301575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:42:41 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 15:47:29 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:53:16 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 16:03:12 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 16:10:47 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2019.1446504592896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:16:29 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 16:21:27 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:07 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 16:37:00 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 16:44:38 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2027.2847828865051 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:50:11 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 16:55:05 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:00:44 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:30 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 17:18:05 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2004.854501247406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:23:35 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 17:28:23 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:33:59 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 17:43:38 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 17:51:06 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1983.2309539318085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:56:32 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 18:01:18 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:06:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 18:16:15 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 18:23:51 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1960.3830828666687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:29:16 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 18:34:00 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:39:30 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 18:49:00 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 18:56:43 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1978.5867528915405 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:02:14 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 19:07:07 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:12:32 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 19:21:53 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 19:29:27 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1963.2390460968018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:34:58 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 19:39:44 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:45:09 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 19:54:44 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 20:02:11 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 1965.4124026298523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:07:40 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 20:12:36 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:18:13 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 20:28:09 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 20:35:49 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2022.3964018821716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:41:25 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 20:46:11 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:51:50 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 21:01:39 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 21:09:17 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2005.7821884155273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:14:56 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 21:19:47 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:25:28 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 21:35:23 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 21:43:00 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2024.6963849067688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:48:30 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 21:53:27 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:58:59 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:53 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 22:16:44 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2024.6359856128693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:22:26 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:35 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:33:20 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 22:43:32 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 22:51:30 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2094.9815740585327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:57:21 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:02:19 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:08:04 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 23:18:21 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Thu Apr  4 23:26:20 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2077.4410831928253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:31:56 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:37:04 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:05 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Thu Apr  4 23:53:07 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Fri Apr  5 00:01:15 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2097.24037027359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:06:56 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 00:12:07 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:17:48 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Fri Apr  5 00:28:02 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Fri Apr  5 00:36:13 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2107.7626485824585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:42:07 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 00:47:16 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:53:01 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Fri Apr  5 01:03:04 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Fri Apr  5 01:11:00 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2077.208876848221 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:16:42 2024]  Iteration number: 0 with current cost as 0.3460325808451482 and parameters 
[ 1.33999893  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.72897385  1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550772 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 01:21:52 2024]  Iteration number: 0 with current cost as 0.28853377548809134 and parameters 
[-1.29406972  2.23743459 -2.12427954 -0.11653103  0.55388708 -2.77010897
  3.06858493  2.18960145  1.18552003 -1.06648308  0.6027152   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522475
 -2.0265424   0.72897374  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:27:41 2024]  Iteration number: 0 with current cost as 0.540885481949374 and parameters 
[ 0.02415267  2.23743464 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648318  0.6027152   1.14432445
  1.31029908 -1.8735467   0.72965071  2.88578419 -0.54534335 -0.47522466
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279228 -2.27309765  3.13337155  2.54856958 -0.67550778 -2.69002211]. 
Working on 0.8 fold... 
[Fri Apr  5 01:37:47 2024]  Iteration number: 0 with current cost as 0.5183899726646668 and parameters 
[ 0.00866845  2.23743444 -2.12427964 -0.11653103  0.55388698 -2.77010897
  3.06858479  2.18960136  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265425   0.7289736   1.60512644  2.83077097 -1.2645671  -0.25136124
 -2.39279228 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002211]. 
Working on 1.0 fold... 
[Fri Apr  5 01:46:00 2024]  Iteration number: 0 with current cost as 0.3275025144923933 and parameters 
[ 1.57604839  2.23743464 -2.12427935 -0.11653074  0.55388708 -2.77010868
  3.06858527  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432474
  1.31029913 -1.87354651  0.72965066  2.88578419 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512664  2.83077121 -1.2645671  -0.25136105
 -2.39279218 -2.2730976   3.13337169  2.54856973 -0.67550773 -2.69002202]. 
Training complete taking 2098.313127040863 seconds. 
Discarding model... 

Training complete taking 50871.40166449547 total seconds. 
Now scoring model... 
Scoring complete taking 2.3780503273010254 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510237542225,), 'R2_train': 0.0637923351235985, 'MAE_train': 12.576626094729802, 'MSE_test': 169.18972262632695, 'R2_test': -0.019844163482825028, 'MAE_test': 11.370996008300859}. 
Saved model results as A1-A1-CNOT_Full-CRZ_results.json. 
