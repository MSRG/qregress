/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:21 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:28 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:38 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:20 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:14 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2210.334417819977 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:45 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:40 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:18 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:12 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:15 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2222.3367178440094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:54 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:54:34 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:03:53 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:35 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:28 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2170.9914877414703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:01 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:30:44 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:53 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:46:45 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:45 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2171.959664583206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:00:31 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:37 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:16:34 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:23:15 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:13 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2193.4630279541016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:36:47 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:43:42 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:05 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:48 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:50 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2188.4997749328613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:16 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:47 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:13 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:36:00 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:41:46 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2158.01615190506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:35 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:56:41 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:01 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:12:40 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:18:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2215.283076286316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:26:26 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:15 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:39 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:49:18 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:02 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2184.0508754253387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:02:28 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:11 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:18:32 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:25 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:28 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2196.108994960785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:39:09 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:46:00 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:55:15 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:05 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:08:08 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2201.946771144867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:57 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:22:37 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:31:55 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:30 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:44:22 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2168.6562118530273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:04 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:58:38 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:07:54 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:14:39 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:20:43 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2178.1433086395264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:28:23 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:35:27 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:44:44 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:51:48 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:57:53 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2229.181583881378 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:41 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:12:46 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:22:05 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:29:11 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:24 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2252.2807157039642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:43:08 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:50:07 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:59:39 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:06:26 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:12:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2227.965318918228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:20:27 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:27:28 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:37:17 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:44:20 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:50:37 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2303.0879011154175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:58:57 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:05:50 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:15:17 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:11 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:03 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2227.7970316410065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:36:13 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:43:41 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:53:28 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:00:39 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:06:57 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2341.2578790187836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:14:46 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:21:52 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:31:40 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:38:53 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:45:05 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2278.4264504909515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:52:56 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:59:51 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:09:13 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:11 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:22:14 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2230.7632591724396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:29:51 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:36:36 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:45:59 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:52:58 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:58:53 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2199.9962236881256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:06:40 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:13:33 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:23:06 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:29:49 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:35:55 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2223.205453634262 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:43:30 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:50:27 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:59:54 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:06:49 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:12:52 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2224.6472640037537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:20:39 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:27:33 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:36:56 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:43:49 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:49:53 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2213.1911084651947 seconds. 
Discarding model... 

Training complete taking 55411.591690301895 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.704310655593872 seconds. 
Saved predicted values as M_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (623.8500267913502,), 'R2_train': -2.022344005804376, 'MAE_train': 22.449042842560807, 'MSE_test': 528.832647555197, 'R2_test': -2.187704789016629, 'MAE_test': 20.860294891734206}. 
Saved model results as M_Full-CRZ_results.json. 
