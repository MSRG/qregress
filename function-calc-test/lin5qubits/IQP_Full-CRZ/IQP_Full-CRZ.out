/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:55 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:14 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:14 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2542.8132584095 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:06 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:24 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:17 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:34 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2507.1074707508087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:04 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:30 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:53 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:55 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2540.106514930725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:18 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:34 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:20 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:14 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2407.5089433193207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:19 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:26:23 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:42:49 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:51:02 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2450.8817489147186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:09 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:41 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:23:58 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:28 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2501.850238084793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:41:07 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:26 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:58:04 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:36 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2554.9841940402985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:23:43 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:40:39 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:51 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:24 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2532.267981529236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:05:45 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:14:18 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:55 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:05 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2563.362701177597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:36 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:13 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:40 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:17 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:22:43 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2524.5533854961395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:30:25 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:06 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:45:49 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:53:34 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:12 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2312.7098019123077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:08:55 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:16:33 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:17 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:32:01 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:45 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2309.4749631881714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:26 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:55:09 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:03:05 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:11:34 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:19:57 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2442.5527050495148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:28:19 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:36:27 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:44:44 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:52:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:00:06 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2380.843290567398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:07:50 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:15:41 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:24:03 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:32:32 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:40:41 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2431.622597694397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:48:22 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:56:14 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:03:58 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:09 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:20:17 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2398.423063516617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:28:30 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:07 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:45:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:53:41 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:02:03 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2526.5919885635376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:10:37 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:19:00 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:27:20 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:35:38 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:43:55 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2502.988894224167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:52:14 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:00:36 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:08:32 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:15 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:23:57 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2368.472996234894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:31:42 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:39:39 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:48:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:56:44 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:05:17 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2524.3019680976868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:13:56 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:17 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:30:13 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:37:58 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:56 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2393.3663251399994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:53:34 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:01:28 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:09:33 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:18:07 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:26:22 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2460.912867307663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:34:36 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:42:13 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:49:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:57:32 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:05:16 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2309.896934747696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:13:07 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:20:43 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:35:53 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:43:27 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2277.1498551368713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:51:03 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:58:39 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:06:23 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:14:03 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:21:32 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2281.9315366744995 seconds. 
Discarding model... 

Training complete taking 61046.677173137665 total seconds. 
Now scoring model... 
Scoring complete taking 2.5937695503234863 seconds. 
Saved predicted values as IQP_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.36924393992999,), 'R2_train': 0.9885218082670201, 'MAE_train': 1.1378144288491423, 'MSE_test': 1.7527885580910945, 'R2_test': 0.9894345167481791, 'MAE_test': 1.177123249985424}. 
Saved model results as IQP_Full-CRZ_results.json. 
