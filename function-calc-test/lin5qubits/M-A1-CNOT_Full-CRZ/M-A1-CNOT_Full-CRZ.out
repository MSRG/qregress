/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:37:11 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:13 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:56 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:56 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:49 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2917.2742178440094 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:41 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:42 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:38 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:59 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:08 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2968.9357419013977 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:29 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:15 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:12 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:47:36 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:55 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2985.795288324356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:05:25 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:04 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:23:00 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:00 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:43:59 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3003.8830897808075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:55:25 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:03:41 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:13:09 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:28:16 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:28 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3040.341835975647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:07 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:54:49 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:10 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:18:24 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:23:57 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2932.30487036705 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:06 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:28 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:49:40 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:03:25 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:08:56 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2695.2891569137573 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:18:58 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 23:26:13 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:34:20 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:04 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:53:33 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2680.583866596222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:03:39 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:10:56 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:19:11 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:32:45 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:38:11 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2669.7490615844727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:48:03 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:55:20 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:27 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:16:53 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:22:19 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2650.2569768428802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:21 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:38 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:47:45 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:01:11 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:06:41 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2657.856393098831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:36 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 02:23:47 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:31:55 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:45:25 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:50 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2653.111568927765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:00:46 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:07:52 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:15:56 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:29:19 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:34:46 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2634.1429555416107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:44:36 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:51:44 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:59:48 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:13:14 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:18:38 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2632.107015609741 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:28:28 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 04:35:40 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:42 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:57:13 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:02:38 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2643.4671022892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:12:33 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 05:19:43 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:27:56 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:41:27 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:46:51 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2650.4463357925415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:56:41 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 06:03:52 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:11:53 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:25:13 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:30:37 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2625.0027437210083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:40:28 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 06:47:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:55:39 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:09:16 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:14:45 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2648.7987372875214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:24:39 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 07:31:55 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:40:11 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:53:52 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:59:12 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2664.388583421707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:09:05 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 08:16:19 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:24:27 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:37:58 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:43:22 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2655.9743678569794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:53:17 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 09:00:30 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:08:36 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:21:54 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:27:10 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2619.6155779361725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:36:54 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 09:43:56 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:51:55 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:05:02 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:10:23 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2597.2853903770447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:20:09 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 10:27:14 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:35:12 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:48:25 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:53:39 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2593.851998567581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 11:03:27 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 11:10:25 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 11:18:18 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 11:31:37 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 11:37:06 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2607.5959899425507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:46:58 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 11:54:01 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 12:01:59 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 12:15:52 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 12:21:22 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2662.4340269565582 seconds. 
Discarding model... 

Training complete taking 68090.49364233017 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.5952515602111816 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (256.3537191753004,), 'R2_train': -0.24194773301542183, 'MAE_train': 14.396656386710248, 'MSE_test': 224.07254103387086, 'R2_test': -0.3506675797020311, 'MAE_test': 12.79502700130196}. 
Saved model results as M-A1-CNOT_Full-CRZ_results.json. 
