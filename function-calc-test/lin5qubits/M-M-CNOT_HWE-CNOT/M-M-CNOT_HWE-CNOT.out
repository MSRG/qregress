/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:39 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 17:35:47 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:00 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:12 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:07 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 17:52:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 17:59:08 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1727.621199131012 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:24 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 18:04:51 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:39 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:17 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 18:20:30 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:03 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 18:27:10 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1682.3084406852722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:27 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 18:32:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:49 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:17 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 18:48:25 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:56 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 18:55:06 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1675.967299938202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:23 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:00:33 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:45 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:43 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 19:16:40 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:10 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 19:23:11 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1685.1850514411926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:28 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:28:38 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:50 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:43 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:22 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 19:44:28 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:58 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 19:50:59 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1670.429637670517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:18 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 19:56:25 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:38 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:15 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:52 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 20:11:59 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:29 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 20:18:26 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1644.2831473350525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:44 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 20:23:48 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:59 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:30 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:04 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 20:39:11 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 20:45:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1634.96741604805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:45:57 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 20:51:07 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:21 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:04 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 21:06:48 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:18 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 21:13:15 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1654.4175448417664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:32 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 21:18:37 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:50 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 21:24:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:01 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 21:34:06 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:38 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 21:40:40 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1647.2059786319733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:59 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 21:46:04 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 21:46:17 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 21:51:50 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:22 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:01:36 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:02:10 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 22:08:21 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1658.3552012443542 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:08:37 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 22:13:43 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 22:13:56 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:32 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:07 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:29:10 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:40 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 22:35:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1642.5004017353058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:00 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 22:41:12 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:25 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:07 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:52:05 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 22:57:19 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:49 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:03:46 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1684.7663629055023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:05 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 23:09:09 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:23 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:58 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:19:37 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 23:25:13 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 23:25:43 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:31:40 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1672.432103395462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:59 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Sun Mar 24 23:37:03 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Sun Mar 24 23:37:14 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:50 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:25 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Sun Mar 24 23:52:30 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Sun Mar 24 23:53:00 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Sun Mar 24 23:59:09 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1648.1622014045715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:59:27 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:04:35 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 00:04:46 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:19 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:56 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 00:20:01 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:31 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 00:26:29 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1640.5747020244598 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:26:46 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:31:51 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 00:32:04 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:48 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:32 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 00:47:35 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 00:48:05 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 00:54:04 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1654.548422574997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:54:20 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 00:59:58 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:00:09 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 01:05:46 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:10:33 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 01:15:38 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 01:16:08 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 01:22:16 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1692.2192471027374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:22:35 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 01:27:40 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:27:51 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 01:33:28 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:03 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 01:43:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 01:43:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 01:49:41 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1645.5400862693787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:50:00 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 01:55:14 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 01:55:25 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:01:06 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:06:04 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 02:11:17 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 02:11:47 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 02:18:05 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1703.3571717739105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:18:22 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 02:23:35 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 02:23:46 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:29:21 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:56 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 02:39:03 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 02:39:33 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 02:45:47 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1661.7026283740997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:46:05 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 02:51:26 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:39 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 02:57:27 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:02:06 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 03:07:21 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 03:07:51 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 03:13:49 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1682.4063618183136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:14:08 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 03:19:29 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 03:19:42 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 03:25:22 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:29:57 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 03:35:22 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 03:35:52 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 03:41:55 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1686.5570352077484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:42:15 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 03:47:22 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 03:47:33 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 03:53:09 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:57:43 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:03:12 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 04:03:42 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 04:09:55 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1680.9680576324463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:10:16 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 04:15:21 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 04:15:32 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 04:21:09 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:25:45 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:30:51 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 04:31:20 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 04:37:17 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1640.466403722763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:37:34 2024]  Iteration number: 0 with current cost as 0.3158571118914961 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10012876  0.52964358 -2.78931118
  3.09312933  2.13569493  1.17390637 -1.01420303  0.61632942  1.21204318
  1.26093092 -1.92610255  0.70708687]. 
[Mon Mar 25 04:43:07 2024]  Iteration number: 50 with current cost as 0.15802374603925967 and parameters 
[-2.90318331  2.23743454 -2.12427992  0.57600278 -1.09959662 -2.77856063
  3.00188253  2.19177881  2.78862963 -0.79742068  0.47991528  2.5058187
  0.21984448 -2.19917186 -0.37957797]. 
Working on 0.4 fold... 
[Mon Mar 25 04:43:20 2024]  Iteration number: 0 with current cost as 0.2673213341857351 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.10627884  0.54312293 -2.77514319
  3.08820188  2.17010728  1.16280328 -1.03368807  0.62922536  1.19242255
  1.24759391 -1.92957904  0.73937425]. 
Working on 0.6 fold... 
[Mon Mar 25 04:49:24 2024]  Iteration number: 0 with current cost as 0.2731554112126978 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11062596  0.5351918  -2.79284563
  3.0831292   2.15169526  1.18203395 -1.02262672  0.62505989  1.20454741
  1.25119578 -1.93043863  0.7242161 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:54:28 2024]  Iteration number: 0 with current cost as 0.28042696181453697 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11373358  0.53829865 -2.79156727
  3.08791444  2.15531654  1.17172212 -1.02289931  0.6260587   1.2045288
  1.24455613 -1.93628971  0.72553925]. 
[Mon Mar 25 04:59:51 2024]  Iteration number: 50 with current cost as 0.09146224204641965 and parameters 
[-2.90318329  2.2374346  -2.12427967  0.61924569  0.34600076 -1.66836898
  3.35170411  3.61778649  1.51476541  1.04008414  1.9611062   2.42925986
  1.60510093 -1.50222167  0.39166901]. 
Working on 1.0 fold... 
[Mon Mar 25 05:00:21 2024]  Iteration number: 0 with current cost as 0.2802935606361306 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11012259  0.53584413 -2.79121815
  3.09782146  2.14161863  1.1624438  -1.02215111  0.61379969  1.20160416
  1.25287728 -1.93215614  0.71242765]. 
[Mon Mar 25 05:06:28 2024]  Iteration number: 50 with current cost as 0.09883688874401034 and parameters 
[-2.90318355  2.2374346  -2.12427967  0.95537948  0.25355227 -1.9243262
  3.92730251  0.49465136  1.19597701  1.12624557  2.07980672  2.4578889
 -0.34773904 -3.6943621   1.17075566]. 
Training complete taking 1751.5056965351105 seconds. 
Discarding model... 

Training complete taking 41768.449214458466 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9245359897613525 seconds. 
Saved predicted values as M-M-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (60.082760029532594,), 'R2_train': 0.7089191923876048, 'MAE_train': 5.410453326163731, 'MSE_test': 84.90095578694752, 'R2_test': 0.48823283773172554, 'MAE_test': 5.963098382934954}. 
Saved model results as M-M-CNOT_HWE-CNOT_results.json. 
