/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:02 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:14 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:03 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:33 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:33 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:11 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1773.1547257900238 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:46 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:34 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:58 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:59 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:34 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1762.9390153884888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:11 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:59 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:23 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:25 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:00 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1766.4325776100159 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:34 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:19 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:42 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:26:43 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:18 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.9789290428162 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:54 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:40 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:04 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:03 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:40 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1761.8328199386597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:06:15 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 20:11:04 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:16:25 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:27 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:02 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1762.3592538833618 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:45:43 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:44 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:19 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.478768825531 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:50 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:09:38 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:00 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:23:59 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:34 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1755.7198390960693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:34:10 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:57 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:21 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:24 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:03 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1767.9096956253052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:23 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:13:46 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:41 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:26:15 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1754.1783344745636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:32:53 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:59 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:57 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:31 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.72074508667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:02:10 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:58 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:12:21 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:21:19 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:53 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1760.8613135814667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:34 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:44 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:50:42 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:54:18 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1763.2400114536285 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:00:52 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:05:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:11:16 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:29 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:24:09 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1795.6068441867828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:30:57 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:35:49 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:41:24 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:50:38 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:54:21 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1818.7764196395874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:01:19 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 01:06:20 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:01 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:21:22 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:25:05 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1839.841028213501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:31:58 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 01:36:58 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:42:31 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:52:03 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:55:44 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1836.9847333431244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:02:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 02:07:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:13:13 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:22:32 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:26:19 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1837.707656621933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:33:21 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 02:38:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:43:53 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:53:11 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:57 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1832.2852766513824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:38 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:08:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:17 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:23:40 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:27:24 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1832.0149343013763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:34:15 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:39:11 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:44:48 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:26 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:58:12 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1849.9611105918884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:04 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:06 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:15:43 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:25:00 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:49 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1834.0044250488281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:35:43 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 04:40:49 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:46:23 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:55:41 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:59:25 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1839.1006298065186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:06:22 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 05:11:30 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:17:07 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:26:47 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:30:37 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1872.1734838485718 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:33 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 05:42:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:48:22 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:57:42 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:01:29 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.4851019382477 seconds. 
Discarding model... 

Training complete taking 44935.748304605484 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2036244869232178 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (256.3537191753004,), 'R2_train': -0.24194773301542183, 'MAE_train': 14.396656386710248, 'MSE_test': 224.07254103387086, 'R2_test': -0.3506675797020311, 'MAE_test': 12.79502700130196}. 
Saved model results as M-A1-CNOT_Full-CRX_results.json. 
