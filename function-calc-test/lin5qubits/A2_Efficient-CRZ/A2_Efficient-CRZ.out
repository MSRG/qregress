/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:39 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:11 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:53 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:38 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.0332844257355 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:51 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:18 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:05 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:52 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 675.5715565681458 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:08 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:56 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:45 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:33 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:19 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 687.421220779419 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:34 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:18 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:00 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:44 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 670.8102321624756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:48 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:18 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:01 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:48 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 676.5006477832794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:03 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:50 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:35 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:23 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:06 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 678.4353711605072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:23 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:07 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:50 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:31 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:25 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.4659843444824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:50:37 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:07 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:52 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:40 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.7489786148071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:55 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:36 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:34 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:21 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 698.1053950786591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:32 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:14 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:01 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:20:44 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:37 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 677.9655153751373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:51 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:20 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:16 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:03 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 686.9182057380676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:16 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:27 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:14 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 669.8152618408203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:30 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:12 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:54 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:54:47 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:35 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 680.439624786377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:47 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:43 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:28 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:06 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.1979682445526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:00 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:11:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:35 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:19 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:19:01 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 671.6759462356567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:13 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:01 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:44 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:30 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 674.0359387397766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:32 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:34:17 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:59 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:40 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:22 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 676.9536244869232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:47 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:28 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:10 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:50:58 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:43 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 670.2868762016296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:54:54 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:56:45 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:30 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:13 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:59 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 679.0481917858124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:06:15 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:08:04 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:50 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:15:29 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 687.1013638973236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:17:39 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:03 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:49 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:31 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 661.6371629238129 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:28:41 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:30:31 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:14 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:35:59 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:45 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 673.8309330940247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:54 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:25 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:47:09 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:56 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 678.4324760437012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:20 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:53:10 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:51 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:42 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:00:25 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 682.6323835849762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:38 2024]  Iteration number: 0 with current cost as 0.4549975158283851 and parameters 
[-4.26569987  2.23743474 -2.12427953 -0.11653092  0.55388708 -2.77010897
  3.06858488  2.18960156  1.1855202  -1.0664833   0.60271521  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:21 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743401 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.06648371  0.60271448  1.14432445
  1.31029867 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:06:06 2024]  Iteration number: 0 with current cost as 0.4011745314291728 and parameters 
[-4.37377097  2.23743432 -2.12427964 -0.11653103  0.55388677 -2.7701096
  3.06858436  2.18960145  1.18551967 -1.06648371  0.60271479  1.14432414
  1.31029867 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:46 2024]  Iteration number: 0 with current cost as 0.3470058589954329 and parameters 
[-20.14851494   2.23743464  -2.12426934  -0.11652588   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18551998  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743432 -2.12427932 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 664.9343750476837 seconds. 
Discarding model... 

Training complete taking 16926.00032734871 total seconds. 
Now scoring model... 
Scoring complete taking 1.7460517883300781 seconds. 
Saved predicted values as A2_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130301213822,), 'R2_train': -0.2611292657414088, 'MAE_train': 13.78007493740001, 'MSE_test': 122.8497457252412, 'R2_test': 0.25948459387248346, 'MAE_test': 10.149093192959475}. 
Saved model results as A2_Efficient-CRZ_results.json. 
