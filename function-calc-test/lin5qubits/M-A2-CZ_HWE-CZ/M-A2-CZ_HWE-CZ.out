/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:11 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:33 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:29 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:31 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:00 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 689.6021549701691 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:02 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:41 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:46:42 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:42 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:14 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 673.123204946518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:16 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:55 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:55 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:54 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:25 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 668.3420302867889 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:25 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:03 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:02 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:03 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:33 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.1270086765289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:33 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:12 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:13 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:13 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:44 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.7651748657227 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:22 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:26 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:57 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 672.7937135696411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:57 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:53 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:59 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:00 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:30 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 692.4921219348907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:30 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:11 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:15 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:15 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 18:57:45 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.8996934890747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:23 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:23 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:32 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 700.641982793808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:25 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:03 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:04 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:04 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:34 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 671.433183670044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:36 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:15 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:16 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:16 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:46 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 669.7931287288666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:47 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:24 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:24 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:54 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 666.4256949424744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:53 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:29 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:33 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:47 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:17 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 681.9892485141754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:57:14 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:59 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:02 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:01 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:31 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 673.8758866786957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:08:29 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:07 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:07 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:15:10 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:39 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.3328311443329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:42 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:20 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:21 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:20 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:49 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 691.319890499115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:13 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:52 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:56 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:55 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:24 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.9576089382172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:42:29 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:06 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:47:06 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:05 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:34 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 666.1197509765625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:53:35 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:15 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:14 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:14 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:01:44 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.3180921077728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:45 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:25 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:24 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:57 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 670.2972958087921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:56 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:33 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:32 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:32 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:02 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 664.6779079437256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:01 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:37 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:37 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:36 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:06 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 663.6199023723602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:04 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:41 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:43 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:04 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:36 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 692.6955425739288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:37 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 21:51:14 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:14 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:13 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:43 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 664.2949981689453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:42 2024]  Iteration number: 0 with current cost as 0.42134316832544183 and parameters 
[-3.19064797  1.19644309 -0.84184908 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960149  1.18552002 -1.06648312  0.60271514  1.14432449
  1.31029902 -1.87354676  0.72965077]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:21 2024]  Iteration number: 0 with current cost as 0.4843524321368117 and parameters 
[-3.33981057  1.31680264 -0.69514514 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:19 2024]  Iteration number: 0 with current cost as 0.4453203292085197 and parameters 
[-3.23834126  1.23115132 -0.79197665 -0.11653095  0.55388715 -2.77010894
  3.06858502  2.18960153  1.1855201  -1.06648308  0.60271514  1.14432449
  1.31029902 -1.87354669  0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:07:24 2024]  Iteration number: 0 with current cost as 0.44645188646368383 and parameters 
[-3.21750199  1.28621372 -0.86892148 -0.11653095  0.55388708 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354673  0.72965077]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:52 2024]  Iteration number: 0 with current cost as 0.4728526464528593 and parameters 
[-3.32287439  1.25059764 -0.67054068 -0.11653095  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  0.60271514  1.14432449
  1.31029902 -1.87354673  0.7296508 ]. 
Training complete taking 674.3142850399017 seconds. 
Discarding model... 

Training complete taking 16882.25378537178 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0674595832824707 seconds. 
Saved predicted values as M-A2-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (85.16597978862421,), 'R2_train': 0.587399410982643, 'MAE_train': 6.411244666454105, 'MSE_test': 80.10125925463224, 'R2_test': 0.5171645152531876, 'MAE_test': 5.925280135556335}. 
Saved model results as M-A2-CZ_HWE-CZ_results.json. 
