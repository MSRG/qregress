/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:38 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:45 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:51 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:58 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:04 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 326.7497956752777 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:41:03 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:42:11 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:18 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:44:24 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:45:35 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 331.2905173301697 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:36 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:45 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:55 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:04 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 337.324031829834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:15 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:53:24 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:54:33 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:44 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:58 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 351.2196898460388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:04 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:59:17 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:25 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:45 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 342.53702425956726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:03:47 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:57 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:11 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:28 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:41 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 357.7415943145752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:47 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:55 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:05 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:32 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 348.99806237220764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:32 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:43 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:55 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:20:36 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.06441855430603 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:42 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:57 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:11 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:21 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:30 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 349.947026014328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:28 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:35 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:29:40 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:30:48 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:54 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 323.5471200942993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:32:52 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:58 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:06 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:12 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 324.56006693840027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:18 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:26 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:31 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:41 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 324.2148644924164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:41 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:49 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:54 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:04 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:09 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 326.44926595687866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:08 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:16 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:22 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:29 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:36 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 327.974445104599 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:35 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:40 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:46 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:51 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:56 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 319.7383642196655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:55 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:02 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:06 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:11 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:16 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 319.4740490913391 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:13 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:19 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:25 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:31 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:39 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 322.97019839286804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:37 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:45 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:50 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:13:57 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:15:05 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 326.753954410553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:03 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:09 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:18:15 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:20 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:24 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 318.755667924881 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:22 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:28 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:37 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:45 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:25:50 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 325.638884305954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:48 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:51 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:58 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:05 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:12 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 321.6763036251068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:09 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:13 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:18 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:35:24 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:30 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.24747610092163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:37:26 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:30 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:36 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:41 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:49 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 321.53726506233215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:46 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:51 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:58 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:46:04 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:10 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 322.870010137558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:09 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:16 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:21 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:29 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:34 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 322.0131742954254 seconds. 
Discarding model... 

Training complete taking 8276.294976949692 total seconds. 
Now scoring model... 
Scoring complete taking 1.0624785423278809 seconds. 
Saved predicted values as A2-A2-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (227.63053606345338,), 'R2_train': -0.10279355079600383, 'MAE_train': 13.231385731446718, 'MSE_test': 214.86093232453808, 'R2_test': -0.29514171659006716, 'MAE_test': 12.615998632261142}. 
Saved model results as A2-A2-CZ_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:15:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:49 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:18:05 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:19:19 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:53 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 387.33087730407715 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:00 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:24:15 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:31 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:26:45 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:28:02 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.351318359375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:11 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:26 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:41 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:32:56 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:12 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.12832832336426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:17 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:34 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:49 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:04 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:40:18 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.0215964317322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:41:24 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:41 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:43:56 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:45:12 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:46:26 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.55767607688904 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:33 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:48 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:04 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:51:19 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:52:41 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 374.8635048866272 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:53:46 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:55:06 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:21 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:57:37 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:52 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.4195430278778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:59 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:13 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:02:30 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:45 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:01 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.6580696105957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:06:07 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:07:22 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:08:37 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:09:53 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:09 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.5209221839905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:15 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:13:31 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:47 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:16:03 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:17:18 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.1923315525055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:18:23 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:19:56 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:14 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:30 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:44 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 386.64110803604126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:50 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:14 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:27:30 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:45 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:30:06 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 381.69416642189026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:31:12 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:32:36 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:34:02 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:35:17 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:36:33 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 387.22273993492126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:37:39 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:53 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:11 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:27 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:43 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 369.6771764755249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:49 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:45:06 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:46:20 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:36 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:48:55 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.2237436771393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:50:02 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:19 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:35 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:49 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:05 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.6586825847626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:14 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:57:29 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:58:46 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:00:01 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:01:20 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 372.12312412261963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:02:25 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:03:42 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:04:57 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:06:13 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:07:28 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.3316271305084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:08:35 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:09:50 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:11:08 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:12:23 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:39 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 371.25253319740295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:14:45 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:16:00 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:17:16 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:18:31 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:19:47 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 368.6625792980194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:20:56 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:22:14 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:30 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:47 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:11 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 385.8287172317505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:27:20 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:28:35 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:29:50 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:31:06 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:21 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.4919877052307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:26 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:43 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:58 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:14 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:38:27 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.77154183387756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:39:33 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:40:49 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:42:04 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:43:20 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:36 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 377.11350178718567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:50 2024]  Iteration number: 0 with current cost as 0.40206921817360797 and parameters 
[-4.25425905  2.23743479 -2.12427948 -0.11653087  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:47:04 2024]  Iteration number: 0 with current cost as 0.32432523471482216 and parameters 
[-4.44706128  2.23743464 -2.12427935 -0.11653117  0.55388708 -2.77010897
  3.06858513  2.1896016   1.18552013 -1.06648323  0.60271525  1.1443246
  1.31029913 -1.87354666  0.72965037  2.88578405 -0.54534349 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:22 2024]  Iteration number: 0 with current cost as 0.3677723207731386 and parameters 
[-4.3440553   2.23743464 -2.12427916 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960193  1.18552046 -1.06648308  0.6027151   1.14432445
  1.31029922 -1.87354633  0.72965057  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:49:38 2024]  Iteration number: 0 with current cost as 0.363780831535238 and parameters 
[-4.35460091  2.23743464 -2.12427964 -0.11653118  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:51:17 2024]  Iteration number: 0 with current cost as 0.3597146207892242 and parameters 
[-7.59805192  2.23743527 -2.124279   -0.11653039  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.6027151   1.14432445
  1.31030025 -1.87354553  0.72965017  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 392.79774045944214 seconds. 
Discarding model... 

Training complete taking 9351.5361931324 total seconds. 
Now scoring model... 
Scoring complete taking 0.9732770919799805 seconds. 
Saved predicted values as A2-A2-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (227.63053606345338,), 'R2_train': -0.10279355079600383, 'MAE_train': 13.231385731446718, 'MSE_test': 214.86093232453808, 'R2_test': -0.29514171659006716, 'MAE_test': 12.615998632261142}. 
Saved model results as A2-A2-CZ_Modified-Pauli-CRZ_results.json. 
